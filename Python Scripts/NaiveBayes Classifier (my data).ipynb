{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stevenloria.com/simple-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from pathlib import Path\n",
    "from beautifultable import BeautifulTable\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "allRunsAccuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE NL VALUES FROM THE FILE!\n",
    "\n",
    "iFile = open(r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\INTENT MINING\\allMerged.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "sentences = iFile.read().split('\\n')\n",
    "\n",
    "with open(r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\INTENT MINING\\allMergedNoNL.csv\", 'w', encoding = \"ISO-8859-1\") as oFile:\n",
    "    for sent in sentences:\n",
    "        parts = sent.split(\"|\")\n",
    "        if len(parts) == 2:\n",
    "            label = parts[0]\n",
    "            sentence = parts[1]\n",
    "\n",
    "            if label == \"NL\": \n",
    "                pass\n",
    "            else: \n",
    "                oFile.write(label+\"|\"+sentence+\"\\n\")\n",
    "            \n",
    "iFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Removing punctuation from file..] \n",
      "[Removing MultiSpace..] \n",
      "[Stemming..]\n",
      "[Writing to file..] C:\\Users\\ani\\Dropbox\\Stuff\\university\\data\\processedMerged250PerClass.txt\n"
     ]
    }
   ],
   "source": [
    "iFilePath = r\"C:\\Users\\ani\\Dropbox\\Stuff\\university\\data\\250PerClass.csv\"\n",
    "\n",
    "fullSWList = ['about', 'above', 'after', 'again', 'against', 'all', 'a' , 'am', 'an', 'and', 'any', 'are', 'as', \n",
    "                'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', \n",
    "                'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', \n",
    "                'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', \n",
    "                'in', 'into', 'is', 'it', 'it', 'its', 'itself', 'just', \"'ll\", \"'m\", 'me', 'more', 'most', 'my', \n",
    "                'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours',\n",
    "                'ourselves', 'out', 'over', 'own', \"'re\", 's', \"'s\", 'same', 'she', 'should', 'so', 'some', 'such',\n",
    "                't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \n",
    "                'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', \n",
    "                'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'with', \n",
    "                'you', 'your', 'yours', 'yourself', 'yourselves','yeah','so','well']\n",
    "\n",
    "UNallowedPunct = {\".\",\",\",\"[\", \"]\", \"=\", \"*\", \"/\", \"+\", \"-\", \"%\", \"#\", \"?\", \"(\", \")\", \"-\", \"_\", \n",
    "                    \";\", \":\", \"'\", \"\\\"\", \"^\", \"`\"}.union(string.ascii_lowercase)\n",
    "\n",
    "def rmSW(sentenceList):\n",
    "    print(\"[Removing punctuation from file..] \")\n",
    "    oSentList = []\n",
    "          \n",
    "    sungleNum = re.compile(\"[0-9]+\")\n",
    "    biggerNum = re.compile(\"(\\d+.\\d+)\")\n",
    "    singleLetter = re.compile(\"(?i)(?<![a-z])[a-z](?![a-z])\")\n",
    "    \n",
    "   \n",
    "    for sent in sentenceList:\n",
    "        parts = sent.split(\"|\")\n",
    "        if len(parts) == 2:\n",
    "            sent = parts[1]\n",
    "            label = parts[0]\n",
    "            sentToTok = word_tokenize(sent)\n",
    "            line = \"\"\n",
    "\n",
    "            for tok in sentToTok:\n",
    "                if tok in UNallowedPunct: pass\n",
    "                elif tok in fullSWList: pass\n",
    "                elif len(tok) == 1: pass\n",
    "                elif sungleNum.match(tok) or biggerNum.match(tok) or singleLetter.match(tok): pass\n",
    "                elif tok == \"OMITTED\" or tok == \"NUMBER\": pass\n",
    "                else: line += tok+\" \"\n",
    "            oSentList.append(label+\"|\"+line)\n",
    "    \n",
    "    return oSentList\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "def stripMultiSpace(sentenceList):\n",
    "    print(\"[Removing MultiSpace..] \")\n",
    "    oSentList = []\n",
    "    \n",
    "    doubleSpaceCommaPattern = r'\\s,\\s'\n",
    "    singleSpaceBothSidesCommaPat = r'(\\s\\,\\s)'\n",
    "    apostrophePat = r'\\s\\''\n",
    "    finalDotPat = r'\\s\\.'\n",
    "    doubleSpacePat = r'\\s\\s'\n",
    "\n",
    "    for sent in sentenceList:\n",
    "        sent = re.sub(doubleSpaceCommaPattern, \",\", sent)\n",
    "        sent = re.sub(singleSpaceBothSidesCommaPat, \",\", sent)\n",
    "        sent = re.sub(apostrophePat, \"'\", sent)\n",
    "        sent = re.sub(finalDotPat, \".\", sent)\n",
    "        sent = re.sub(doubleSpacePat, \" \", sent)\n",
    "        oSentList.append(sent)\n",
    "            \n",
    "    return oSentList\n",
    "        \n",
    "#------------------------------------------------------    \n",
    "\n",
    "def stem(sentenceList):\n",
    "    print(\"[Stemming..]\")\n",
    "    stemmer = LancasterStemmer()\n",
    "    oSentList = []\n",
    "    \n",
    "    for sent in sentenceList:\n",
    "        parts = sent.split(\"|\")\n",
    "        sent = parts[1]\n",
    "        label = parts[0]\n",
    "        sentToTok = word_tokenize(sent)\n",
    "        line = \"\"\n",
    "        \n",
    "        for tok in sentToTok:\n",
    "            stem = stemmer.stem(tok)\n",
    "            line += stem+\" \"\n",
    "            \n",
    "        oSentList.append(label+\"|\"+line+\"\\n\")\n",
    "            \n",
    "    return oSentList  \n",
    "\n",
    "#------------------------------------------------------    \n",
    "\n",
    "def main():\n",
    "    curFile = open(iFilePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "\n",
    "    sentences = curFile.read().split(\"\\n\")\n",
    "    #print(sentences)\n",
    "    sentences = rmSW(sentences)\n",
    "    sentences = stripMultiSpace(sentences)\n",
    "    sentences = stem(sentences)\n",
    "    \n",
    "    oFilePath = r\"C:\\Users\\ani\\Dropbox\\Stuff\\university\\data\\processedMerged250PerClass.txt\"\n",
    "    with open(oFilePath, 'w', encoding = \"utf-8\") as oFile:\n",
    "        print(\"[Writing to file..]\",oFilePath)\n",
    "        for sent in sentences:\n",
    "            parts = sent.split(\"|\")\n",
    "            if len(parts) == 2:\n",
    "                sent = parts[1]\n",
    "                label = parts[0]\n",
    "                if label == \"NL\":\n",
    "                    pass\n",
    "                else:\n",
    "                    oFile.write(label+\"|\"+sent)\n",
    "\n",
    "    curFile.close()\n",
    "    \n",
    "#---------------- \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split         [TRAIN:TEST]       [75:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(r\"C:\\Users\\ani\\Dropbox\\Stuff\\university\\data\\processedMerged250PerClass.txt\", 'r', encoding = \"ISO-8859-1\")\n",
    "sentences = file.read().split('\\n')\n",
    "text = \"\"\n",
    "\n",
    "for sent in sentences:\n",
    "    text += sent+\"\\n\"\n",
    "\n",
    "sentences = text.split(\"\\n\")\n",
    "\n",
    "trainS = []\n",
    "testS = []\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "def split_testTrain():\n",
    "    nrsplit = 0\n",
    "    textsize = len(sentences)\n",
    "    trainTestRatio = round(0.75 * textsize)\n",
    "\n",
    "    #print(\"Total sent: \",textsize, \" | TRAIN sent: \",trainTestRatio,\" | TEST sent: \",textsize-trainTestRatio)\n",
    "\n",
    "    while not len(sentences) == 0:\n",
    "        nrsplit += 1\n",
    "        if nrsplit <= trainTestRatio:\n",
    "            curLine = random.choice(sentences)\n",
    "            sentences.remove(curLine)\n",
    "            trainS.append(curLine) \n",
    "        elif nrsplit > trainTestRatio:\n",
    "            curLine = random.choice(sentences)\n",
    "            sentences.remove(curLine)\n",
    "            testS.append(curLine) \n",
    "\n",
    "    #print(\"Sent: \",len(sentences),\" | Train: \",len(trainS),\" | Test:\",len(testS))   \n",
    "    print(\"TRAIN: \",len(trainS),\" | TEST:\",len(testS))   \n",
    "    \n",
    "    for sent in trainS:\n",
    "        parts = sent.split('|')\n",
    "        if len(parts) > 1:\n",
    "            label = parts[0]\n",
    "            sent = parts[1]\n",
    "            tup = (sent,label)\n",
    "            train.append(tup)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for sent in testS:\n",
    "        parts = sent.split('|')\n",
    "        if len(parts) > 1:\n",
    "            label = parts[0]\n",
    "            sent = parts[1]\n",
    "            tup = (sent,label)\n",
    "            test.append(tup)\n",
    "        else:\n",
    "            continue\n",
    "### ------------------ END SPLITING TEST / TRAIN with my own data\n",
    "            \n",
    "#split_testTrain()\n",
    "#print(\"\\n\",train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takes about 5 min to train and output result PER ITERATION!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  939  | TEST: 313\n",
      "\n",
      "Accuracy: 57.19 %\n",
      "Most Informative Features\n",
      "        contains(second) = True               CD : SM     =     30.3 : 1.0\n",
      "           contains(bal) = True               CD : SM     =     29.1 : 1.0\n",
      "        contains(exampl) = True               EX : CD     =     23.6 : 1.0\n",
      "           contains(let) = True               EX : AP     =     16.4 : 1.0\n",
      "            contains(av) = True               CD : SM     =     14.4 : 1.0\n",
      "iteration execution time: 0.17 min\n",
      "Runs:  1\n",
      "Execution time: 0.17 min\n"
     ]
    }
   ],
   "source": [
    "### ------------------ Naive Bayes train\n",
    "start = time.time()\n",
    "runs = 1\n",
    "\n",
    "with open(r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\INTENT MINING\\NBayeslogNew.txt\", \"a\", encoding = \"utf-8\") as oFile:\n",
    "    \"\"\" -------- ATTENTION: Nr of Iterations x 5 MIN!!! -------------- \"\"\"\n",
    "    while(runs <= 1):  \n",
    "        startIter = time.time()\n",
    "        split_testTrain()           #redo the train-test split\n",
    "        \n",
    "        bayes = NaiveBayesClassifier(train)        # TIME CONSUMING!!! TRAINING PHASE!\n",
    "\n",
    "        # Compute accuracyf\n",
    "        accuracy = bayes.accuracy(test)*100\n",
    "        allRunsAccuracy.append(accuracy)\n",
    "        oFile.write(\"\\nAccuracy: {0:.2f} %\".format(accuracy))\n",
    "        print(\"\\nAccuracy: {0:.2f} %\".format(bayes.accuracy(test)*100))\n",
    "        \n",
    "        # Show 5 most informative features\n",
    "        # This cannot be saved in the output file as it return None\n",
    "        bayes.show_informative_features(5)\n",
    "        runs += 1\n",
    "        endIter = time.time()\n",
    "        print(\"iteration execution time: {0:.2f} min\".format((endIter - startIter)/60))\n",
    "        \n",
    "end = time.time()\n",
    "print(\"Runs: \",len(allRunsAccuracy))\n",
    "print(\"Execution time: {0:.2f} min\".format((end - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fd79ca758ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, train, test, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "start = time.time()\n",
    "\n",
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]\n",
    "\n",
    "with open(r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\NBayeslog.txt\",\"a\",encoding = \"utf-8\") as oFile:\n",
    "    cl = NaiveBayesClassifier(train)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Classify some text\n",
    "    print(\"The other notion is existential stability,which means there exists the flipped it's called an existential quantifier   ==>  \",bayes.classify(\"The other notion is existential stability,which means there exists the flipped it's called an existential quantifier \"))  # \"pos\"\n",
    "    print(\"In fact,like saw,we looked at a counter example that allowed us to actually destabilize,which means drive the state off to infinity,by being unlucky or unclever in how we transitioned in and out of the different modes    ==>> \",bayes.classify(\"In fact,like saw,we looked at a counter example that allowed us to actually destabilize,which means drive the state off to infinity,by being unlucky or unclever in how we transitioned in and out of the different modes \"))   # \"neg\"\n",
    "\n",
    "    # Classify a TextBlob\n",
    "    blob = TextBlob(\"And in the next lecture,we will see,first of all,why the tortoise is able to beat the hare and what to do about it,meaning how do you make the rabbit overtake the turtle  \"\n",
    "                    \"We know what the solution to this system is\", classifier=bayes)\n",
    "    print(blob)\n",
    "    print(blob.classify())\n",
    "\n",
    "    for sentence in blob.sentences:\n",
    "        print(sentence)\n",
    "        print(sentence.classify())\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = cl.accuracy(test)*100\n",
    "    oFile.write(\"Accuracy: {0:.2f} % \\n\".format(accuracy))\n",
    "    print(\"Accuracy: {0:.2f} %\".format(cl.accuracy(test)*100))\n",
    "\n",
    "    # Show 5 most informative features\n",
    "    cl.show_informative_features(5)\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Execution time: {0:.3f} sec | {1:.5f} min\".format((end - start),(end - start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
