{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"\"\"\n",
    "[\n",
    "music\n",
    "]\n",
    "hello\n",
    ",\n",
    "i\n",
    "'m\n",
    "good\n",
    "to\n",
    "see\n",
    "you\n",
    "back\n",
    "in\n",
    "our\n",
    "course\n",
    ".\n",
    "before\n",
    "we\n",
    "start\n",
    "with\n",
    "this\n",
    "video\n",
    ",\n",
    "let\n",
    "me\n",
    "remind\n",
    "you\n",
    "what\n",
    "has\n",
    "been\n",
    "covered\n",
    "in\n",
    "earlier\n",
    "videos\n",
    ".\n",
    "by\n",
    "now\n",
    ",\n",
    "you\n",
    "should\n",
    "have\n",
    "an\n",
    "idea\n",
    "of\n",
    "what\n",
    "an\n",
    "embedded\n",
    "processor\n",
    "is\n",
    "and\n",
    "how\n",
    "it\n",
    "works\n",
    ".\n",
    "you\n",
    "\n",
    "are\n",
    "interchangeable\n",
    "in\n",
    "the\n",
    "sense\n",
    "that\n",
    "the\n",
    "same\n",
    "application\n",
    "can\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "[music]\n",
    "hello, i'm good to see\n",
    "you back in our course.\n",
    "before we start with this video,\n",
    "let me remind you what has been\n",
    "covered in earlier videos.\n",
    "by now, you should have an idea of what\n",
    "an embedded processor is and how it works.\n",
    "you should also be familiar with\n",
    "the features of an embedded processor.\n",
    "i must remind you again,\n",
    "that in this course we make\n",
    "emphasis on microcontrollers.\n",
    "therefore, the large field\n",
    "of embedded processors\n",
    "is now reduced to only microcontrollers.\n",
    "after you learn about microcontrollers,\n",
    "the next step would be to somehow\n",
    "put your hands on them and\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b63ae506b9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# initial text is full of new lines, so we have to remove them first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentencesInit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from tabulate import tabulate\n",
    "\n",
    "# initial text is full of new lines, so we have to remove them first. \n",
    "text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "sentencesInit = sent_tokenize(text)\n",
    "#sentences = sentences.replace('\\n',' ')\n",
    "#print(sentences)\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "taggedTok = pos_tag(tokens)\n",
    "\n",
    "POS = tabulate(taggedTok)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "taggedTok = pos_tag(tokens)\n",
    "for tok in taggedTok:\n",
    "        \n",
    "    if tok[0] == \",\" or tok[0] == \".\" or tok[0] == \"[\" or tok[0] == \"]\":\n",
    "        # if token is a punctuation symbol, don't save in the output\n",
    "        pass\n",
    "    else:\n",
    "        tokens.append(tok)\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "music\n",
      "musician\n",
      "musical\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"music\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"musician\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"musical\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/sf_Shared_Folder/TEST/one file\"\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dataSetTrain = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "dataSetTest = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "#print(dataSetTrain.target_names) #prints all the categories\n",
    "#print(\"\\n\".join(dataSetTrain.data[0].split(\"\\n\")[:3])) #prints first line of the first data file\n",
    "#print(\"\\n\".join(dataSetTrain.data[1].split(\"\\n\")[:4])) #prints first line of the first data file\n",
    "#print(\"\\n\")\n",
    "#print(\"\\n\".join(dataSetTest.data[1].split(\"\\n\")[:4])) #prints first line of the first data file\n",
    "#print(len(dataSetTrain))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "counts = X_train_counts = count_vect.fit_transform(dataSetTrain.data)\n",
    "shaped = X_train_counts.shape\n",
    "\n",
    "#print(shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def findRealTerm(iFile, lemmaIn, POSfile):   \n",
    "    # take the path of the input file and look for the file ending on \"_stemmedbyPOS.txt\"\n",
    "    # split each line into 3, look in line[3] for the first match of the current lemma\n",
    "    # when found, take line[0] which is the full word and return that word\n",
    "    # exit the function\n",
    "    #print(\"Looking for term for lemma: \" + lemmaIn)\n",
    "    \n",
    "    baseName = iFile.name.split(\".en\", 1)[0]\n",
    "    \n",
    "    targetfname = baseName+\".en_stemmedbyPOS.txt\"\n",
    "    \n",
    "    #if os.path.basename(targetfname) in POSfileList:\n",
    "        #print(\"File found: \", targetfname)\n",
    "     #   try:\n",
    "            #targetf = open(targetfname, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "            #print(targetf)\n",
    "            \n",
    "    # !\n",
    "    res = \"\"\n",
    "    posfileCont = POSfile.read().split()\n",
    "\n",
    "    for line in posfileCont:\n",
    "        line = line.split(\",\")\n",
    "\n",
    "        word = line[0]\n",
    "        lemma = line[2]\n",
    "\n",
    "        if lemma == lemmaIn:\n",
    "            res = word\n",
    "            # print(type(word),word)\n",
    "            # TODO - return the right output\n",
    "    \n",
    "    return res\n",
    "      #  finally:\n",
    "       #     targetf.close()\n",
    "\n",
    "\n",
    "\n",
    "root = \"/media/sf_Shared_Folder/TEST/one file\"\n",
    "lookupFile = \"test_stemmedbyPOS.txt\"\n",
    "lemmasFile = \"test_lemmatized.txt\"\n",
    "\n",
    "curFile = open(root+\"/\"+lemmasFile, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "lookFile = open(root+\"/\"+lookupFile, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "\n",
    "#findRealTerm(curFile,\"wiggl\",lookFile)\n",
    "\n",
    "items = {\"mus\",\"want\",\"capt\",\"prec\",\"inform\",\"wiggl\",\"output\",\"difinit\",\"der\",\"wil\",\"allow\",\"exact\",\"der\",\"defin\"}\n",
    "for item in items:\n",
    "    print(type(findRealTerm(curFile,item,lookFile)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
