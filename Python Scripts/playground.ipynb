{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"\"\"\n",
    "[\n",
    "music\n",
    "]\n",
    "hello\n",
    ",\n",
    "i\n",
    "'m\n",
    "good\n",
    "to\n",
    "see\n",
    "you\n",
    "back\n",
    "in\n",
    "our\n",
    "course\n",
    ".\n",
    "before\n",
    "we\n",
    "start\n",
    "with\n",
    "this\n",
    "video\n",
    ",\n",
    "let\n",
    "me\n",
    "remind\n",
    "you\n",
    "what\n",
    "has\n",
    "been\n",
    "covered\n",
    "in\n",
    "earlier\n",
    "videos\n",
    ".\n",
    "by\n",
    "now\n",
    ",\n",
    "you\n",
    "should\n",
    "have\n",
    "an\n",
    "how\n",
    "it\n",
    "works\n",
    ".\n",
    "you\n",
    "\n",
    "are\n",
    "interchangeable\n",
    "in\n",
    "the\n",
    "application\n",
    "can\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"\n",
    "[music]\n",
    "hello, i'm good to see\n",
    "you back in our course.\n",
    "before we start with this video,\n",
    "let me remind you what has been\n",
    "covered in earlier videos.\n",
    "by now, you should have an idea of what\n",
    "an embedded processor is and how it works.\n",
    "you should also be familiar with\n",
    "the features of an embedded processor.\n",
    "i must remind you again,\n",
    "that in this course we make\n",
    "emphasis on microcontrollers.\n",
    "therefore, the large field\n",
    "of embedded processors\n",
    "is now reduced to only microcontrollers.\n",
    "after you learn about microcontrollers,\n",
    "the next step would be to somehow\n",
    "put your hands on them and\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from tabulate import tabulate\n",
    "\n",
    "# initial text is full of new lines, so we have to remove them first. \n",
    "text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "sentencesInit = sent_tokenize(text)\n",
    "#sentences = sentences.replace('\\n',' ')\n",
    "#print(sentences)\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "taggedTok = pos_tag(tokens)\n",
    "\n",
    "POS = tabulate(taggedTok)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "taggedTok = pos_tag(tokens)\n",
    "for tok in taggedTok:\n",
    "        \n",
    "    if tok[0] == \",\" or tok[0] == \".\" or tok[0] == \"[\" or tok[0] == \"]\":\n",
    "        # if token is a punctuation symbol, don't save in the output\n",
    "        pass\n",
    "    else:\n",
    "        tokens.append(tok)\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "music\n",
      "musician\n",
      "musical\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"music\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"musician\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"musical\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/media/sf_Shared_Folder/TEST/one file\"\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dataSetTrain = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "dataSetTest = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "#print(dataSetTrain.target_names) #prints all the categories\n",
    "#print(\"\\n\".join(dataSetTrain.data[0].split(\"\\n\")[:3])) #prints first line of the first data file\n",
    "#print(\"\\n\".join(dataSetTrain.data[1].split(\"\\n\")[:4])) #prints first line of the first data file\n",
    "#print(\"\\n\")\n",
    "#print(\"\\n\".join(dataSetTest.data[1].split(\"\\n\")[:4])) #prints first line of the first data file\n",
    "#print(len(dataSetTrain))\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "counts = X_train_counts = count_vect.fit_transform(dataSetTrain.data)\n",
    "shaped = X_train_counts.shape\n",
    "\n",
    "#print(shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will\n",
      "wiggling\n",
      "allow\n",
      "information\n",
      "derivatively\n",
      "capture\n",
      "difinition\n",
      "NOT FOUND: defin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipdb\n",
    "\n",
    "def findRealTerm(iFilePath, lemmaIn, POSfilePath):   \n",
    "    # take the path of the input file and look for the file ending on \"_stemmedbyPOS.txt\"\n",
    "    # split each line into 3, look in line[3] for the first match of the current lemma\n",
    "    # when found, take line[0] which is the full word and return that word\n",
    "    # exit the function\n",
    "    \n",
    "    # PROBLEM: returns empty <class 'str'>\n",
    "    # SOLUTION: provide path to files instead of opening and sending them to the fuctions. If files are sent to\n",
    "    # the function, it operates on a closed file. So open the file within the function and it works\n",
    "    # PROBLEM: Now it outputs unique values, while we want each time it finds a word, to output it, no matter if\n",
    "    # it's looked up already or not.\n",
    "    \n",
    "    res = \"\"\n",
    "\n",
    "    try:\n",
    "        posFile = open(root+\"/\"+lookupFile, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK    \n",
    "        posfileCont = posFile.read().split()\n",
    "\n",
    "        for line in posfileCont:\n",
    "            line = line.split(\",\")\n",
    "\n",
    "            word = line[0]\n",
    "            lemma = line[2]\n",
    "\n",
    "            if lemma == lemmaIn:\n",
    "                res = word\n",
    "                break\n",
    "    finally:\n",
    "        posFile.close()\n",
    "        \n",
    "    if res == \"\":\n",
    "        res = \"NOT FOUND: \"+lemmaIn\n",
    "    return res\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "root = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\"\n",
    "lookupFile = \"test_stemmedbyPOS.txt\"\n",
    "lemmasFile = \"test_lemmatized.txt\"\n",
    "\n",
    "lemmasFilePath = root+\"/\"+lemmasFile\n",
    "posFilePath = root+\"/\"+lookupFile\n",
    "#print(findRealTerm(curFile,'capt',lookFile))\n",
    "\n",
    "#findRealTerm(curFile,\"wiggl\",lookFile)\n",
    "\n",
    "items = {'defin','allow','wil','difinit','der','wiggl','difinit','inform','difinit','capt','defin'}\n",
    "for item in items:\n",
    "    #print(\"Looking for: \", item)\n",
    "    print(findRealTerm(lemmasFilePath,item,posFilePath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you,6\n",
      "to,3\n",
      "in,3\n",
      "an,3\n",
      "of,3\n",
      "embedded,3\n",
      "the,3\n",
      "microcontrollers,3\n",
      "i,2\n",
      "course,2\n"
     ]
    }
   ],
   "source": [
    "import collections, re\n",
    "\n",
    "#texts = ['John likes to watch movies. Mary likes too.',\n",
    "#'John also likes to watch football games.']\n",
    "\n",
    "#txtList = []\n",
    "#txtList.append(textCorpus)\n",
    "#res = []\n",
    "\"\"\"\n",
    "bagsofwords = [collections.Counter(re.findall(r'\\w+', txt)) for txt in txtList]\n",
    "\n",
    "sumbags = sum(bagsofwords, collections.Counter())\n",
    "\n",
    "\n",
    "print(type(bagsofwords))\n",
    "print(bagsofwords[0])\n",
    "\n",
    "print(type(sumbags))\n",
    "\"\"\"\"\"\"\n",
    "print(res)         # [('robots', 25), ('control', 14), ('behaviors', 12)]      // LIST\n",
    "print(res[2])      # ('behaviors', 12)   // TUPLE\n",
    "print(res[2][0])   # behaviors   // take only the first part of the tuple\n",
    "print(res[2][1])   # 12   // take only the second part of the tuple\n",
    "\"\"\"\n",
    "words = re.findall(r'\\w+', text)\n",
    "res = collections.Counter(words).most_common(10)\n",
    "\n",
    "for tpl in res:\n",
    "    final = str(tpl[0])+\",\"+str(tpl[1])\n",
    "    print(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
