{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import dis\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and split it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Intent Mining ALL files\"  #HP path ALL files\n",
    "data = \"\"\n",
    "labels, texts = [], []\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if filePath.endswith(\".txt\"):\n",
    "                if filePath.endswith(\"_AutoRuleLabels.txt\"): \n",
    "                    pass\n",
    "                elif filePath.endswith(\"_labels.txt\"):\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]    \n",
    "                    \n",
    "                    data = curFile.read()\n",
    "                    \n",
    "                    for i, line in enumerate(data.split(\"\\n\")):\n",
    "                        content = line.split(\"|\")\n",
    "                        if not len(content) == 2:\n",
    "                            pass\n",
    "                        else:\n",
    "                            labels.append(content[0])\n",
    "                            texts.append(content[1])\n",
    "                    \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 10000\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "#dataPath = r\"D:\\Ani's Documents\\~ Documents ~\\TUDelft\\~ Lectures and Homeworks\\~ MSc THESIS\\CODE\\DATASETS\\ML dataset\\corpus\"\n",
    "dataPath = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Intent Mining ALL files\\01_how-do-i-find-the-maximum-and-minimum-values-of-f-on-a-given-domain.en_labels.txt\"\n",
    "#data = open(dataPath).read().lower()\n",
    "data = open(dataPath).read()\n",
    "labels, texts = [], []\n",
    "\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    #content = line.split()\n",
    "    content = line.split(\"|\")\n",
    "    labels.append(content[0])\n",
    "    texts.append(content[1])\n",
    "\n",
    "print(len(texts), len(labels))\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train x\n",
      "6814    in the,on the right side and on the left side ...\n",
      "7943                    that's what this condition means \n",
      "7725                    mass acceleration equal to force \n",
      "2992           you know collisions are great fun to make \n",
      "667     and we alreaOMITTED know a function that if yo...\n",
      "1385                             so,what's going on here \n",
      "7307    the charged particles it generates produce sci...\n",
      "1140    well that didn't work but it wasn't from lack ...\n",
      "6990    what it really is doing is,it's kind of saying...\n",
      "6163    so here is the front,a spinning thing,that's a...\n",
      "6155                                                okay \n",
      "4450    and then you paint and use your elements,like ...\n",
      "1778                            and take a look at,say,f \n",
      "3987                   and that's basically how it works \n",
      "5657    and if fact,the way this would look my observe...\n",
      "4955                        users the dragging operation \n",
      "1655    and when OMITTED is equal to,u is and when OMI...\n",
      "8331     so we just carry out that kind of a calculation \n",
      "6337    and,and,by default i have selected -1 meter in...\n",
      "2290    for example,under the grandfather,father,son,r...\n",
      "3968    so OMITTED we've got the same track here and y...\n",
      "2659    so we need put values in each of these array b...\n",
      "5633                       so,let's just pole place away \n",
      "729     what i don't mean is that i can take a square ...\n",
      "6480    i mean,obviously we're,we hope that the robot'...\n",
      "7097    and to be able to describe the valley of stabi...\n",
      "6371    and effectively what will happen is that the r...\n",
      "5330             that is where this course will help you \n",
      "3868      we've created spacing which is two_pi elements \n",
      "6747    hello and welcome to the seventh and final pro...\n",
      "                              ...                        \n",
      "2733            that's a fairly basic processing program \n",
      "8346    the beauty and you guys aren't doing it in thi...\n",
      "6787    and perhaps i'm either going truly,go to goal,...\n",
      "5200                        it could even endanger lives \n",
      "649     right,h is the thing that's wiggling,so i can ...\n",
      "3051                                    ping,then rumble \n",
      "2533    welcome to my last additional video,which is a...\n",
      "6366                            and this is how we do it \n",
      "2014    so what i'm claiming here is that the antideri...\n",
      "8724                                    anybody remember \n",
      "7888    so, nothing but frame components of my positio...\n",
      "2425    some key repository functions got ensure the l...\n",
      "1731                   you're not allowed to just cancel \n",
      "8488                 if you wanted to keep track of that \n",
      "6927              but let's go really down to the basics \n",
      "3906                  so basically,hue is the color tone \n",
      "2334    music data could be repurposed in different wa...\n",
      "1328    i mean this is positive,negative,negative,posi...\n",
      "8631    and that's the definition of chaos,infinite se...\n",
      "4199    in order to have a closer view of a timer even...\n",
      "6774    and one more thing,here in the map land comman...\n",
      "7391    the amplitude OMITTED is then just the amplitu...\n",
      "8338    so it allows you to cheat a little bit,and jus...\n",
      "3425       lets say it's going a thousand times a second \n",
      "8168    so the final form for everyone of these omegas...\n",
      "8223                   well,not because of the constants \n",
      "4002             so this is what it ends up looking like \n",
      "1315               so let compute the derivative at zero \n",
      "8487    if you want to keep track of them,this has to ...\n",
      "4662    however,character usually has a high degree of...\n",
      "Name: text, Length: 6664, dtype: object\n",
      "\n",
      "\n",
      "Validation X\n",
      "1366    well,think back to what the graph of sine look...\n",
      "5454               it provides strong security for wi-fi \n",
      "1713    i'm going to check now that this value of delt...\n",
      "4403                                       here's a view \n",
      "5998                                  well,that's simple \n",
      "3626        and we use that to access the current record \n",
      "3272                                           excuse me \n",
      "6418    and what this transformation matrix is,does,is...\n",
      "6041    what i want to do today is relate this wall-fo...\n",
      "8183    so if OMITTED is here,you can look at what hap...\n",
      "5346    wired connections generally tend to have highe...\n",
      "7112    and this can serve as a size estimator for the...\n",
      "112       so,think about when i just threw the ball down \n",
      "8855    when you code it,i highly encourage just to av...\n",
      "8339    so if you can do one wheel,really doing multip...\n",
      "1726    you might be tempted to think that you can get...\n",
      "297     music we've alreaOMITTED learned about the cha...\n",
      "4062    the signals should be preferably used the same...\n",
      "5198    on the contrary,there are many challenging iss...\n",
      "2458    provisions for storage should include plans fo...\n",
      "2850    if you want to add something to a variable and...\n",
      "2428    these and other functions performed by trusted...\n",
      "5174    imagine you have a temperature sensor whose va...\n",
      "4554    you will get a smooth rendition and,if you wan...\n",
      "2338    so one of the problems with data sharing espec...\n",
      "593     but,here,i've drawn a picture of a circle and ...\n",
      "8857    later on in class we can even add other stuff ...\n",
      "3418    basically what the filter does sound is it,it ...\n",
      "8444         make the vectors consistent between the two \n",
      "5691    and we couldn't track or used the original poi...\n",
      "                              ...                        \n",
      "3676    but firstly,we start with a straightforward sk...\n",
      "6636             the contour of the boundary of the wall \n",
      "6946    you guys should be able to take exponentials a...\n",
      "5244                                               sound \n",
      "7560    this way,the speed of light is a maximum veloc...\n",
      "8653    how do we find the equations of motion of a ri...\n",
      "8637    i'm also showing speed in here somewhat,so as ...\n",
      "5395                  this is essential in two main ways \n",
      "699     well,this is the limit as OMITTED approaches i...\n",
      "6918    between the two of us,we're going to teach you...\n",
      "4593       and what we minimizes here,is that difference \n",
      "829     contrast that with a situation where i'm up here \n",
      "8703    this is written as is in a coordinate frame in...\n",
      "4149    it would make sense to map the set of tasks on...\n",
      "7277    in this photograph on the left,you see a centr...\n",
      "6241    and they have a resolution of NUMBER ticks per...\n",
      "2181         adobe pdf is good example of an open format \n",
      "4354    however,a program is at this kind of visual cl...\n",
      "6670      this one right here but i picked the first one \n",
      "2815    so this call is going to give us a random numb...\n",
      "46      i've got one critical point on this interval b...\n",
      "6911    by that i really mean we,you know,work out a f...\n",
      "7522    music in this eighth module,we are discussing ...\n",
      "6742    and also you can decide how to combine the two...\n",
      "4625      so free for us to compute current edge lengths \n",
      "3932    the form,it produces it,you load the sound,you...\n",
      "7906    and that means,as you guys already identified,...\n",
      "1864    now,i could cut that interval between eight an...\n",
      "7429    it is the equality of these two properties whi...\n",
      "5949                         well,let's see what happens \n",
      "Name: text, Length: 2222, dtype: object\n",
      "\n",
      "\n",
      "Train y\n",
      "[3 3 3 ... 3 3 3]\n",
      "\n",
      "\n",
      "Validation Y\n",
      "[1 1 1 ... 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "print(\"\\n\\nTrain x\")\n",
    "print(train_x)\n",
    "print(\"\\n\\nValidation X\")\n",
    "print(valid_x)\n",
    "print(\"\\n\\nTrain y\")\n",
    "print(train_y)\n",
    "print(\"\\n\\nValidation Y\")\n",
    "print(valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectors as features\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "\n",
    "## Warning! \n",
    "BIG DATASET - Time consuming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "wordEmbPath = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\wiki-news-300d-1M.vec\"\n",
    "\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open(wordEmbPath, encoding = \"utf-8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text / NLP based features\n",
    "\n",
    "A number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:\n",
    "\n",
    "- Character Count of the documents – total number of characters in the documents\n",
    "- Average Word Density of the documents – average length of the words used in the documents\n",
    "- Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
    "- Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
    "- Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
    "- Frequency distribution of Part of Speech Tags:\n",
    "\t- Noun Count\n",
    "\t- Verb Count\n",
    "\t- Adjective Count\n",
    "\t- Adverb Count\n",
    "\t- Pronoun Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ccf80dec2660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_density'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'char_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'punctuation_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mistitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'upper_case_word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2549\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-ccf80dec2660>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_density'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'char_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'punctuation_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mistitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'upper_case_word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-ccf80dec2660>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_density'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'char_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word_count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'punctuation_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title_word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mistitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'upper_case_word_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "\n",
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n",
    "\n",
    "- Naive Bayes Classifier\n",
    "- Linear Classifier\n",
    "- Support Vector Machine\n",
    "- Bagging Models\n",
    "- Boosting Models\n",
    "    \n",
    "Lets implement these models and understand their details. The following function is a utility function which can be used to train a model. It accepts the **classifier**, **feature vector of training data**, **labels of training data** and **feature vectors of valid data as inputs**. Using these inputs, the model is trained and accuracy score is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.6896\n",
      "NB, WordLevel TF-IDF:  0.6912\n",
      "NB, N-Gram Vectors:  0.5024\n",
      "NB, CharLevel Vectors:  0.676\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.6968\n",
      "LR, WordLevel TF-IDF:  0.6972\n",
      "LR, N-Gram Vectors:  0.5032\n",
      "LR, CharLevel Vectors:  0.716\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.498\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.6952\n",
      "RF, WordLevel TF-IDF:  0.6996\n"
     ]
    }
   ],
   "source": [
    "#Bagging models\n",
    "\n",
    "# Random Forest on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
