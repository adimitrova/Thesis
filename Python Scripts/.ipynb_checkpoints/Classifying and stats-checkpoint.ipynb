{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/data_science_python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from pathlib import Path\n",
    "from beautifultable import BeautifulTable\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "allRunsAccuracy = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                           Sentence\n",
      "0    CD  provided some conditions are satisfied,the ext...\n",
      "1    CD  but how am i supposed to find those maximum an...\n",
      "2    CD  we've actually alreaOMITTED done this in some ...\n",
      "3    CD                       here's a frour-step process \n",
      "4    AP  first,differentiate your function can find all...\n",
      "5    CD  those are places where the derivative is equal...\n",
      "6    CD  and also lists the end points if they're inclu...\n",
      "7    CD                          check those points,right \n",
      "8    AP  check the end points,check the critical points...\n",
      "9    EX                             let's work an example \n"
     ]
    }
   ],
   "source": [
    "prefix = r\"C:\\Users\\ani\"\n",
    "sentences = pandas.read_csv(prefix + r'\\Dropbox\\Stuff\\university\\data\\allMergedNoNL.csv', sep='|', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"Label\", \"Sentence\"], encoding = \"ISO-8859-1\")\n",
    "print(sentences[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "      <td>and again,here i'm going to,here i'm asking yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>6652</td>\n",
       "      <td>6639</td>\n",
       "      <td>rumble</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM</th>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "      <td>and this function is called the residual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EX</th>\n",
       "      <td>562</td>\n",
       "      <td>561</td>\n",
       "      <td>so let me show you a demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM</th>\n",
       "      <td>570</td>\n",
       "      <td>570</td>\n",
       "      <td>in the next video,we will have an overview of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence                                                               \n",
       "         count unique                                                top freq\n",
       "Label                                                                        \n",
       "AP         317    317  and again,here i'm going to,here i'm asking yo...    1\n",
       "CD        6652   6639                                            rumble     2\n",
       "CM         277    277          and this function is called the residual     1\n",
       "EX         562    561                         so let me show you a demo     2\n",
       "SM         570    570  in the next video,we will have an overview of ...    1"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.groupby('Label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                           Sentence  length\n",
      "0    CD  provided some conditions are satisfied,the ext...     119\n",
      "1    CD  but how am i supposed to find those maximum an...      63\n",
      "2    CD  we've actually alreaOMITTED done this in some ...     131\n",
      "3    CD                       here's a frour-step process       28\n",
      "4    AP  first,differentiate your function can find all...      67\n",
      "\n",
      "\n",
      "[\"and i'm labeling these points OMITTED sub i,so my left hand OMITTED point is OMITTED sub and my right hand OMITTED point is OMITTED sub n,and my cut points in between where i'm cutting up to partition are i over n. so,the first one here,or the one after the of anyway,is over n. the next one is over n,the next one is over n,and each of those subintervals has the same width,over n. now,i also have to choose points to sample the function at,and i'm just going to choose the right hand end point here \", \"some epsilon bigger than zero,a corresponding value of delta is going to be epsilon over NUMBER and then,i verify that if OMITTED is within NUMBER,is within delta of NUMBER,then NUMBER is within epsilon of NUMBER and that's exactly what it means to say that the limit of NUMBER is NUMBER as OMITTED approaches NUMBER for every epsilon,there's some delta so that whenever OMITTED is within delta of NUMBER,then the function NUMBER is within epsilon of you can imagine trying to do this with other functions,right \", \"they end up meeting in same location and one way of doing this is to assume you have a rain sensor disk around you and then when you see other robots in that disk instead of thinking of them as obsticles we think of them as buddies so what we are going to do is each robot is going to aim toward the center of gravity of all it's neighboors so everyone that is in that disk,disk,and because of the disk assumption or disk abstraction we just talked about,we can actually compute where the center of gravi ty is of our neighbors \", \"and the fourth one is if you do get around the obstacle there will be a point where you're going to start making progress again toward the goal location and you will probably no lee,no longer need to slide along the wall in order to make progress,so if those both tho,both of those conditions are,end up being true,then you can go ahead and switch back to your goal-seeking behavior,so which is either go to goal,or what i usually like do to is if,if i'm close to an obstacle,which i typically am because of the follow all behavior i first switch into avoid obstacles and go to goal before i switch into pure go to goal behavior \"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Quantity')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fc50cacb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences['length'] = sentences['Sentence'].map(lambda text: len(text))\n",
    "print(sentences.head())\n",
    "\n",
    "sentences.length.plot(bins=20, kind='hist')\n",
    "\n",
    "print(\"\\n\")\n",
    "sentences.length.describe()\n",
    "print(list(sentences.Sentence[sentences.length > 500]))\n",
    "\n",
    "#sentences.hist(column='length', by='Label', bins='auto',figsize=(10, 15), color=\"c\")\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [provided, some, conditions, are, satisfied, t...\n",
       "1       [but, how, am, i, supposed, to, find, those, m...\n",
       "2       [we, 've, actually, alreaOMITTED, done, this, ...\n",
       "3                      [here, 's, a, frour-step, process]\n",
       "4       [first, differentiate, your, function, can, fi...\n",
       "5       [those, are, places, where, the, derivative, i...\n",
       "6       [and, also, lists, the, end, points, if, they,...\n",
       "7                           [check, those, points, right]\n",
       "8       [check, the, end, points, check, the, critical...\n",
       "9                            [let, 's, work, an, example]\n",
       "10                     [okay, let, 's, work, an, example]\n",
       "11      [let, 's, look, at, the, function, given, by, ...\n",
       "12      [but, let, 's, only, consider, this, function,...\n",
       "13      [let, 's, consider, a, maximum, minimum, value...\n",
       "14                           [and, now, i, differentiate]\n",
       "15      [yeah, the, first, step, is, to, differentiate...\n",
       "16      [so, before, i, differentiate, it, i, 'm, just...\n",
       "17      [instead, of, over, OMITTED, squared, minus, s...\n",
       "18      [it, 's, going, to, make, it, a, little, bit, ...\n",
       "19      [so, what, 's, the, derivative, of, this, func...\n",
       "20      [by, the, power, rule, and, the, chain, rule, ...\n",
       "21      [with, the, derivative, in, hand, i, can, find...\n",
       "22      [yeah, the, next, step, is, to, list, the, cri...\n",
       "23                                     [so, let, 's, see]\n",
       "24      [what, are, the, critical, points, of, this, f...\n",
       "25      [will, those, be, places, where, the, derivati...\n",
       "26      [let, 's, first, think, about, when, the, deri...\n",
       "27      [well, in, order, for, that, derivative, to, b...\n",
       "28               [when, 's, a, fraction, equal, to, zero]\n",
       "29      [well, that, occurred, exactly, when, the, num...\n",
       "                              ...                        \n",
       "8350    [i, just, gave, you, the, omegas, essentially,...\n",
       "8351    [when, you, code, it, i, highly, encourage, ju...\n",
       "8352    [and, you, want, to, solve, your, attitude, an...\n",
       "8353    [later, on, in, class, we, can, even, add, oth...\n",
       "8354    [so, when, you, were, just, asking, that, ques...\n",
       "8355    [but, are, these, is, constant, because, we, d...\n",
       "8356                          [these, are, just, tensors]\n",
       "8357    [i, mean, if, you, say, constant, i, have, to,...\n",
       "8358    [the, inertia, is, not, a, matrix, like, a, dc...\n",
       "8359           [we, could, have, picked, something, else]\n",
       "8360         [this, is, really, fundamentally, a, tensor]\n",
       "8361    [which, means, just, like, a, vector, it, when...\n",
       "8362    [so, i, just, said, look, this, been, a, compl...\n",
       "8363        [you, break, it, up, into, two, parts, right]\n",
       "8364    [here, i, can, immediately, say, this, part, i...\n",
       "8365    [and, this, derivative, body, frame, is, the, ...\n",
       "8366    [so, this, OMITTED, OMITTED, frame, derivative...\n",
       "8367    [could, then, to, get, the, inertial, derivati...\n",
       "8368    [so, this, gives, me, the, answer, but, i, 've...\n",
       "8369    [remember, when, we, 're, using, transport, th...\n",
       "8370    [it, does, n't, mean, you, have, to, write, th...\n",
       "8371    [often, we, have, that, 's, why, we, 've, chos...\n",
       "8372    [here, we, have, n't, chosen, the, OMITTED, fr...\n",
       "8373    [i, think, this, was, my, confusion, no, OMITT...\n",
       "8374                          [it, 's, constant, exactly]\n",
       "8375    [for, any, shape, with, asymmetries, and, what...\n",
       "8376                [the, only, thing, is, it, 's, rigid]\n",
       "8377    [that, 's, the, single, rigid, body, and, now,...\n",
       "8378    [you, can, see, this, looks, suspiciously, sim...\n",
       "8379    [but, then, all, this, there, 's, all, these, ...\n",
       "Name: Sentence, Length: 8380, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split a sent into its individual words\n",
    "def split_into_tokens(sent):\n",
    "    #sent = unicode(sent, 'utf8')  # convert bytes into proper unicode\n",
    "    return TextBlob(sent).words\n",
    "\n",
    "sentences.Sentence.apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [provided, some, condition, are, satisfied, th...\n",
       "1    [but, how, am, i, supposed, to, find, those, m...\n",
       "2    [we, 've, actually, alreaOMITTED, done, this, ...\n",
       "3                   [here, 's, a, frour-step, process]\n",
       "4    [first, differentiate, your, function, can, fi...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing\n",
    "def split_into_lemmas(sent):\n",
    "    words = TextBlob(sent).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "sentences.Sentence.head().apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5657\n"
     ]
    }
   ],
   "source": [
    "# Data to vectors\n",
    "\n",
    "# convert each message, represented as a list of tokens (lemmas) above, \n",
    "# into a vector that machine learning models can understand\n",
    "\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(sentences['Sentence'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all right,now we've got ta figure out,when is the derivative equal to zero,so i'm trying to solve this \n",
      "  (0, 5)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 191)\t1\n",
      "  (0, 1339)\t1\n",
      "  (0, 1734)\t1\n",
      "  (0, 1929)\t1\n",
      "  (0, 2175)\t1\n",
      "  (0, 2413)\t1\n",
      "  (0, 2695)\t1\n",
      "  (0, 3373)\t1\n",
      "  (0, 3518)\t1\n",
      "  (0, 4301)\t1\n",
      "  (0, 4650)\t1\n",
      "  (0, 4663)\t1\n",
      "  (0, 4967)\t1\n",
      "  (0, 5048)\t1\n",
      "  (0, 5084)\t1\n",
      "  (0, 5128)\t2\n",
      "  (0, 5233)\t1\n",
      "  (0, 5478)\t1\n",
      "  (0, 5514)\t1\n",
      "  (0, 5650)\t1\n",
      "(1, 5657)\n",
      "OMITTED appears 3 times, i appears twice etc\n",
      "5657\n",
      "5421\n"
     ]
    }
   ],
   "source": [
    "message1000 = sentences['Sentence'][999]\n",
    "print(message1000)\n",
    "\n",
    "bow1000 = bow_transformer.transform([message1000])\n",
    "print(bow1000)\n",
    "print(bow1000.shape)\n",
    "print(\"OMITTED appears 3 times, i appears twice etc\")\n",
    "\n",
    "featureList = bow_transformer.get_feature_names()\n",
    "finalFeatureList = []\n",
    "\n",
    "def removeNumsFromFeatureList(featureList):\n",
    "    isNum = re.compile(\"[0-9]\")\n",
    "    hasApostrophe = re.compile(\"'\\w+\")\n",
    "        \n",
    "    for feat in featureList:\n",
    "        if isNum.match(feat): pass\n",
    "        elif hasApostrophe.match(feat): pass\n",
    "        elif \"OMITTED\" in feat.upper(): pass\n",
    "        elif \"NUMBER\" in feat.upper(): pass\n",
    "        elif (\"-\" or \"+\") in feat.upper(): pass\n",
    "        else: finalFeatureList.append(feat)\n",
    "\n",
    "print(len(featureList))\n",
    "removeNumsFromFeatureList(featureList)\n",
    "print(len(finalFeatureList))\n",
    "\n",
    "#print(bow_transformer.get_feature_names()[5214])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (8380, 5657)\n",
      "number of non-zeros: 117146\n",
      "sparsity: 0.25%\n",
      "(8380, 5657)\n"
     ]
    }
   ],
   "source": [
    "sentences_bow = bow_transformer.transform(sentences['Sentence'])\n",
    "print('sparse matrix shape:', sentences_bow.shape)\n",
    "print('number of non-zeros:', sentences_bow.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * sentences_bow.nnz / (sentences_bow.shape[0] * sentences_bow.shape[1])))\n",
    "\n",
    "#after the counting, the term weighting and normalization can be done with TF-IDF, using scikit-learn's TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(sentences_bow)\n",
    "tfidf1000 = tfidf_transformer.transform(bow1000)\n",
    "#print(tfidf1451)\n",
    "\n",
    "# To transform the entire bag-of-words corpus into TF-IDF corpus at once:\n",
    "sentences_tfidf = tfidf_transformer.transform(sentences_bow)\n",
    "print(sentences_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RUN THE CLASSIFIER!\n",
    "# TIME CONSUMING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "# Using Multinomial Naive Bayes classifier\n",
    "%time label_classifier = MultinomialNB().fit(sentences_tfidf, sentences['Label'].fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: CD\n",
      "Actual: CD\n"
     ]
    }
   ],
   "source": [
    "print('Predicted:', label_classifier.predict(tfidf1000)[0])\n",
    "print('Actual:', sentences.Label[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 0.7960620525059666\n",
      "\n",
      "========Confusion matrix=========\n",
      " [[   0    0    2    0    0    0]\n",
      " [   0    3  314    0    0    0]\n",
      " [   0    0 6652    0    0    0]\n",
      " [   0    0  277    0    0    0]\n",
      " [   0    0  555    0    7    0]\n",
      " [   0    0  561    0    0    9]]\n",
      "(row=Actual, col=Predicted)\n",
      "\n",
      "_______________Multinomial Naive Bayes______________\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "                  0.00      0.00      0.00         2\n",
      "         AP       1.00      0.01      0.02       317\n",
      "         CD       0.80      1.00      0.89      6652\n",
      "         CM       0.00      0.00      0.00       277\n",
      "         EX       1.00      0.01      0.02       562\n",
      "         SM       1.00      0.02      0.03       570\n",
      "\n",
      "avg / total       0.80      0.80      0.71      8380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "all_predictions = label_classifier.predict(sentences_tfidf)\n",
    "#print(all_predictions)\n",
    "\n",
    "print('Accuracy:\\t', accuracy_score(sentences['Label'].fillna(\"\"), all_predictions))\n",
    "print('\\n========Confusion matrix=========\\n', confusion_matrix(sentences['Label'].fillna(\"\"), all_predictions))\n",
    "print('(row=Actual, col=Predicted)')\n",
    "\n",
    "print(\"\\n_______________Multinomial Naive Bayes______________\")\n",
    "print(classification_report(sentences['Label'].fillna(\"\"), all_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
