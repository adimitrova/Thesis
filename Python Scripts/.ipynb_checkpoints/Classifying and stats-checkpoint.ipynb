{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/data_science_python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from pathlib import Path\n",
    "from beautifultable import BeautifulTable\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate\n",
    "\n",
    "allRunsAccuracy = []\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                           Sentence\n",
      "0    CD  provided some conditions are satisfied,the ext...\n",
      "1    CD  but how am i supposed to find those maximum an...\n",
      "2    CD  we've actually alreaOMITTED done this in some ...\n",
      "3    CD                       here's a frour-step process \n",
      "4    AP  first,differentiate your function can find all...\n",
      "5    CD  those are places where the derivative is equal...\n",
      "6    CD  and also lists the end points if they're inclu...\n",
      "7    CD                          check those points,right \n",
      "8    AP  check the end points,check the critical points...\n",
      "9    EX                             let's work an example \n"
     ]
    }
   ],
   "source": [
    "# ADD THE FILE THAT IS NOT PREPROCESSED YET\n",
    "prefix = r\"C:\\Users\\ani\"\n",
    "sentences = pandas.read_csv(prefix + r'\\Dropbox\\Stuff\\university\\data\\250PerClass.csv', sep='|', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"Label\", \"Sentence\"], encoding = \"ISO-8859-1\")\n",
    "print(sentences[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainS = []\n",
    "testS = []\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "textsize = len(sentences)\n",
    "trainTestRatio = round(0.75 * textsize)\n",
    "\n",
    "def split_testTrain():\n",
    "    nrsplit = 0\n",
    "\n",
    "    print(\"Total sent: \",textsize, \" | TRAIN sent: \",trainTestRatio,\" | TEST sent: \",textsize-trainTestRatio)\n",
    "\n",
    "    while not len(sentences) == 0:\n",
    "        nrsplit += 1\n",
    "        if nrsplit <= trainTestRatio:\n",
    "            curLine = random.choice(list(sentences[\"Label\"],sentences[\"Sentence\"]))\n",
    "            print(curLine)\n",
    "            sentences.remove(curLine)\n",
    "            trainS.append(curLine) \n",
    "        elif nrsplit > trainTestRatio:\n",
    "            curLine = random.choice(sentences)\n",
    "            sentences.remove(curLine)\n",
    "            testS.append(curLine) \n",
    "\n",
    "    print(\"Sent: \",len(sentences),\" | Train: \",len(trainS),\" | Test:\",len(testS))   \n",
    "    print(\"TRAIN: \",len(trainS),\" | TEST:\",len(testS))   \n",
    "    \n",
    "    for sent in trainS:\n",
    "        parts = sent.split('|')\n",
    "        if len(parts) > 1:\n",
    "            label = parts[0]\n",
    "            sent = parts[1]\n",
    "            tup = (sent,label)\n",
    "            train.append(tup)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for sent in testS:\n",
    "        parts = sent.split('|')\n",
    "        if len(parts) > 1:\n",
    "            label = parts[0]\n",
    "            sent = parts[1]\n",
    "            tup = (sent,label)\n",
    "            test.append(tup)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "split_testTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>to find out whether it's if it's true basicall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>250</td>\n",
       "      <td>248</td>\n",
       "      <td>NUMBER seconds after the ball was released,the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CM</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>the first transform is translate,which is a ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EX</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>well,here's the graph for our funciton f. and,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SM</th>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>so do play around with that one and we need to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence                                                               \n",
       "         count unique                                                top freq\n",
       "Label                                                                        \n",
       "AP         250    250  to find out whether it's if it's true basicall...    1\n",
       "CD         250    248  NUMBER seconds after the ball was released,the...    2\n",
       "CM         250    250  the first transform is translate,which is a ba...    1\n",
       "EX         250    250  well,here's the graph for our funciton f. and,...    1\n",
       "SM         250    250  so do play around with that one and we need to...    1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.groupby('Label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                           Sentence  length\n",
      "0    CD  provided some conditions are satisfied,the ext...     119\n",
      "1    CD  but how am i supposed to find those maximum an...      63\n",
      "2    CD  we've actually alreaOMITTED done this in some ...     131\n",
      "3    CD                       here's a frour-step process       28\n",
      "4    AP  first,differentiate your function can find all...      67\n",
      "\n",
      "\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Quantity')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x234827363c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences['length'] = sentences['Sentence'].map(lambda text: len(text))\n",
    "print(sentences.head())\n",
    "\n",
    "sentences.length.plot(bins=20, kind='hist')\n",
    "\n",
    "print(\"\\n\")\n",
    "sentences.length.describe()\n",
    "print(list(sentences.Sentence[sentences.length > 500]))\n",
    "\n",
    "#sentences.hist(column='length', by='Label', bins='auto',figsize=(10, 15), color=\"c\")\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [provided, some, conditions, are, satisfied, t...\n",
       "1       [but, how, am, i, supposed, to, find, those, m...\n",
       "2       [we, 've, actually, alreaOMITTED, done, this, ...\n",
       "3                      [here, 's, a, frour-step, process]\n",
       "4       [first, differentiate, your, function, can, fi...\n",
       "5       [those, are, places, where, the, derivative, i...\n",
       "6       [and, also, lists, the, end, points, if, they,...\n",
       "7                           [check, those, points, right]\n",
       "8       [check, the, end, points, check, the, critical...\n",
       "9                            [let, 's, work, an, example]\n",
       "10                     [okay, let, 's, work, an, example]\n",
       "11      [let, 's, look, at, the, function, given, by, ...\n",
       "12      [but, let, 's, only, consider, this, function,...\n",
       "13      [let, 's, consider, a, maximum, minimum, value...\n",
       "14                           [and, now, i, differentiate]\n",
       "15      [yeah, the, first, step, is, to, differentiate...\n",
       "16      [so, before, i, differentiate, it, i, 'm, just...\n",
       "17      [instead, of, over, OMITTED, squared, minus, s...\n",
       "18      [it, 's, going, to, make, it, a, little, bit, ...\n",
       "19      [so, what, 's, the, derivative, of, this, func...\n",
       "20      [by, the, power, rule, and, the, chain, rule, ...\n",
       "21      [with, the, derivative, in, hand, i, can, find...\n",
       "22      [yeah, the, next, step, is, to, list, the, cri...\n",
       "23                                     [so, let, 's, see]\n",
       "24      [what, are, the, critical, points, of, this, f...\n",
       "25      [will, those, be, places, where, the, derivati...\n",
       "26      [let, 's, first, think, about, when, the, deri...\n",
       "27      [well, in, order, for, that, derivative, to, b...\n",
       "28               [when, 's, a, fraction, equal, to, zero]\n",
       "29      [well, that, occurred, exactly, when, the, num...\n",
       "                              ...                        \n",
       "1220    [last, the, integral, the, accumulated, error,...\n",
       "1221    [and, the, other, thing, you, want, to, see, i...\n",
       "1222    [you, can, see, there, 's, a, little, bit, but...\n",
       "1223    [and, what, you, want, to, edit, is, this, var...\n",
       "1224    [my, tips, for, this, week, are, the, same, as...\n",
       "1225    [just, make, sure, to, read, the, week, sectio...\n",
       "1226    [and, use, as, many, fprintf, statements, as, ...\n",
       "1227    [and, this, obstacle, avoidance, controller, t...\n",
       "1228    [so, when, i, tell, you, that, the, robot, is,...\n",
       "1229    [and, this, is, important, because, what, we, ...\n",
       "1230    [so, in, order, for, you, to, be, able, to, ca...\n",
       "1231    [and, once, you, 've, done, that, properly, of...\n",
       "1232    [and, then, you, have, to, figure, out, the, p...\n",
       "1233    [so, play, around, with, that, and, you, shoul...\n",
       "1234    [and, again, here, i, 'm, going, to, here, i, ...\n",
       "1235    [and, then, we, 're, just, going, to, sum, the...\n",
       "1236    [and, then, just, like, last, weekend, the, go...\n",
       "1237    [my, tips, for, this, week, are, again, to, re...\n",
       "1238    [so, the, two, techniques, that, we, 're, goin...\n",
       "1239    [and, in, blending, what, we, 're, going, to, ...\n",
       "1240    [then, what, we, 're, also, going, to, test, i...\n",
       "1241    [but, you, 're, free, to, design, the, state, ...\n",
       "1242    [now, my, tips, for, this, week, are, as, usua...\n",
       "1243                      [i, talked, about, progression]\n",
       "1244    [so, is, our, go, to, goal, vector, we, 're, a...\n",
       "1245    [and, just, a, quick, idea, what, are, these, ...\n",
       "1246             [the, first, one, is, dynamical, models]\n",
       "1247    [this, is, really, nothing, but, a, differenti...\n",
       "1248    [first, is, called, the, initial, condition, c...\n",
       "1249                       [what, are, dynamical, models]\n",
       "Name: Sentence, Length: 1250, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split a sent into its individual words\n",
    "def split_into_tokens(sent):\n",
    "    #sent = unicode(sent, 'utf8')  # convert bytes into proper unicode\n",
    "    return TextBlob(sent).words\n",
    "\n",
    "sentences.Sentence.apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [provided, some, condition, are, satisfied, th...\n",
       "1    [but, how, am, i, supposed, to, find, those, m...\n",
       "2    [we, 've, actually, alreaOMITTED, done, this, ...\n",
       "3                   [here, 's, a, frour-step, process]\n",
       "4    [first, differentiate, your, function, can, fi...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing\n",
    "def split_into_lemmas(sent):\n",
    "    words = TextBlob(sent).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "sentences.Sentence.head().apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2135\n"
     ]
    }
   ],
   "source": [
    "# Data to vectors\n",
    "\n",
    "# convert each message, represented as a list of tokens (lemmas) above, \n",
    "# into a vector that machine learning models can understand\n",
    "\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(sentences['Sentence'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of features:  2135\n",
      "Nr of features after cleaning:  2078\n"
     ]
    }
   ],
   "source": [
    "#message1000 = sentences['Sentence'][999]\n",
    "#print(message1000)\n",
    "#\n",
    "#bow1000 = bow_transformer.transform([message1000])\n",
    "#print(bow1000)\n",
    "#print(bow1000.shape)\n",
    "#print(\"OMITTED appears 3 times, i appears twice etc\")\n",
    "\n",
    "featureList = bow_transformer.get_feature_names()\n",
    "finalFeatureList = []\n",
    "\n",
    "def removeNumsFromFeatureList(featureList):\n",
    "    isNum = re.compile(\"[0-9]\")\n",
    "    hasApostrophe = re.compile(\"'\\w+\")\n",
    "        \n",
    "    for feat in featureList:\n",
    "        if isNum.match(feat): pass\n",
    "        elif hasApostrophe.match(feat): pass\n",
    "        elif \"OMITTED\" in feat.upper(): pass\n",
    "        elif \"NUMBER\" in feat.upper(): pass\n",
    "        elif (\"-\" or \"+\") in feat.upper(): pass\n",
    "        else: finalFeatureList.append(feat)\n",
    "\n",
    "print(\"Nr of features: \",len(featureList))\n",
    "removeNumsFromFeatureList(featureList)\n",
    "print(\"Nr of features after cleaning: \",len(finalFeatureList))\n",
    "\n",
    "#print(bow_transformer.get_feature_names()[5214])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (1250, 2135)\n",
      "number of non-zeros: 18191\n",
      "sparsity: 0.68%\n",
      "(1250, 2135)\n"
     ]
    }
   ],
   "source": [
    "sentences_bow = bow_transformer.transform(sentences['Sentence'])\n",
    "print('sparse matrix shape:', sentences_bow.shape)\n",
    "print('number of non-zeros:', sentences_bow.nnz)\n",
    "print('sparsity: %.2f%%' % (100.0 * sentences_bow.nnz / (sentences_bow.shape[0] * sentences_bow.shape[1])))\n",
    "\n",
    "#after the counting, the term weighting and normalization can be done with TF-IDF, using scikit-learn's TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(sentences_bow)\n",
    "#tfidf1000 = tfidf_transformer.transform(bow1000)\n",
    "#print(tfidf1451)\n",
    "\n",
    "# To transform the entire bag-of-words corpus into TF-IDF corpus at once:\n",
    "sentences_tfidf = tfidf_transformer.transform(sentences_bow)\n",
    "print(sentences_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE CLASSIFIER!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.99 ms\n"
     ]
    }
   ],
   "source": [
    "# Using Multinomial Naive Bayes classifier\n",
    "%time label_classifier = MultinomialNB().fit(sentences_tfidf, sentences['Label'].fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Predicted:', label_classifier.predict(tfidf1000)[0])\n",
    "#print('Actual:', sentences.Label[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========Confusion matrix=========\n",
      " [[205  16   1  13  15]\n",
      " [  1 221   1  14  13]\n",
      " [ 16  18 184  11  21]\n",
      " [ 11  21   0 211   7]\n",
      " [  8  12   3  13 214]]\n",
      "(row=Actual, col=Predicted)\n",
      "\n",
      "_______________Multinomial Naive Bayes________________\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         AP       0.85      0.82      0.84       250\n",
      "         CD       0.77      0.88      0.82       250\n",
      "         CM       0.97      0.74      0.84       250\n",
      "         EX       0.81      0.84      0.82       250\n",
      "         SM       0.79      0.86      0.82       250\n",
      "\n",
      "avg / total       0.84      0.83      0.83      1250\n",
      "\n",
      "Accuracy:\t 0.83\n",
      "______________________________________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_predictions = label_classifier.predict(sentences_tfidf)\n",
    "#print(all_predictions)\n",
    "\n",
    "print('\\n========Confusion matrix=========\\n', confusion_matrix(sentences['Label'].fillna(\"\"), all_predictions))\n",
    "print('(row=Actual, col=Predicted)')\n",
    "\n",
    "print(\"\\n_______________Multinomial Naive Bayes________________\\n\")\n",
    "print(classification_report(sentences['Label'].fillna(\"\"), all_predictions))\n",
    "print('Accuracy:\\t {0:.2f}'.format(accuracy_score(sentences['Label'].fillna(\"\"), all_predictions)))\n",
    "print(\"______________________________________________________\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
