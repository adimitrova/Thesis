{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do researchers deal with it:\n",
    "- [Word embeddings Wiki](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "- [Gensim Python library](https://en.wikipedia.org/wiki/Gensim)\n",
    "- [Inference Rules Wiki](https://en.wikipedia.org/wiki/Rule_of_inference)\n",
    "\n",
    "### LSTM: \n",
    "- [Long-Short term memory (LSTM)](https://www.datacamp.com/community/tutorials/lstm-python-stock-market#lstm)\n",
    "- [Learn via example](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "\n",
    "### Literature:\n",
    "- [Intent extraction from social media texts using sequential segmentation and deep learning models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8119461) uses CRFs and Bi-LSTM for intent extraction from texts from social media in 2 categories - Cosmetics and Tourism. Look into these algos\n",
    "    - Citation: \n",
    "`@INPROCEEDINGS{8119461, \n",
    "author={T. L. Luong and M. S. Cao and D. T. Le and X. H. Phan}, \n",
    "booktitle={2017 9th International Conference on Knowledge and Systems Engineering (KSE)}, \n",
    "title={Intent extraction from social media texts using sequential segmentation and deep learning models}, \n",
    "year={2017}, \n",
    "pages={215-220}, \n",
    "doi={10.1109/KSE.2017.8119461}, \n",
    "month={Oct},}`\n",
    "\n",
    "\n",
    "- In [Semantic Indexing for Recorded Educational Lecture Videos](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1598977) they extracted scripts from videos with timestamps on each word and cluster them in order to allow for finding of the exact position of a particular thing in the video. They also use a retrieval method to find “example”, “explanation”, “overview”, “repetition”, “exercise” for a particular word or topic word. \n",
    "    - Citation: `@INPROCEEDINGS{1598977, \n",
    "author={S. Repp and M. Meinel}, \n",
    "booktitle={Fourth Annual IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOMW'06)}, \n",
    "title={Semantic indexing for recorded educational lecture videos}, \n",
    "year={2006}, \n",
    "pages={5 pp.-245}, \n",
    "month={March},}`\n",
    "\n",
    "\n",
    "- In [Olex: Effective Rule Learning for Text Categorization](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4641927) Sees the problem as a text classification task and applied Inference Rules onto it. Not particularly for intent mining, but for different categories, similar to what I have. The inference rules are of the form: \\begin{equation}If \\space T_1 \\space or \\space \\dots \\space T_n \\space occurs \\space in \\space document \\space d,\\space and \\space none \\space of \\space T_{n+1} \\dots T_{n+m} \\space occurs \\space in \\space d, \\space then \\space classify \\space d \\space under \\space category \\space C \\end{equation}  This includes `one` positive literal and `0+` negative literals and temrs are `n-grams`\n",
    "\n",
    "    - Citation `@ARTICLE{4641927, \n",
    "author={P. Rullo and V. L. Policicchio and C. Cumbo and S. Iiritano}, \n",
    "journal={IEEE Transactions on Knowledge and Data Engineering}, \n",
    "title={Olex: Effective Rule Learning for Text Categorization}, \n",
    "year={2009}, \n",
    "volume={21}, \n",
    "number={8}, \n",
    "pages={1118-1132}, \n",
    "doi={10.1109/TKDE.2008.206}, \n",
    "ISSN={1041-4347}, \n",
    "month={Aug},}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Rules method\n",
    "\n",
    "### General Rules\n",
    "1. If sentence has no label, proceed with label search.\n",
    "2. If no label can be assigned, assign the last applied labeled from a previous sentence\n",
    "\n",
    "-----------------------------------\n",
    "### ALL RULES\n",
    "- EX <<< `example` || OR `for instance` || `assume` || `suppose` || `imagine` || `as` || `simulation` || `diagram` [✔] \n",
    "- EX <<< `Let's` && try || think || see || pick || take a look || say .. [✔]\n",
    "- CD <<< `Let's` && look at || make || put || do || start || prove || evaluate || back || try || just && NOT `example` || `assume` || `suppose` || `imagine`\n",
    "- SM <<< `Let's` && summarize\n",
    "- SM <<< `in other words` && past tense\n",
    "- SM <<< `later` || `next time` || `last time` || `summary` || `summarize`\n",
    "- SM <<< if (lineNr < 10 `OR` lineNr > fileLinesNr - 10) `&&` (past tense) =>> (within the first or last 10 lines + past tense)\n",
    "- AP <<< `in other words` && should\n",
    "- AP <<< `best practice(s)`\n",
    "- CD <<< `in other words` && present tense\n",
    "- CD <<< `so` && `it's` || `i'm`\n",
    "- CD <<< `so this is` || `actually` && NOT `example` || `summary` || `next` || `last`\n",
    "- CM <<< `called` && concept\n",
    "- CM <<< `what is` .. && concept\n",
    "- CM <<< `theorem` || `algorithm` || `method`\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "### Logical expressions (copy/paste in thesis later - LATEX style)\n",
    "#### ♦ EXAMPLE\n",
    "1. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"let's\" \\in d \\space) \\space \\land (\"try\" \\in d \\space \\lor \"see\" \\in d \\space \\lor \"think\" \\in d \\space \\lor \"pick\" \\in d \\space \\lor \"say\" \\in d) \\end{equation} \n",
    "\n",
    "2. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"example\" \\in d \\space) \\lor (\"for \\space instance\" \\in d) \\space \\lor (\"suppose\" \\in d) \\space \\lor (\"assume\" \\in d) \\space \\lor (\"includes\" \\in d) \\space \\lor (\"imagine\" \\in d) \\space \\end{equation} \n",
    "\n",
    "Latex Formula Formatter: https://www.codecogs.com/eqnedit.php\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pseudocode\n",
    "\n",
    "`Disregard all sentences that have NL label, totally ignore, then:\n",
    "    if sentence has no label:\n",
    "        for line in text:\n",
    "            if lineNR < 10 OR lineNR > nrOfLines-10:\n",
    "                for word in line:\n",
    "                    if (60%+ of the words on the line are in PAST TENSE):\n",
    "                        go over the SM rules  (append res to curSentLabels)\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in PRESENT TENSE):\n",
    "                        go over the CD rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in FUTURE TENSE):\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "            if lineNR > 10 AND lineNR < nrOfLines-10:\n",
    "                go over the EX rules  (append res to curSentLabels)\n",
    "                go over the CD rules  (append res to curSentLabels)\n",
    "                go over the AP rules  (append res to curSentLabels)\n",
    "                go over the CM rules  (append res to curSentLabels)\n",
    "        Count the labels with a special method for this: [✔]\n",
    "            if all rules fail to assign a label, i.e. if all labels return count 0:\n",
    "                search for the last labeled sentence:\n",
    "                    assign its label to the current sentence`                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the tense of the verbs in the sentence\n",
    "\n",
    "- [All POS Tags](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('is', 'VBZ')\n",
      "('going', 'VBG')\n",
      "('use', 'VB')\n",
      "('are', 'VBP')\n",
      "('going', 'VBG')\n",
      "('use', 'VB')\n",
      "Past tense: 0.00 %\n",
      "All verbs:  6\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = nltk.word_tokenize(\"\"\"so this is something that mathew's going to use and that nick and i are going to use in the examples a bit later in this lesson \"\"\")\n",
    "\n",
    "\"\"\"\n",
    "VBD\t\tverb, past tense\t\t\t\t\ttook\n",
    "VBN\t\tverb, past participle\t\t\t\ttaken\n",
    "VB\t\tverb, base form\t\t\t\t\t\ttake\n",
    "VBG\t\tverb, gerund/present participle\t\ttaking\n",
    "VBP\t\tverb, sing. present, non-3d\t\t\ttake\n",
    "VBZ\t\tverb, 3rd person sing. present\t\ttakes\n",
    "\"\"\"\n",
    "\n",
    "verbs = 0\n",
    "pastTense = 0\n",
    "\n",
    "textPOS = nltk.pos_tag(text)\n",
    "for tag in textPOS:\n",
    "    if str(tag[1]).startswith(\"V\"):\n",
    "        print(tag)\n",
    "        verbs += 1\n",
    "        if str(tag[1]) == \"VBD\" or str(tag[1]) == \"VBN\":\n",
    "            pastTense += 1\n",
    "            \n",
    "percPastTense = (pastTense / verbs) * 100\n",
    "\n",
    "print(\"Past tense: {0:.2f} %\".format(percPastTense))\n",
    "print(\"All verbs: \",verbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Inference Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RULES #####\n",
    "\n",
    "### ------------------EX--------------------\n",
    "def EX_dictSearch(sent):\n",
    "    nrOfWordsFound = 0\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"example\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"for instance\" in bigrams: nrOfWordsFound += 1\n",
    "    elif \"assume\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"suppose\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"imagine\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"as\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"simulation\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"diagram\" in monograms: nrOfWordsFound += 1\n",
    "        \n",
    "    if nrOfWordsFound > 0:\n",
    "        return \"EX\"\n",
    "    else: \n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### ------------------EX--------------------\n",
    "    \n",
    "def EX_lets(sent):\n",
    "    mainWordNR = 0      # Let's\n",
    "    secondaryWordsNR = 0     \n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if \"let's\" in bigrams or \"let 's\" in bigrams: mainWordNR += 1\n",
    "    \n",
    "    if \"try\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"think\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"see\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"pick\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"take a look\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"say\" in monograms: secondaryWordsNR += 1\n",
    "        \n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0:\n",
    "        return \"EX\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### -------------------CD-------------------\n",
    "\n",
    "def CD_lets(sent):\n",
    "    mainWordNR = 0      # Let's     # main word looking for in conjunction with one or more of the secondary words\n",
    "    secondaryWordsNR = 0 \n",
    "    negWords = 0      # words that must NOT occur for the label to apply, i.e. this should stay at ZERO\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"let's\" in bigrams or \"let 's\" in bigrams: mainWordNR += 1\n",
    "    \n",
    "    if \"look at\" in bigrams: secondaryWordsNR += 1\n",
    "    elif \"make\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"put\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"do\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"start\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"prove\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"back\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"try\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"just\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"be\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"take\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"bring\" in monograms: secondaryWordsNR += 1\n",
    "\n",
    "    if \"example\" in monograms: negWords += 1\n",
    "    if \"diagram\" in monograms: negWords += 1\n",
    "    if \"assume\" in monograms: negWords += 1\n",
    "    if \"imagine\" in monograms: negWords += 1\n",
    "    if \"suppose\" in monograms: negWords += 1\n",
    "        \n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0 and negWords == 0:\n",
    "        return \"CD\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EX'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSent = \"so let's just get rid of the ones we don't need, for example\"\n",
    "res = EX_dictSearch(testSent)\n",
    "# ----------- disregard the above test ---------\n",
    "\n",
    "def getFinalLabel(curSentLabels):     # gets the list of assigned labels after all rules have been checked\n",
    "    EX,AP,CD,CM,SM, maxCount = 0, 0, 0, 0, 0, 0\n",
    "    labels = []\n",
    "    maxLabel = \"\"\n",
    "    \n",
    "    # counting the labels returned from the rules checks\n",
    "    for item in curSentLabels:\n",
    "        if item == \"EX\": EX += 1\n",
    "        elif item == \"AP\": AP += 1\n",
    "        elif item == \"CD\": CD += 1\n",
    "        elif item == \"CM\": CM += 1\n",
    "        elif item == \"SM\": SM += 1\n",
    "        else: pass   #pass NOLBL items\n",
    "\n",
    "    # adding the labels into a list of items to make it easy to get the max value\n",
    "    labels.append(\"EX,\"+str(EX))\n",
    "    labels.append(\"AP,\"+str(AP))\n",
    "    labels.append(\"CD,\"+str(CD))\n",
    "    labels.append(\"CM,\"+str(CM))\n",
    "    labels.append(\"SM,\"+str(SM))\n",
    "\n",
    "    # getting the max value \n",
    "    for item in labels:\n",
    "        parts = item.split(\",\")\n",
    "        label = parts[0]\n",
    "        count = int(parts[1])\n",
    "        if count > 0:\n",
    "            if count > maxCount:\n",
    "                maxCount = count\n",
    "                maxLabel = label\n",
    "\n",
    "    return maxLabel\n",
    "\n",
    "getFinalLabel(testSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ---------- Global variables ----------------------------------------\n",
    "correctLabels = 0\n",
    "totalSentences = 0\n",
    "accuracy = correctLabels / totalSentences\n",
    "\n",
    "### ---------- Local variables - reset per file ------------------------\n",
    "lineNR = 0\n",
    "totalLines = 0\n",
    "curSentLabels = []          # All the labels assigned to the current sentence (to get majority vote from it later)\n",
    "\n",
    "originalLabel = \"\"          # label from the sentence\n",
    "majorityLabel = \"\"          # label assigned after the rules application\n",
    "majorityLabelUsers = \"\"     # label assigned by other users (majority vote)\n",
    "##### ----------------------------------------------------------------------------------------------------- #####\n",
    "\n",
    "ignore_dict = ['inaudible','OMITTED','NUMBER','sound','music','laughter','yeah','blank_audio']\n",
    "\n",
    "curSentLabels.append(EX_dictSearch(testSent))\n",
    "curSentLabels.append(EX_lets(testSent))\n",
    "curSentLabels.append(CD_lets(testSent))\n",
    "\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# label_sentences(takesAFile)\n",
    "# get_ngrams(takesASentence)\n",
    "# ruleX(takesASentence)\n",
    "\n",
    "def label_sentences(sentenceList):\n",
    "    sentences = sentenceList\n",
    "    sent_processed = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        #Split each senetnce to bi- and tri- grams\n",
    "        monograms = get_ngrams(sent, 1)\n",
    "        bigrams = get_ngrams(sent, 2)\n",
    "        trigrams = get_ngrams(sent, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
