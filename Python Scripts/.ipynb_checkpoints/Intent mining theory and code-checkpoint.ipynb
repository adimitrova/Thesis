{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How do researchers deal with it:\n",
    "- [Word embeddings Wiki](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "- [Gensim Python library](https://en.wikipedia.org/wiki/Gensim)\n",
    "- [Inference Rules Wiki](https://en.wikipedia.org/wiki/Rule_of_inference)\n",
    "\n",
    "### LSTM: \n",
    "- [Long-Short term memory (LSTM)](https://www.datacamp.com/community/tutorials/lstm-python-stock-market#lstm)\n",
    "- [Learn via example](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "\n",
    "### Literature:\n",
    "- [Intent extraction from social media texts using sequential segmentation and deep learning models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8119461) uses CRFs and Bi-LSTM for intent extraction from texts from social media in 2 categories - Cosmetics and Tourism. Look into these algos\n",
    "    - Citation: \n",
    "`@INPROCEEDINGS{8119461, \n",
    "author={T. L. Luong and M. S. Cao and D. T. Le and X. H. Phan}, \n",
    "booktitle={2017 9th International Conference on Knowledge and Systems Engineering (KSE)}, \n",
    "title={Intent extraction from social media texts using sequential segmentation and deep learning models}, \n",
    "year={2017}, \n",
    "pages={215-220}, \n",
    "doi={10.1109/KSE.2017.8119461}, \n",
    "month={Oct},}`\n",
    "\n",
    "\n",
    "- In [Semantic Indexing for Recorded Educational Lecture Videos](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1598977) they extracted scripts from videos with timestamps on each word and cluster them in order to allow for finding of the exact position of a particular thing in the video. They also use a retrieval method to find “example”, “explanation”, “overview”, “repetition”, “exercise” for a particular word or topic word. \n",
    "    - Citation: `@INPROCEEDINGS{1598977, \n",
    "author={S. Repp and M. Meinel}, \n",
    "booktitle={Fourth Annual IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOMW'06)}, \n",
    "title={Semantic indexing for recorded educational lecture videos}, \n",
    "year={2006}, \n",
    "pages={5 pp.-245}, \n",
    "month={March},}`\n",
    "\n",
    "\n",
    "- In [Olex: Effective Rule Learning for Text Categorization](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4641927) Sees the problem as a text classification task and applied Inference Rules onto it. Not particularly for intent mining, but for different categories, similar to what I have. The inference rules are of the form: \\begin{equation}If \\space T_1 \\space or \\space \\dots \\space T_n \\space occurs \\space in \\space document \\space d,\\space and \\space none \\space of \\space T_{n+1} \\dots T_{n+m} \\space occurs \\space in \\space d, \\space then \\space classify \\space d \\space under \\space category \\space C \\end{equation}  This includes `one` positive literal and `0+` negative literals and temrs are `n-grams`\n",
    "\n",
    "    - Citation `@ARTICLE{4641927, \n",
    "author={P. Rullo and V. L. Policicchio and C. Cumbo and S. Iiritano}, \n",
    "journal={IEEE Transactions on Knowledge and Data Engineering}, \n",
    "title={Olex: Effective Rule Learning for Text Categorization}, \n",
    "year={2009}, \n",
    "volume={21}, \n",
    "number={8}, \n",
    "pages={1118-1132}, \n",
    "doi={10.1109/TKDE.2008.206}, \n",
    "ISSN={1041-4347}, \n",
    "month={Aug},}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Rules method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Theory\n",
    "### General Rules\n",
    "1. If sentence has no label, proceed with label search.\n",
    "2. If no label can be assigned, assign the last applied labeled from a previous sentence\n",
    "\n",
    "-----------------------------------\n",
    "### ALL RULES\n",
    "- [✔] EX_1 <<< `example` || OR `for instance` || `assume` || `suppose` || `imagine` || `as` || `simulation` || `diagram` \n",
    "- [✔] EX_2 <<< `Let's` __&&__ try || think || see || pick || take a look || say ..\n",
    "- [✔] CD_1 <<< `Let's` __&&__ look at || make || put || do || start || prove || evaluate || back || try || just __AND NO__ `example` || `assume` || `suppose` || `imagine` || `diagram`\n",
    "- [✔] CD_2 <<< `in other words` || `basically` __AND NO__ `should` || `have to` || `must` \n",
    "- [✔] CD_3 <<< `so` is the first word in the sentence __&&__ `it's` || `i'm`\n",
    "- [✔] CD_4 <<< `so this is` || `actually` __AND NO__ `example` || `summary` || `next` || `last`\n",
    "- [✔] CD_5 <<< `means` || `mean` || `given` || `define` || `explain` __AND NO__ Present continuous tense (going to)\n",
    "- [✔] CD_6 <<< `what if` __AND NO__ `example` || `instance`\n",
    "- [✔] SM_1 <<< `Let's` __&&__ summarize || `recap` \n",
    "- [✔] SM_2 <<< `in other words` __&&__ past tense\n",
    "- [✔] SM_3 <<< `this week` || `this lesson` || `today` __&&__ present tense || future tense\n",
    "- [✔] SM_4 <<< `later` || `next time` || `last time` || `summary` || `summarize` || `here is` || `here are` || `discuss` || `next` \n",
    "- [✔] SM_5 <<< if (lineNr < 10 `OR` lineNr > fileLinesNr - 10) __&&__ (past tense) =>> (within the first or last 10 lines + past tense)\n",
    "- [✔] SM_6 <<< `going to` __&&__ `look` || `see` || `be` || `think` || `explain` || `explained` __AND NO__ present tense (&& future or past)\n",
    "- [✔] AP_1 <<< `in other words` __&&__ `should` || `could` || `would` [✔]\n",
    "- [✔] AP_2 <<< `encourage` || `step` || `first` || `finally` || `second` || `should` || `could` || `would` || `best practice(s)` || `need to`\n",
    "- [✔] AP_3 <<< `if` __&&__ `use` || `can` || `should` || `could` || `want`\n",
    "- [✔] CM_1 <<< `called` __&&__ concept\n",
    "- [✔] CM_2 <<< `what is` .. __&&__ concept\n",
    "- [✔] CM_3 <<< `theorem` || `algorithm` || `method` || `let's use` || `theory`\n",
    "- [✔] CM_4 <<< first occurence of the terms in the title of the file\n",
    "- [**X**] CM_5 <<< `let's` __&&__ `use` - **REPEATS CM_3** \n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "### Logical expressions (copy/paste in thesis later - LATEX style)\n",
    "#### ♦ EXAMPLE\n",
    "1. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"let's\" \\in d \\space) \\space \\land (\"try\" \\in d \\space \\lor \"see\" \\in d \\space \\lor \"think\" \\in d \\space \\lor \"pick\" \\in d \\space \\lor \"say\" \\in d) \\end{equation} \n",
    "\n",
    "2. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"example\" \\in d \\space) \\lor (\"for \\space instance\" \\in d) \\space \\lor (\"suppose\" \\in d) \\space \\lor (\"assume\" \\in d) \\space \\lor (\"includes\" \\in d) \\space \\lor (\"imagine\" \\in d) \\space \\end{equation} \n",
    "\n",
    "Latex Formula Formatter: https://www.codecogs.com/eqnedit.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pseudocode\n",
    "\n",
    "`Disregard all sentences that have NL label, totally ignore, then: [✔]\n",
    "    if sentence has no label:\n",
    "        for line in text:\n",
    "            if lineNR < 10 OR lineNR > nrOfLines-10:\n",
    "                for word in line:\n",
    "                    if (60%+ of the words on the line are in PAST TENSE):\n",
    "                        go over the SM rules  (append res to curSentLabels)\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in PRESENT TENSE):\n",
    "                        go over the CD rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in FUTURE TENSE):\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "            if lineNR > 10 AND lineNR < nrOfLines-10:\n",
    "                go over the EX rules  (append res to curSentLabels)\n",
    "                go over the CD rules  (append res to curSentLabels)\n",
    "                go over the AP rules  (append res to curSentLabels)\n",
    "                go over the CM rules  (append res to curSentLabels)\n",
    "        Count the labels with a special method for this: [✔]\n",
    "            if all rules fail to assign a label, i.e. if all labels return count 0:\n",
    "                search for the last labeled sentence:\n",
    "                    assign its label to the current sentence`                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [THEORY] Checking the tense of the verbs in the sentence\n",
    "\n",
    "- [All POS Tags](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "NLTK PPOS TAGS for verbs:\n",
    "\n",
    "- VBD\t\tverb, past tense\t\t\t\t\t`took`      +1 if checked for PAST, else 0\n",
    "- VBN\t\tverb, past participle\t\t\t\t`taken`     +1 if checked for PAST, else 0\n",
    "- VB\t\tverb, base form\t\t\t\t\t\t`take`      +0.5\n",
    "- VBG\t\tverb, gerund/present participle\t\t`taking`    +0.5\n",
    "- VBP\t\tverb, sing. present, non-3d\t\t\t`take`      +1 if checked for PRESENT, else 0\n",
    "- VBZ\t\tverb, 3rd person sing. present\t\t`takes`     +1 if checked for PRESENT, else 0\n",
    "- MD        modal verb (will, shall)            `will`      +1 if checked for FUTURE, else 0\n",
    "\n",
    "\n",
    "**Simply counting verbs isn't enough, artificial boost is added if certain types of verbs are present so that they\n",
    "form a specific English tense. All listed below:**\n",
    "\n",
    "- **Past perfect**: `VBD`(had) + `VBN`(been) ------- BUT NO VBG(-ing/gerund)\n",
    "- **Past continuous tense**: `VBD`(was/were) + `VBG`(-ing/gerund)\n",
    "- **Past perfect continuous**: `VBD`(had) + `VBN`(been) + `VBG`(-ing/gerund)\n",
    "- **PRESENT perfect**: `VBP`(have) + `VBN`(been) ------- BUT NO VBG(-ing/gerund)\n",
    "- **PRESENT perfect continuous**: `VBP`(have) + `VBN`(been) + `VBG`(-ing/gerund)\n",
    "- **PRESENT continuous**: `VBP`(is/are) + `VBG`(-ing/gerund)\n",
    "- **Future continuous**: `MD`(WILL) + `VBG`(-ing/gerund)\n",
    "- **Future perfect**: `MD`(will) + `VB`(have) + `VBN`(PP)\n",
    "- **Future perfect continuous**: `MD`(will) + `VBN`(been) + `VB`(have) + `VBG`(-ing/gerund)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Inference Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Check for present / past / future tense\n",
    "\n",
    "All return a number which is the percentage of likelihood that the sentence is of certain tense\n",
    "\n",
    "**Version 1** checked also for the more complicated tenses such as perfect, continuous etc of each present, past, future by assigning addiotnal scores according to whether all criteria is covered, in order to make the output higher and ==> more certain.\n",
    "\n",
    "**Version 2** only counts the types of verbs for each tense\n",
    "- checkPRES(sentence)\n",
    "- checkPAST(sentence)\n",
    "- checkFUTURE(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ======================================== PAST TENSE CHECK ==============================================\n",
    "\n",
    "puncDict = [\",\",\".\",\";\",\"-\",\"_\",\"`\",\"'\",\"?\",\"!\",\":\"]\n",
    "\n",
    "def checkPAST2(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, pastTense, grammarCase, VBGgerund, VBNpp, VBDHad = 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "#            print(tag)\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBD\": \n",
    "                pastTense += 1\n",
    "                VBDHad += 1\n",
    "            elif str(tag[1]) == \"VBN\": \n",
    "                pastTense += 1\n",
    "                VBNpp += 1\n",
    "            elif str(tag[1]) == \"VBG\": VBGgerund += 1\n",
    "            else: pass\n",
    "    \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense\n",
    "    # TODO re-evaluate points\n",
    "    #  1   1   0.5\n",
    "    # VBD VBN  VBG\n",
    "    #print(\"VBD \",VBDHad,\" VBN \",VBNpp,\" VBG \",VBGgerund)\n",
    "    if VBDHad > 0 and VBNpp > 0 and VBGgerund == 0: grammarCase += 2                   #Past perfect \n",
    "    elif VBDHad > 0 and VBNpp == 0 and VBGgerund > 0: grammarCase += 1.5                #Past continuous tense \n",
    "    elif VBDHad > 0 and VBNpp > 0 and VBGgerund > 0: grammarCase += 2.5                  #Past perfect continuous\n",
    "    else: grammarCase += 0\n",
    "        \n",
    "    if pastTense == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        #grammarCase = grammarCase * 0.1 \n",
    "        # big / small = 100 / x\n",
    "        #calculate verbs over words\n",
    "        ratio_verbs = len(text) / verbs\n",
    "        perc_verbs= (100 / ratio_verbs) \n",
    "\n",
    "        #calculate past tense verbs over all verbs\n",
    "        ratio_PastVerbs = verbs / pastTense\n",
    "        perc_PastVerbsOverVerbs = (100 / ratio_PastVerbs)\n",
    "\n",
    "        #calculate past tense verbs over all words\n",
    "        ratio_PastVerbsOverWords = len(text) / pastTense\n",
    "        perc_VerbsPastTenseOverWords = 100 / ratio_PastVerbsOverWords\n",
    "\n",
    "        \"\"\"\n",
    "        if perc_PastVerbsOverVerbs >= 30:\n",
    "            return perc_PastVerbsOverVerbs\n",
    "        else:\n",
    "            print(\"{0:.2f}\".format(perc_PastVerbsOverVerbs))\n",
    "            return \"NPAST\"\n",
    "        \"\"\"\n",
    "\n",
    "        if not grammarCase == 0:\n",
    "            return grammarCase * 100\n",
    "        else:\n",
    "            if perc_PastVerbsOverVerbs >= 30:\n",
    "                return perc_PastVerbsOverVerbs\n",
    "            else:\n",
    "                #print(\"{0:.2f}\".format(perc_PastVerbsOverVerbs))\n",
    "                return -1\n",
    "            \n",
    "            \n",
    "# ========================================= PRESENT TENSE CHECK =============================================\n",
    "    \n",
    "def checkPRESENT2(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, presTenseVerbs, grammarCase, VBGgerund, VBPhave, VBNbeen = 0, 0, 0, 0, 0, 0 \n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBP\": \n",
    "                presTenseVerbs += 1\n",
    "                VBPhave += 1\n",
    "            elif str(tag[1]) == \"VBN\": \n",
    "                VBNbeen += 1\n",
    "            elif str(tag[1]) == \"VBG\":\n",
    "                presTenseVerbs += 1\n",
    "                VBGgerund += 1\n",
    "            elif str(tag[1]) == \"VB\":\n",
    "                presTenseVerbs += 1\n",
    "            elif str(tag[1]) == \"VBZ\":\n",
    "                presTenseVerbs += 1\n",
    "            else: pass\n",
    "                \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense.\n",
    "    #print(\"VBG \",VBGgerund,\" VBP \",VBPhave,\" VBN \",VBNbeen)\n",
    "    # TODO re-evaluate points\n",
    "    # 0.5   1    0\n",
    "    # VBG  VBP  VBN\n",
    "    \n",
    "    if VBGgerund == 0 and VBPhave > 0 and VBNbeen > 0: grammarCase += 1                   # PRESENT perfect\n",
    "    elif VBGgerund > 0 and VBPhave > 0 and VBNbeen == 0: grammarCase += 1.5               # PRESENT continuous \n",
    "    elif VBGgerund > 0 and VBPhave > 0 and VBNbeen > 0: grammarCase += 1.5                # PRESENT perfect continuous\n",
    "    else: grammarCase += 0\n",
    "    \n",
    "    #grammarCase = grammarCase * 0.1\n",
    "    #calculate past tense verbs over all verbs\n",
    "    if presTenseVerbs == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_PresVerbs = verbs / presTenseVerbs\n",
    "        perc_PresVerbsOverVerbs = (100 / ratio_PresVerbs)\n",
    "\n",
    "        if not grammarCase == 0:\n",
    "            return grammarCase * 100\n",
    "        else:\n",
    "            if perc_PresVerbsOverVerbs >= 30:\n",
    "                return perc_PresVerbsOverVerbs\n",
    "            else:\n",
    "                #print(\"{0:.2f}\".format(perc_PresVerbsOverVerbs))\n",
    "                return -1\n",
    "\n",
    "\n",
    "# ======================================= FUTURE TENSE CHECK ===============================================\n",
    "\n",
    "def checkFUTURE2(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, futureTenseWords, grammarCase, MDmodal, VBGgerund, VBhave, VBNpp = 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBG\":\n",
    "                VBGgerund += 1\n",
    "                futureTenseWords += 1\n",
    "            elif str(tag[1]) == \"VB\":\n",
    "                VBhave += 1\n",
    "            elif str(tag[1]) == \"VBN\":\n",
    "                VBNpp += 1\n",
    "        if str(tag[1]) == \"MD\" and str(tag[0]) == \"will\" or str(tag[0]) == \"shall\":\n",
    "            MDmodal += 1\n",
    "            futureTenseWords += 1\n",
    "                \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense.\n",
    "    #print(\"VBG \",VBGgerund,\" VBP \",VBPhave,\" VBN \",VBNbeen)\n",
    "    # TODO re-evaluate points\n",
    "    # 0.5   0    0    1\n",
    "    # VBG   VB  VBN   MD\n",
    "    \n",
    "    if MDmodal > 0 and VBhave > 0 and VBNpp > 0: grammarCase += 1                         # Fut. perf.    MD/VB/VBN\n",
    "    elif MDmodal > 0 and VBGgerund > 0: grammarCase += 1.5                                # Fut. cont.  MD/VBG\n",
    "    elif MDmodal > 0 and VBhave > 0 and VBNpp > 0 and VBGgerund > 0: grammarCase += 1.5   # Fut. perf. cont. MD/VB/VBN/VBG\n",
    "    else: grammarCase += 0\n",
    "    \n",
    "    #grammarCase = grammarCase * 1.0\n",
    "    #calculate past tense verbs over all verbs\n",
    "    if futureTenseWords == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_FutureVerbs = verbs / futureTenseWords\n",
    "        perc_FutureWordsOverVerbs = (100 / ratio_FutureVerbs)\n",
    "\n",
    "        if not grammarCase == 0:\n",
    "            return grammarCase * 100\n",
    "        else:\n",
    "            if perc_FutureWordsOverVerbs >= 30:\n",
    "                return perc_FutureWordsOverVerbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Version 2 (USE THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ================================= SECOND VERSION OF TENSE CHECK ========================================\n",
    "\n",
    "# ======================================== PAST TENSE CHECK ==============================================\n",
    "\n",
    "puncDict = [\",\",\".\",\";\",\"-\",\"_\",\"`\",\"'\",\"?\",\"!\",\":\"]\n",
    "\n",
    "def checkPAST(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, pastTense, VBGgerund = 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "#            print(tag)\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBD\": pastTense += 1\n",
    "            elif str(tag[1]) == \"VBN\": pastTense += 1\n",
    "            elif str(tag[1]) == \"VBG\": pastTense += 1\n",
    "            else: pass\n",
    "        \n",
    "    if pastTense == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        #calculate past tense verbs over all verbs\n",
    "        ratio_PastVerbs = verbs / pastTense\n",
    "        perc_PastVerbsOverVerbs = (100 / ratio_PastVerbs)\n",
    "        \n",
    "        return perc_PastVerbsOverVerbs\n",
    "            \n",
    "            \n",
    "# ========================================= PRESENT TENSE CHECK =============================================\n",
    "    \n",
    "def checkPRESENT(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, presTenseVerbs = 0, 0 \n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBP\": presTenseVerbs += 1\n",
    "            elif str(tag[1]) == \"VBG\":\n",
    "                presTenseVerbs += 1\n",
    "            elif str(tag[1]) == \"VBZ\": presTenseVerbs += 1\n",
    "            else: pass\n",
    "            \n",
    "    if presTenseVerbs == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_PresVerbs = verbs / presTenseVerbs\n",
    "        perc_PresVerbsOverVerbs = (100 / ratio_PresVerbs)\n",
    "        \n",
    "        return perc_PresVerbsOverVerbs\n",
    "\n",
    "\n",
    "# ======================================= FUTURE TENSE CHECK ===============================================\n",
    "\n",
    "def checkFUTURE(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, futureTenseWords, grammarCase = 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBG\": futureTenseWords += 1\n",
    "        if str(tag[1]) == \"MD\" and str(tag[0]) == \"will\" or str(tag[0]) == \"shall\":\n",
    "            futureTenseWords += 1\n",
    "            \n",
    "    if futureTenseWords == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_FutureVerbs = verbs / futureTenseWords\n",
    "        perc_FutureWordsOverVerbs = (100 / ratio_FutureVerbs)\n",
    "        \n",
    "        return perc_FutureWordsOverVerbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past tense chance: -1 %\n",
      "Present tense chance: 50 %\n",
      "Future tense chance: 50 %\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "that means we will create this next time\n",
    "\"\"\"\n",
    "\n",
    "# -------- check sent for PAST tense\n",
    "print(\"Past tense chance: {} %\".format(int(checkPAST(sentence))))\n",
    "\n",
    "# -------- check sent for PRESENT tense    \n",
    "print(\"Present tense chance: {} %\".format(int(checkPRESENT(sentence))))\n",
    "\n",
    "# -------- check sent for FUTURE tense\n",
    "print(\"Future tense chance: {} %\".format(int(checkFUTURE(sentence))))\n",
    "\n",
    "print(\"i am going to eat a kiwi\".find(\"to\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Functions:\n",
    "- Final label count \n",
    "- title keywords exttract to search for CMs\n",
    "- extract n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# WORKS - takes a file name\n",
    "# USE to find key words from tghe title and find the first occurence of the concepts in the text and label as CM\n",
    "def title_getConcept(oFileNoExt):\n",
    "    \n",
    "    # remove all surrounding stuff like my naming convention etc from the title \n",
    "    mainTitlelist = oFileNoExt.split(\"_\")\n",
    "    maintitle = mainTitlelist[1]\n",
    "    punct = {'_','-','.'}\n",
    "    finaltitle = \"\"\n",
    "    \n",
    "    # extract the final title, i.e. the main part of the title\n",
    "    for word in maintitle.split():\n",
    "        for letter in word:\n",
    "            if letter in punct:\n",
    "                finaltitle += \" \"\n",
    "                pass\n",
    "            else:\n",
    "                finaltitle += letter\n",
    "    \n",
    "    title_keywords = []\n",
    "    for word in finaltitle.split(\" \"):\n",
    "        if word == 'en':\n",
    "             pass\n",
    "        else:\n",
    "            title_keywords.append(word)\n",
    "    \n",
    "    return title_keywords  # returns a list of keywords\n",
    "\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# WORKS \n",
    "# USE at the end to export only one label per sentence - the one with the majority vote\n",
    "# NOTE: equal case is NOT considered, so it may crash\n",
    "def getFinalLabel(curSentLabels):     # gets the list of assigned labels after all rules have been checked\n",
    "    EX,AP,CD,CM,SM, maxCount = 0, 0, 0, 0, 0, 0\n",
    "    labels = []\n",
    "    maxLabel = \"\"\n",
    "    \n",
    "    # counting the labels returned from the rules checks\n",
    "    for item in curSentLabels:\n",
    "        if item == \"EX\": EX += 1\n",
    "        elif item == \"AP\": AP += 1\n",
    "        elif item == \"CD\": CD += 1\n",
    "        elif item == \"CM\": CM += 1\n",
    "        elif item == \"SM\": SM += 1\n",
    "        else: pass   #pass NOLBL items\n",
    "\n",
    "    # adding the labels into a list of items to make it easy to get the max value\n",
    "    labels.append(\"EX,\"+str(EX))\n",
    "    labels.append(\"AP,\"+str(AP))\n",
    "    labels.append(\"CD,\"+str(CD))\n",
    "    labels.append(\"CM,\"+str(CM))\n",
    "    labels.append(\"SM,\"+str(SM))\n",
    "\n",
    "    # getting the max value \n",
    "    for item in labels:\n",
    "        parts = item.split(\",\")\n",
    "        label = parts[0]\n",
    "        count = int(parts[1])\n",
    "        if count > 0:\n",
    "            if count > maxCount:\n",
    "                maxCount = count\n",
    "                maxLabel = label\n",
    "\n",
    "    return maxLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RULES #####\n",
    "\n",
    "### ------------------EX--------------------\n",
    "def EX_1(sent):\n",
    "    nrOfWordsFound = 0\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    words = {\"example\", \"for instance\", \"assume\", \"suppose\", \"imagine\", \"as\", \"simulation\", \"diagram\"}\n",
    "    \n",
    "    for wd in words:\n",
    "        if len(wd) == 2:\n",
    "            if wd in bigrams:\n",
    "                nrOfWordsFound += 1\n",
    "        else:\n",
    "            if wd in monograms:\n",
    "                nrOfWordsFound += 1\n",
    "        \n",
    "    if nrOfWordsFound > 0:\n",
    "        return \"EX\"\n",
    "    else: \n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### ------------------EX--------------------\n",
    "    \n",
    "def EX_2(sent):\n",
    "    mainWordNR = 0      # Let's\n",
    "    secondaryWordsNR = 0     \n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    words = {\"try\", \"think\", \"see\", \"pick\", \"take a look\", \"say\"}\n",
    "    \n",
    "    if (\"let's\" or \"let 's\") in bigrams: mainWordNR += 1\n",
    "    \n",
    "    for wd in words:\n",
    "        if len(wd) == 3:\n",
    "            if wd in trigrams:\n",
    "                secondaryWordsNR += 1\n",
    "        if wd in monograms:\n",
    "                secondaryWordsNR += 1\n",
    "        \n",
    "    #print(,\" \", \" \",)\n",
    "    #print(mainWordNR,\" \",secondaryWordsNR)\n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0:\n",
    "        return \"EX\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### -------------------CD-------------------\n",
    "\n",
    "def CD_1(sent):\n",
    "    mainWordNR = 0      # Let's     # main word looking for in conjunction with one or more of the secondary words\n",
    "    secondaryWordsNR = 0 \n",
    "    negWordsNR = 0      # words that must NOT occur for the label to apply, i.e. this should stay at ZERO\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    secondaryWords = {\"look at\", \"make\", \"put\", \"do\", \"start\", \"prove\", \"back\", \"try\", \"just\", \"be\", \"take\", \"bring\"}\n",
    "    negWords = {\"example\", \"diagram\", \"assume\", \"imagine\", \"suppose\"}\n",
    "    \n",
    "    if (\"let's\" or \"let 's\") in bigrams: mainWordNR += 1\n",
    "    \n",
    "    for wd in secondaryWords:\n",
    "        if len(wd) == 2:\n",
    "            if wd in bigrams:\n",
    "                secondaryWordsNR += 1\n",
    "        else:\n",
    "            if wd in monograms:\n",
    "                secondaryWordsNR += 1\n",
    "    \n",
    "    for wd in negWords:\n",
    "        if wd in monograms:\n",
    "                negWordsNR += 1\n",
    "    \n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0 and negWordsNR == 0:\n",
    "        return \"CD\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CD_2(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if (\"in other words\" or \"basically\") in monograms and not (\"should\" or \"must\") in monograms or not \"have to\" in bigrams and checkPRESENT(sent) >= 50:\n",
    "        return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def CD_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if monograms[0] == \"so\" and (\"it 's\" or \"it's\") or (\"i 'm\" or \"i'm\") in bigrams: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def CD_4(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    if \"so this is\" in trigrams or \"actually\" in monograms and not ('example' or 'summary' or 'next' or 'last') in monograms: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------   \n",
    "\n",
    "def CD_5(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if (\"mean\" or \"given\" or \"define\") in monograms and checkPRESENT(sent) >= 50 or \"going to\" in bigrams: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "    \n",
    "    \n",
    "def CD_6(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"what if\" in bigrams and not (\"example\" or \"instance\") in monograms: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "    \n",
    "### --------------------------------------\n",
    "def SM_1(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if (\"let's\" or \"let 's\") in bigrams and \"summarize\" or \"recap\" in monograms: return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_2(sent):\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if \"in other words\" in trigrams and (checkFUTURE(sent) or checkPAST(sent)) >= 40: return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if ((\"this week\" or \"this lesson\") in bigrams or \"today\" in monograms) and (checkPRESENT(sent) or checkFUTURE(sent)) >= 50: \n",
    "        return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_4(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    words = {'later' , 'next time' , 'last time' , 'summary' , 'summarize' , \n",
    "             'here is' , 'here are' , 'discuss' , 'next', 'recap'}\n",
    "    \n",
    "    for wd in words:\n",
    "        if len(wd) == 2:\n",
    "            if wd in bigrams: return \"SM\"\n",
    "        elif wd in monograms: return \"SM\"\n",
    "        else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_5(sent, lineNR, NrOfLines):\n",
    "    if lineNR < 15 or lineNR > NrOfLines - 15 and checkPAST > 40: return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "    \n",
    "def SM_6(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    words = {\"look\", \"see\", \"be\", \"think\", \"explain\"} \n",
    "    \n",
    "    if \"going to\" in bigrams:\n",
    "        for wd in words:\n",
    "            if wd in monograms and checkPRESENT(sent) < 30 and (checkFUTURE(sent) or checkPAST(sent)) > 40 : return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def AP_1(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if \"in other words\" in trigrams and (\"should\" or \"would\" or \"could\") in monograms: return \"AP\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def AP_2(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    words = {'encourage' , 'step' , 'first' , 'finally' , 'second' , 'should' ,\n",
    "             'could' , 'would' , 'best practice', 'need to', 'good idea'}\n",
    "    \n",
    "    for wd in words:\n",
    "        if len(wd) == 2:\n",
    "            if wd in bigrams: return \"AP\"\n",
    "        elif wd in monograms: return \"AP\"\n",
    "        else: return \"NOLBL\"  \n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def AP_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    \n",
    "    if \"if\" in monograms and (\"use\" or \"can\" or \"should\", \"could\" or \"want\") in monograms: return \"AP\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CM_1(sent, oFileNoExt):\n",
    "    mainConcepts = title_getConcept(oFileNoExt)\n",
    "    monograms = get_ngrams(sent, 1)    \n",
    "    \n",
    "    if \"called\" in monograms:\n",
    "        for wd in mainConcepts:\n",
    "            if wd in monograms: return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CM_2(sent, oFileNoExt):\n",
    "    mainConcepts = title_getConcept(oFileNoExt)\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"what is\" in bigrams:\n",
    "        for wd in mainConcepts:\n",
    "            if wd in monograms: return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def CM_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if (\"theory\" or \"theorem\" or \"algorithm\" or \"method\") in monograms or (\"let's\" or \"let 's\") in bigrams and \"use\" in monograms: return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CM_4(sent, oFileNoExt, seen):\n",
    "    mainConcepts = title_getConcept(oFileNoExt)\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    \n",
    "    for gr in monograms:\n",
    "        for cpt in mainConcepts:\n",
    "            if (gr in mainConcepts or cpt in monograms) and seen == 0: return \"CM\"\n",
    "    else: return \"NOLBL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program logic, called from the file traverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------- STUPID TEST EXAMPLE ------------------------------------\n",
    "testSent = \"this week,in week nine,we're still doing applications of the derivative,but it's not so much word problems anymore\"\n",
    "#res = EX_2(testSent)\n",
    "#print(res)\n",
    "# ----------- disregard the above test ---------\n",
    "curSentLabels = []\n",
    "\n",
    "ignore_dict = ['inaudible','OMITTED','NUMBER','sound','music','laughter','yeah','blank_audio']\n",
    "\n",
    "curSentLabels.append(EX_1(testSent))\n",
    "curSentLabels.append(EX_2(testSent))\n",
    "curSentLabels.append(CD_1(testSent))\n",
    "curSentLabels.append(CD_2(testSent))\n",
    "curSentLabels.append(CD_3(testSent))\n",
    "curSentLabels.append(CD_4(testSent))\n",
    "curSentLabels.append(CD_5(testSent))\n",
    "curSentLabels.append(CD_6(testSent))\n",
    "curSentLabels.append(SM_1(testSent))\n",
    "curSentLabels.append(SM_2(testSent))\n",
    "curSentLabels.append(SM_3(testSent))\n",
    "curSentLabels.append(SM_4(testSent))\n",
    "curSentLabels.append(SM_5(testSent, 5, 76))\n",
    "curSentLabels.append(SM_6(testSent))\n",
    "curSentLabels.append(AP_1(testSent))\n",
    "curSentLabels.append(AP_2(testSent))\n",
    "curSentLabels.append(AP_3(testSent))\n",
    "curSentLabels.append(CM_1(testSent, \"02_backup.en_labels.txt\"))\n",
    "curSentLabels.append(CM_2(testSent, \"02_backup.en_labels.txt\"))\n",
    "curSentLabels.append(CM_3(testSent))\n",
    "curSentLabels.append(CM_4(testSent, \"03_data-management-across-the-research-lifecycle.en_labels.txt\", 0))\n",
    "\n",
    "#print(curSentLabels)\n",
    "\n",
    "print(getFinalLabel(curSentLabels))\n",
    "\n",
    "# label_sentences(takesAFile)\n",
    "# get_ngrams(takesASentence)\n",
    "# ruleX(takesASentence)\n",
    "# main(takesInputFile)\n",
    "\n",
    "curSentLabels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# figure out how to detect whether a term has been seen in a file or not. (meaning a term from the title)\n",
    "\n",
    "def termSeen(sent, term):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    \n",
    "    if term in sent: return 1\n",
    "    else: return 0\n",
    "    \n",
    "\"\"\"\n",
    "    // TODO\n",
    "    When this returns 0, we can update the value for the respective term in titleTermsSeen from 0 to 1 and \n",
    "    we can call CM_4 for each term and it will work only if the term's value is 0, \n",
    "    so for each sentence:\n",
    "        check dictionary, if value is 0, call CM_4\n",
    "        if CM_4 returns a value \"CM\", then update the dictionary for that value with 1\n",
    "        else don't call CM_4 at all\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(iFile, oPathNoExt):    # main application with all logic following the pseudocode=\n",
    "    correctLabels = 0\n",
    "    #accuracy = correctLabels / totalSentences   * 100\n",
    "\n",
    "    ### ---------- Local variables - reset per file ------------------------\n",
    "    lineNR = 0\n",
    "    totalLines = 0\n",
    "    curSentLabels = []          # All the labels assigned to the current sentence (to get majority vote from it later)\n",
    "\n",
    "    originalLabel = \"\"          # label from the sentence\n",
    "    finalMajorityLabel = \"\"     # label assigned after the rules application SHOULD NOT BE MAJORITY, \n",
    "                                # but at the end only one real label should be inside the list of labels after the evaluation\n",
    "    finalMajorityLabelUsers = \"\"     # label assigned by other users (majority vote)\n",
    "    countCorLabels = 0\n",
    "    latestLabeledSent = \"\"\n",
    "    latestLabeledSentLBL = \"\"\n",
    "    titleTerms = \"\"\n",
    "    titleTermsSeen = {}\n",
    "\n",
    "    #accuracy = (countCorLabels/totalSentences) * 100\n",
    "    #print(\"Accuracy: {0:.2f} %\".format(accuracy))\n",
    "\n",
    "    print(\"[LABELLING file: ] \" + os.path.basename(iFile.name))\n",
    "    baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "    OFName = baseName + \".en_AutoRuleLabels.txt\"\n",
    "\n",
    "    sentences = iFile.read().split(\"\\n\")\n",
    "    titleTerms = title_getConcept(oPathNoExt)\n",
    "    \n",
    "    for term in titleTerms:\n",
    "        titleTermsSeen[term] = 0\n",
    "    \n",
    "    for sent in sentences:\n",
    "        totalLines += 1\n",
    "        \n",
    "\n",
    "    with open(OFName, \"w\") as oFile:    # opening the output file to write in the same place where the original file is\n",
    "        for sent in sentences:\n",
    "            lineNR += 1\n",
    "            sentLBLandText = sent.split(\"|\")\n",
    "            sent = sentLBLandText[1]\n",
    "            originalLabel = sentLBLandText[0]\n",
    "\n",
    "            if originalLabel == \"NL\":\n",
    "                pass\n",
    "            else: \n",
    "                if finalMajorityLabel == \"\":\n",
    "                    if lineNR < 15 or lineNR > len(sentences)-15:   # assigning threshold of first or last 10 sentences\n",
    "                        if checkPAST(sent) > 50:\n",
    "                            curSentLabels.append(SM_1(sent))\n",
    "                            curSentLabels.append(SM_2(sent))\n",
    "                            curSentLabels.append(SM_3(sent))\n",
    "                            curSentLabels.append(SM_4(sent))\n",
    "                            curSentLabels.append(SM_5(sent, lineNR, totalLines))\n",
    "                            curSentLabels.append(SM_6(sent))\n",
    "                        elif checkPRESENT(sent) > 50:\n",
    "                            curSentLabels.append(CD_1(sent))\n",
    "                            curSentLabels.append(CD_2(sent))\n",
    "                            curSentLabels.append(CD_3(sent))\n",
    "                            curSentLabels.append(CD_4(sent))\n",
    "                            curSentLabels.append(CD_5(sent))\n",
    "                            curSentLabels.append(CD_6(sent))\n",
    "                        elif checkFUTURE(sent) > 50:\n",
    "                            curSentLabels.append(CM_1(sent, oPathNoExt))\n",
    "                            curSentLabels.append(CM_2(sent, oPathNoExt))\n",
    "                            curSentLabels.append(CM_3(sent))\n",
    "                            #curSentLabels.append(CM_4(sent))   # special case, TODO\n",
    "                    else:\n",
    "                        curSentLabels.append(EX_1(sent))\n",
    "                        curSentLabels.append(EX_2(sent))\n",
    "                        curSentLabels.append(CD_1(sent))\n",
    "                        curSentLabels.append(CD_2(sent))\n",
    "                        curSentLabels.append(CD_3(sent))\n",
    "                        curSentLabels.append(CD_4(sent))\n",
    "                        curSentLabels.append(CD_5(sent))\n",
    "                        curSentLabels.append(CD_6(sent))\n",
    "                        curSentLabels.append(AP_1(sent))\n",
    "                        curSentLabels.append(AP_2(sent))\n",
    "                        curSentLabels.append(AP_3(sent))\n",
    "                        curSentLabels.append(CM_1(sent, oPathNoExt))\n",
    "                        curSentLabels.append(CM_2(sent, oPathNoExt))\n",
    "                        curSentLabels.append(CM_3(sent))\n",
    "                        #curSentLabels.append(CM_4(sent, oPathNoExt, seen))   # special case, TODO\n",
    "            \n",
    "            \"\"\" ### UPDATE FINALMAJORITYLABEL HERE - IT DOES NOT OUTPUT IN THE FILE!!!! \"\"\"\n",
    "            if getFinalLabel(curSentLabels) != (\"SM\" or \"CD\" or \"CP\" or \"AP\" or \"EX\"):\n",
    "                # if there exists a sentence that's already labeled, take its label and assign to the current sent\n",
    "                # this only happens if no label is returned from the finalMajorityLabel function\n",
    "                if latestLabeledSent != \"\":\n",
    "                    finalMajorityLabel = latestLabeledSentLBL\n",
    "            else: \n",
    "                latestLabeledSent = sent\n",
    "                latestLabeledSentLBL = getFinalLabel(curSentLabels)\n",
    "                \n",
    "            oFile.write(finalMajorityLabel+\"|\"+sent+\"\\n\") #write the final label and sentence in the output file\n",
    "            print(finalMajorityLabel)\n",
    "            if finalMajorityLabel == originalLabel:\n",
    "                correctLabels += 1\n",
    "            finalMajorityLabel = \"\" #reset finalMajorityLabel back to \"\"   finalMajorityLabel = \"\"\n",
    "            titleTerms = \"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THIS PART - going over all files and calling the main program on each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LABELLING file: ] 06_cruise-controllers.en_labels.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total number of 1 TXT files found.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "path = r\"C:\\Users\\a.dimitrova\\Desktop\\Course data Thesis\\INTENT MINING\"\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if filePath.endswith(\"_AutoRuleLabels.txt\"): pass\n",
    "            else:\n",
    "                counter += 1\n",
    "                curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "\n",
    "                main(curFile, fileExtRemoved)\n",
    "\n",
    "                curFile.close()\n",
    "            \n",
    "                \n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
