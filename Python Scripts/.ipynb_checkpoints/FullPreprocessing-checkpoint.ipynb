{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data pre-processing \n",
    "Data is collected from 11 MOOCs in the field of Computer Science, robotics, mathematics and physics and are processed total of 563 TXT files. \n",
    "\n",
    "#### Preprocessing steps: \n",
    "\n",
    "- sentence by sentence split\n",
    "- lower case\n",
    "- noise removal \n",
    "    - SOME punctuation STAYS \n",
    "        - point, single space and comma\n",
    "    - SOME stopwords stay \n",
    "        - between,we,i,in,here,that,you,it,that,this,there,few,if,so,to,a,an,is,until,while\n",
    "    - mention removal\n",
    "- word normalization\n",
    "    - tokenization \n",
    "    - lemmatization \n",
    "    - stemming \n",
    "- word standardization\n",
    "    - regex\n",
    "\n",
    "#### Overall logic:\n",
    "1. traverse recursively all folder and files\n",
    "2. When a file is found, save it's name into a file list\n",
    "3. For each file in the file list, apply all the **preprocessing steps** and save it as a new file with \"\\_PREPROCESSED\" added at the end in the same folder\n",
    "    3.1. Changed: Now I keep deparate file after each preprocessing step as I may need certain file for some algorithms. \n",
    "\n",
    "#### Next steps (In the Data Analysis file) :  \n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from pathlib import Path\n",
    "from beautifultable import BeautifulTable\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming processing from Lancaster to WordNet stems\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS', 'WRB']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        # returns a\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        # returns n\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        # returns r\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        # returns v\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all allowed characters.\n",
    "# {...} is the syntax for a set literal in Python.\n",
    "allowedPunct = {\",\", \" \",\"a\",\"i\"}.union(string.ascii_lowercase)\n",
    "\n",
    "# Use full for BoF and others. \n",
    "# Use part in order to keep some data important if we need to match tuples from the text based on natual speach\n",
    "# LINUX path\n",
    "path_swAni_full = \"/media/sf_Shared_Folder/stopwordsALL.txt\"\n",
    "path_swAni_part = \"/media/sf_Shared_Folder/stopwords.txt\"\n",
    "\n",
    "# WINDOWS path Toshiba (r = raw)\n",
    "path_swAni_full_W = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\stopwordsALL.txt\"\n",
    "path_swAni_part_W = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\stopwords.txt\"\n",
    "\n",
    "# WINDOWS path HP hope laptop (r = raw)\n",
    "path_swAni_full_W = r\"D:\\VirtualBox VMs\\Shared Folder\\stopwordsALL.txt\"\n",
    "path_swAni_part_W = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\stopwords.txt\"\n",
    "path_swAni_paSS_W = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\passStopWordListAni.txt\"\n",
    "\n",
    "swFile = open(path_swAni_full_W, 'r', encoding = \"ISO-8859-1\")\n",
    "stopWords = swFile.read().split()\n",
    "#print(stopWords)\n",
    "\n",
    "fullSWList = ['about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', \n",
    "                'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', \n",
    "                'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', \n",
    "                'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', \n",
    "                'in', 'into', 'is', 'it', 'it', 'its', 'itself', 'just', \"'ll\", \"'m\", 'me', 'more', 'most', 'my', \n",
    "                'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours',\n",
    "                'ourselves', 'out', 'over', 'own', \"'re\", 's', \"'s\", 'same', 'she', 'should', 'so', 'some', 'such',\n",
    "                't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \n",
    "                'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', \n",
    "                'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'with', \n",
    "                'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "partialSWList = ['about', 'above', 'after', 'again', 'against', 'all', 'am', 'and', 'any', 'are', 'as', 'at', \n",
    "                'be', 'because', 'been', 'before', 'being', 'below', 'both', 'but', 'by', 'can', 'did', 'do', \n",
    "                'does', 'doing', 'don', 'down', 'during', 'each', 'for', 'from', 'further', 'had', 'has', \n",
    "                'have', 'having', 'he', 'her', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'into', \n",
    "                'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not', \n",
    "                'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', \n",
    "                'over', 'own', 's', 'same', 'she', 'should', 'some', 'such', 't', 'than', 'the', 'their', \n",
    "                'theirs', 'them', 'themselves', 'then', 'these', 'they', 'those', 'through', 'too', 'under', \n",
    "                'up', 'very', 'was', 'were', 'what', 'when', 'where', 'which', 'who', 'whom', 'why', 'with', \n",
    "                'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "whitelistSW = ['a', 'an', 'between', 'few', 'here', 'i', 'if', 'in', 'is', 'it', 'so', 'that', \n",
    "                'there', 'this', 'to', 'until', 'we', 'while', 'you']\n",
    "\n",
    "tinySWList = ['we','that','now','you','in','the','is','and','to']\n",
    "\n",
    "#stopWords = stopwords.words('english')\n",
    "#print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ METHODS  --------------------------------------------------------\n",
    "\n",
    "# function #1    \n",
    "def splitSentences(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_Raw.txt\"):\n",
    "        print(\"[Splitting sentences on file:] \" + os.path.basename(iFile.name))   #file name only\n",
    "        #print(\"[Splitting sentences on file:] \" + iFile.name)   # full file path\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_sent.txt\"\n",
    "        \n",
    "        # the WITH keyword makes it possible to omit the file.close() function at the end to close the file\n",
    "        with open(OFName,\"w\") as oFile:\n",
    "            #print(oFile.name)\n",
    "            text = iFile.read()    \n",
    "            # initial text is full of new lines, so we have to remove them first. \n",
    "            text = text.replace(\"\\n\", \" \")    \n",
    "            # have the sentences split and print them one by one\n",
    "            sentences = sent_tokenize(text)\n",
    "            # write to the output file\n",
    "            for sent in sentences:\n",
    "                oFile.write(sent+\"\\n\")\n",
    "        oFile.close()\n",
    "    \n",
    "      \n",
    "# function #3\n",
    "def sentTokenize(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sent.txt\"):\n",
    "        print(\"[Tokenizing file: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokens.txt\"\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            tokens = iFile.read()\n",
    "            tokens = word_tokenize(tokens)\n",
    "            for tok in tokens:\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\", \"=\", \"*\", \"/\", \"+\", \n",
    "                                  \"-\", \"%\", \"#\", \"?\", \"(\", \")\", \"-\", \"_\", \n",
    "                                  \";\", \":\", \"'\", \"\\\"\", \"^\", \"`\"}.union(string.ascii_lowercase)\n",
    "                if tok in UNallowedPunct:\n",
    "                    pass\n",
    "                else:\n",
    "                    oFile.write(tok+\"\\n\")\n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Removing punctuation from file, but keeping the sentence structure\n",
    "def rmPunctuationSent(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sent.txt\"):\n",
    "        print(\"[Removing punctuation from file: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_sentNoPunct.txt\"\n",
    "        \n",
    "        sungleNum = re.compile(\"[0-9]+\")\n",
    "        biggerNum = re.compile(\"(\\d+.\\d+)\")\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                sentences = iFile.read().split('\\n')\n",
    "                #tokens = word_tokenize(tokens)\n",
    "                for sent in sentences:\n",
    "                    sentToTok = word_tokenize(sent)\n",
    "                    #print(sentToTok)\n",
    "                    for tok in sentToTok:\n",
    "                        # full punct removal list\n",
    "                        UNallowedPunct = {\"[\", \"]\", \"=\", \"*\", \"/\", \"+\", \n",
    "                                          \"-\", \"%\", \"#\", \"?\", \"(\", \")\", \"-\", \"_\", \n",
    "                                          \";\", \":\", \"'\", \"\\\"\", \"^\", \"`\"}.union(string.ascii_lowercase)\n",
    "                        #if tok in UNallowedPunctSM:  #1\n",
    "                        if tok in allowedPunct:    #2\n",
    "                            oFile.write(tok+\" \")   #2\n",
    "                            #pass  #1\n",
    "                        elif tok in UNallowedPunct or len(tok) == 1:\n",
    "                            #oFile.write(tok+\" \")   #1\n",
    "                            pass    #2\n",
    "                        elif sungleNum.match(tok) or biggerNum.match(tok):\n",
    "                            oFile.write(\"NUMBER \")\n",
    "                        else: \n",
    "                            oFile.write(tok+\" \")   #2\n",
    "                    oFile.write(\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Removing stop words from file\n",
    "def rmStopWords(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_tokens.txt\"):\n",
    "        print(\"[Removing StopWords: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_noStopWordsALL.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for tok in tokenList:\n",
    "                    if tok in fullSWList:\n",
    "                        # if token is a stop word, don't save in the output (skip)\n",
    "                        pass\n",
    "                    else:\n",
    "                        oFile.write(tok+\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def stripMultiSpace(iFile):\n",
    "    if iFile.name.endswith(\"_sentNoPunct.txt\"):\n",
    "    #if iFile.name.endswith(\"_sentNoPunctWithCommDot.txt\"):\n",
    "        print(\"[Removing MultiSpace: ] \" + os.path.basename(iFile.name))\n",
    "        \n",
    "        baseName = iFile.name.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_sentNoPunctEnhanced.txt\"\n",
    "        \n",
    "        doubleSpaceCommaPattern = r'\\s,\\s'\n",
    "        singleSpaceBothSidesCommaPat = r'(\\s\\,\\s)'\n",
    "        apostrophePat = r'\\s\\''\n",
    "        finalDotPat = r'\\s\\.'\n",
    "        doubleSpacePat = r'\\s\\s'\n",
    "        \n",
    "        content = iFile.read().split(\"\\n\")\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            for item in content:\n",
    "                #print(\"BEFORE: \",item)\n",
    "                item = re.sub(doubleSpaceCommaPattern, \",\", item)\n",
    "                item = re.sub(singleSpaceBothSidesCommaPat, \",\", item)\n",
    "                item = re.sub(apostrophePat, \"'\", item)\n",
    "                item = re.sub(finalDotPat, \".\", item)\n",
    "                item = re.sub(doubleSpacePat, \" \", item)\n",
    "                oFile.write(item+\"\\n\")\n",
    "                #print(\"AFTER: \",item)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def rmSWSent(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sentNoPunctEnhanced.txt\"):\n",
    "        print(\"[Removing StopWords: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_sentNoPunctEnchancedNoSWTiny.txt\"\n",
    "        \n",
    "        sentList = iFile.read().split('\\n')\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for sent in sentList:\n",
    "                    wordList = sent.split()\n",
    "                    for word in wordList:\n",
    "                        if word in tinySWList:\n",
    "                            # if token is a stop word, don't save in the output (skip)\n",
    "                            pass\n",
    "                        else:\n",
    "                            oFile.write(word+\" \")  \n",
    "                    oFile.write(\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def replaceNumsAndFunc(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sentNoPunctEnhanced.txt\"):\n",
    "    #if iFile.name.endswith(\"_sentNoPunct.txt\"):\n",
    "        print(\"[Removing functions and single characters: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_sentNoPunct2.txt\"\n",
    "        \n",
    "        sentList = iFile.read().split('\\n')\n",
    "        \n",
    "        ifNotSomeOfTheAllowed = re.compile(\"[^(\\s+(,)\\s+)|(\\s+(.)\\s+)|(\\s+(a)\\s+)|(\\s+(i)\\s+)]\")\n",
    "        isNum = re.compile(\"[0-9]\")\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for sent in sentList:\n",
    "                    wordList = sent.split()\n",
    "                    for word in wordList:\n",
    "                        if (len(word) == 1):\n",
    "                            if ifNotSomeOfTheAllowed.match(word):\n",
    "                                #word = re.sub(ifNotSomeOfTheAllowed, \" \", item)\n",
    "                                if isNum.match(word):\n",
    "                                    #replace with NUMBER\n",
    "                                    oFile.write(\"NUMBER \")\n",
    "                                else:\n",
    "                                    oFile.write(\"OMITTED \")\n",
    "                            else:\n",
    "                                oFile.write(word+\" \")\n",
    "                        else:\n",
    "                            oFile.write(word+\" \")  \n",
    "                    oFile.write(\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def prepForLabeling(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sentNoPunct2.txt\"):\n",
    "        print(\"[Removing functions and single characters: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        #print(\"BASE NAME: \", baseName)\n",
    "        \n",
    "        OFName = baseName + \".en_labels.txt\"\n",
    "        \n",
    "        sentList = iFile.read().split('\\n')\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for sent in sentList:\n",
    "                    if not len(sent) == 0:\n",
    "                        oFile.write(\"|\"+sent+\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### functions part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function #5 - partOfSpeechTag Tagging\n",
    "def POStag(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_noStopWordsALL.txt\"):\n",
    "        print(\"[Part of Speech tagging: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokPOStag.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:            \n",
    "            taggedTok = pos_tag(tokenList)\n",
    "            \n",
    "            for tok in taggedTok:\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\"}.union(string.ascii_lowercase)\n",
    "                if tok[0] in UNallowedPunct:\n",
    "                    # if token is a punctuation symbol, don't save in the output\n",
    "                    pass\n",
    "                else:\n",
    "                    #print(tok[0],',',tok[1])    # output: now , RB\n",
    "                    #print(tok)                  # output: ('now', 'RB')\n",
    "                    outpLine = tok[0]+\",\"+tok[1]\n",
    "                    oFile.write(outpLine+\"\\n\")\n",
    "                    #oFile.write(str(tok)+\"\\n\")\n",
    "                    \n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Outputs data as a tuple (Word,WordNetPOSTag,Stem)\n",
    "\n",
    "POS TAG: read line by line, split each line by (\",\"), write line[0] to the file with comma, following it\n",
    "STEM: take line[1] and compare POS tag, return the appropriate POS tag as per Wordnet, i.e. \n",
    "r for adverb\n",
    "a for adjective\n",
    "n for noun\n",
    "v for verb\n",
    "\"\"\"\n",
    "def stem(iFile, oPathNoExt):\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    if not iFile.name.endswith(\"_tokPOStag.txt\"):\n",
    "    #if not iFile.name.endswith(\"NounVerbAdjectiveAdverb.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Stemming: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_stemmedbyPOS.txt\"\n",
    "        \n",
    "        content = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:   \n",
    "           \n",
    "            for line in content:\n",
    "                curline = line.split(\",\")    #list with 2 elements\n",
    "                word = curline[0]\n",
    "                posTag = curline[1]\n",
    "                \n",
    "                stem = stemmer.stem(word)\n",
    "                \n",
    "                if(penn_to_wn(posTag) == \"a\"):\n",
    "                    oFile.write(word+\",a,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"n\"):\n",
    "                    oFile.write(word+\",n,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"r\"):\n",
    "                    oFile.write(word+\",r,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"v\"):\n",
    "                    oFile.write(word+\",v,\"+stem+\"\\n\")\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            oFile.close() \n",
    "            \n",
    "# Requires the output from stem() and more specifically the POS tag in order to know what \n",
    "# word to make out of the stem. If not specific, it assumes NOUN\n",
    "def lemmatize(iFile, oPathNoExt):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if not iFile.name.endswith(\"en_stemmedbyPOS.txt\"):\n",
    "    #if not iFile.name.endswith(\"NounVerbAdjectiveAdverb.en_stemmedbyPOS.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Lemmatizing: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_lemmatized.txt\"\n",
    "        \n",
    "        content = iFile.read().split()\n",
    "        \n",
    "        #print(\"Word | Stem | Lemma | LemmaPOS\")\n",
    "        #print(\"------------------------------\")\n",
    "        with open(OFName, \"w\") as oFile:    \n",
    "            for line in content:\n",
    "                curline = line.split(\",\")    #list with 2 elements\n",
    "                word = curline[0]\n",
    "                posTag = curline[1]\n",
    "                stem = curline[2]\n",
    "                \n",
    "                lemma = nltk.stem.WordNetLemmatizer().lemmatize(stem)\n",
    "                lemmaPOS = nltk.stem.WordNetLemmatizer().lemmatize(stem, 'v')\n",
    "                #print(word+\" | \"+stem+\" | \"+lemma+\" | \"+lemmaPOS)\n",
    "                oFile.write(lemmaPOS+\"\\n\")\n",
    "            \n",
    "            oFile.close()\n",
    "            \n",
    "def cleanup(iFile):\n",
    "    \"\"\"\n",
    "    if iFile.name.endswith(\"_sentNoPunctWithCommDot.txt\"):\n",
    "        if('.en' not in iFile.name):\n",
    "            try:\n",
    "                print(\"DELETING FILE: \",iFile.name)\n",
    "                os.remove(iFile.name)\n",
    "            except PermissionError as err:\n",
    "                print(\"SKIPPING: (FileInUse)\",err,\"\\n\")\n",
    "                pass\n",
    "    \"\"\"\n",
    "    if not iFile.name.endswith(\".en_labels.txt\"):\n",
    "        try:\n",
    "            print(\"DELETING FILE: \",iFile.name)\n",
    "            os.remove(iFile.name)\n",
    "        except PermissionError as err:\n",
    "            print(\"SKIPPING: (FileInUse)\",err,\"\\n\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of 112 TXT files found.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------- MAIN PROGRAM  -------------------------------------------------------\n",
    "\n",
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Selected files TRAIN\"\n",
    "#path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Coursera Downloads PreProcessed\"\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            else: \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    #fileName = print(os.path.abspath(filePath))\n",
    "                    #curFile = open(filePath, 'r', encoding = \"ascii\")\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "\n",
    "                    try:\n",
    "                        fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                        #outpFileBase = open(FileExtRemoved + \"_PROC.txt\",\"w\")\n",
    "                        \n",
    "                        \"\"\"\n",
    "                        call each processing function here and pass it the file\n",
    "                        First argument: current input file\n",
    "                        Second argument: path without extension of the current file\n",
    "                        the path will be used to save the output file with the same name and same location\n",
    "                        but with different file ending based on what the fuunction did\n",
    "\n",
    "                        Functions take input files ending on:\n",
    "                        splitSentences()\t>>\t_Raw.txt\n",
    "                        sentTokenize()\t\t>>\t_sent.txt\n",
    "                        POStag()\t\t\t>>\t_noStopWordsALL.txt\n",
    "                        rmStopWords()\t\t>>\t_tokens.txt\n",
    "                        stem()\t\t\t\t>>\t_tokPOStag.txt\n",
    "                        lemmatize()\t\t\t>>\t_stemmed.txt\n",
    "                        rmPunctuationSent()\t>>\t_sent.txt\n",
    "                        stripMultiSpace\t\t>>\t_sentNoPunctWithCommDot\n",
    "                        rmSWSent \t\t\t>>\t_sentNoPunctEnhanced\n",
    "                        \"\"\"  \n",
    "                        #splitSentences(curFile, fileExtRemoved)\n",
    "                        #sentTokenize(curFile, fileExtRemoved)\n",
    "                        #rmStopWords(curFile, fileExtRemoved)\n",
    "                        #POStag(curFile, fileExtRemoved)\n",
    "                        #stem(curFile, fileExtRemoved)\n",
    "                        #lemmatize(curFile, fileExtRemoved)\n",
    "                        #rmPunctuationSent(curFile, fileExtRemoved)\n",
    "                        #stripMultiSpace(curFile)\n",
    "                        #rmSWSent(curFile, fileExtRemoved)\n",
    "                        #replaceNumsAndFunc(curFile, fileExtRemoved)\n",
    "                        #prepForLabeling(curFile, fileExtRemoved)\n",
    "                        \n",
    "                        curFile.close()\n",
    "                        #cleanup(curFile)\n",
    "                        \n",
    "                        # for sentence processing: run one after another: rmPunctuationSent(), replaceNumsAndFunc(),\n",
    "                        # prepForLabeling() and cleanup()\n",
    "                        \n",
    "                    except UnicodeEncodeError:\n",
    "                        print(\"CANNOT PROCESS (Encoding err): \" + curFile.name)\n",
    "                        pass\n",
    "                    except FileNotFoundError:\n",
    "                        print(\"CANNOT PROCESS (FileNotFoundError err): \" + curFile.name)\n",
    "                        pass\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "                    \n",
    "                    \n",
    "        \"\"\" #### USE ONLY TO CLEANUP DATA - DELETED SRT files\n",
    "        if filePath.endswith(\".srt\"):\n",
    "            curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "            curFile.close()\n",
    "            cleanup(curFile)\n",
    "        \"\"\" \n",
    "        \n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "---------- offtopic ---------\n",
    "\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
