{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data pre-processing \n",
    "Data is collected from 11 MOOCs in the field of Computer Science, robotics, mathematics and physics and are processed total of 563 TXT files. \n",
    "\n",
    "#### Preprocessing steps: \n",
    "\n",
    "- sentence by sentence split\n",
    "- lower case\n",
    "- noise removal \n",
    "    - SOME punctuation STAYS \n",
    "        - point, single space and comma\n",
    "    - SOME stopwords stay \n",
    "        - between,we,i,in,here,that,you,it,that,this,there,few,if,so,to,a,an,is,until,while\n",
    "    - mention removal\n",
    "- word normalization\n",
    "    - tokenization \n",
    "    - lemmatization \n",
    "    - stemming \n",
    "- word standardization\n",
    "    - regex\n",
    "\n",
    "#### Overall logic:\n",
    "1. traverse recursively all folder and files\n",
    "2. When a file is found, save it's name into a file list\n",
    "3. For each file in the file list, apply all the **preprocessing steps** and save it as a new file with \"\\_PREPROCESSED\" added at the end in the same folder\n",
    "    3.1. Changed: Now I keep deparate file after each preprocessing step as I may need certain file for some algorithms. \n",
    "\n",
    "#### Next steps (In the Data Analysis file) :  \n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from pathlib import Path\n",
    "from beautifultable import BeautifulTable\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming processing from Lancaster to WordNet stems\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS', 'WRB']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        # returns a\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        # returns n\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        # returns r\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        # returns v\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all allowed characters.\n",
    "# {...} is the syntax for a set literal in Python.\n",
    "allowedPunct = {\",\", \" \",\"a\",\"i\"}.union(string.ascii_lowercase)\n",
    "\n",
    "# Use full for BoF and others. \n",
    "# Use part in order to keep some data important if we need to match tuples from the text based on natual speach\n",
    "# LINUX path\n",
    "path_swAni_full = \"/media/sf_Shared_Folder/stopwordsALL.txt\"\n",
    "path_swAni_part = \"/media/sf_Shared_Folder/stopwords.txt\"\n",
    "\n",
    "# WINDOWS path Toshiba (r = raw)\n",
    "path_swAni_full_W = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\stopwordsALL.txt\"\n",
    "path_swAni_part_W = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\stopwords.txt\"\n",
    "\n",
    "# WINDOWS path HP hope laptop (r = raw)\n",
    "path_swAni_full_W = r\"D:\\VirtualBox VMs\\Shared Folder\\stopwordsALL.txt\"\n",
    "path_swAni_part_W = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\stopwords.txt\"\n",
    "path_swAni_paSS_W = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\passStopWordListAni.txt\"\n",
    "\n",
    "swFile = open(path_swAni_full_W, 'r', encoding = \"ISO-8859-1\")\n",
    "stopWords = swFile.read().split()\n",
    "#print(stopWords)\n",
    "\n",
    "fullSWList = ['about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', 'as', \n",
    "                'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', \n",
    "                'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', \n",
    "                'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', \n",
    "                'in', 'into', 'is', 'it', 'it', 'its', 'itself', 'just', \"'ll\", \"'m\", 'me', 'more', 'most', 'my', \n",
    "                'myself', 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours',\n",
    "                'ourselves', 'out', 'over', 'own', \"'re\", 's', \"'s\", 'same', 'she', 'should', 'so', 'some', 'such',\n",
    "                't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \n",
    "                'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', \n",
    "                'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'with', \n",
    "                'you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "partialSWList = ['about', 'above', 'after', 'again', 'against', 'all', 'am', 'and', 'any', 'are', 'as', 'at', \n",
    "                'be', 'because', 'been', 'before', 'being', 'below', 'both', 'but', 'by', 'can', 'did', 'do', \n",
    "                'does', 'doing', 'don', 'down', 'during', 'each', 'for', 'from', 'further', 'had', 'has', \n",
    "                'have', 'having', 'he', 'her', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'into', \n",
    "                'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not', \n",
    "                'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', \n",
    "                'over', 'own', 's', 'same', 'she', 'should', 'some', 'such', 't', 'than', 'the', 'their', \n",
    "                'theirs', 'them', 'themselves', 'then', 'these', 'they', 'those', 'through', 'too', 'under', \n",
    "                'up', 'very', 'was', 'were', 'what', 'when', 'where', 'which', 'who', 'whom', 'why', 'with', \n",
    "                'your', 'yours', 'yourself', 'yourselves']\n",
    "\n",
    "whitelistSW = ['a', 'an', 'between', 'few', 'here', 'i', 'if', 'in', 'is', 'it', 'so', 'that', \n",
    "                'there', 'this', 'to', 'until', 'we', 'while', 'you']\n",
    "\n",
    "tinySWList = ['we','that','now','you','in','the','is','and','to']\n",
    "\n",
    "#stopWords = stopwords.words('english')\n",
    "#print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------ METHODS  --------------------------------------------------------\n",
    "\n",
    "# function #1    \n",
    "def splitSentences(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_Raw.txt\"):\n",
    "        print(\"[Splitting sentences on file:] \" + os.path.basename(iFile.name))   #file name only\n",
    "        #print(\"[Splitting sentences on file:] \" + iFile.name)   # full file path\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_sent.txt\"\n",
    "        \n",
    "        # the WITH keyword makes it possible to omit the file.close() function at the end to close the file\n",
    "        with open(OFName,\"w\") as oFile:\n",
    "            #print(oFile.name)\n",
    "            text = iFile.read()    \n",
    "            # initial text is full of new lines, so we have to remove them first. \n",
    "            text = text.replace(\"\\n\", \" \")    \n",
    "            # have the sentences split and print them one by one\n",
    "            sentences = sent_tokenize(text)\n",
    "            # write to the output file\n",
    "            for sent in sentences:\n",
    "                oFile.write(sent+\"\\n\")\n",
    "        oFile.close()\n",
    "    \n",
    "      \n",
    "# function #3\n",
    "def sentTokenize(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sent.txt\"):\n",
    "        print(\"[Tokenizing file: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokens.txt\"\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            tokens = iFile.read()\n",
    "            tokens = word_tokenize(tokens)\n",
    "            for tok in tokens:\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\", \"=\", \"*\", \"/\", \"+\", \n",
    "                                  \"-\", \"%\", \"#\", \"?\", \"(\", \")\", \"-\", \"_\", \n",
    "                                  \";\", \":\", \"'\", \"\\\"\", \"^\", \"`\"}.union(string.ascii_lowercase)\n",
    "                if tok in UNallowedPunct:\n",
    "                    pass\n",
    "                else:\n",
    "                    oFile.write(tok+\"\\n\")\n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Removing punctuation from file, but keeping the sentence structure\n",
    "def rmPunctuationSent(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sent.txt\"):\n",
    "        print(\"[Removing punctuation from file: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_sentNoPunct.txt\"\n",
    "        \n",
    "        sungleNum = re.compile(\"[0-9]+\")\n",
    "        biggerNum = re.compile(\"(\\d+.\\d+)\")\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                sentences = iFile.read().split('\\n')\n",
    "                #tokens = word_tokenize(tokens)\n",
    "                for sent in sentences:\n",
    "                    sentToTok = word_tokenize(sent)\n",
    "                    #print(sentToTok)\n",
    "                    for tok in sentToTok:\n",
    "                        # full punct removal list\n",
    "                        UNallowedPunct = {\"[\", \"]\", \"=\", \"*\", \"/\", \"+\", \n",
    "                                          \"-\", \"%\", \"#\", \"?\", \"(\", \")\", \"-\", \"_\", \n",
    "                                          \";\", \":\", \"'\", \"\\\"\", \"^\", \"`\"}.union(string.ascii_lowercase)\n",
    "                        #if tok in UNallowedPunctSM:  #1\n",
    "                        if tok in allowedPunct:    #2\n",
    "                            oFile.write(tok+\" \")   #2\n",
    "                            #pass  #1\n",
    "                        elif tok in UNallowedPunct or len(tok) == 1:\n",
    "                            #oFile.write(tok+\" \")   #1\n",
    "                            pass    #2\n",
    "                        elif sungleNum.match(tok) or biggerNum.match(tok):\n",
    "                            oFile.write(\"NUMBER \")\n",
    "                        else: \n",
    "                            oFile.write(tok+\" \")   #2\n",
    "                    oFile.write(\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Removing stop words from file\n",
    "def rmStopWords(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_tokens.txt\"):\n",
    "        print(\"[Removing StopWords: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_noStopWordsALL.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for tok in tokenList:\n",
    "                    if tok in fullSWList:\n",
    "                        # if token is a stop word, don't save in the output (skip)\n",
    "                        pass\n",
    "                    else:\n",
    "                        oFile.write(tok+\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "def stripMultiSpace(iFile):\n",
    "    if iFile.name.endswith(\"_sentNoPunct.txt\"):\n",
    "    #if iFile.name.endswith(\"_sentNoPunctWithCommDot.txt\"):\n",
    "        print(\"[Removing MultiSpace: ] \" + os.path.basename(iFile.name))\n",
    "        \n",
    "        baseName = iFile.name.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_sentNoPunctEnhanced.txt\"\n",
    "        \n",
    "        doubleSpaceCommaPattern = r'\\s,\\s'\n",
    "        singleSpaceBothSidesCommaPat = r'(\\s\\,\\s)'\n",
    "        apostrophePat = r'\\s\\''\n",
    "        finalDotPat = r'\\s\\.'\n",
    "        doubleSpacePat = r'\\s\\s'\n",
    "        \n",
    "        content = iFile.read().split(\"\\n\")\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            for item in content:\n",
    "                #print(\"BEFORE: \",item)\n",
    "                item = re.sub(doubleSpaceCommaPattern, \",\", item)\n",
    "                item = re.sub(singleSpaceBothSidesCommaPat, \",\", item)\n",
    "                item = re.sub(apostrophePat, \"'\", item)\n",
    "                item = re.sub(finalDotPat, \".\", item)\n",
    "                item = re.sub(doubleSpacePat, \" \", item)\n",
    "                oFile.write(item+\"\\n\")\n",
    "                #print(\"AFTER: \",item)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def rmSWSent(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sentNoPunctEnhanced.txt\"):\n",
    "        print(\"[Removing StopWords: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_sentNoPunctEnchancedNoSWTiny.txt\"\n",
    "        \n",
    "        sentList = iFile.read().split('\\n')\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for sent in sentList:\n",
    "                    wordList = sent.split()\n",
    "                    for word in wordList:\n",
    "                        if word in tinySWList:\n",
    "                            # if token is a stop word, don't save in the output (skip)\n",
    "                            pass\n",
    "                        else:\n",
    "                            oFile.write(word+\" \")  \n",
    "                    oFile.write(\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def replaceNumsAndFunc(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sentNoPunctEnhanced.txt\"):\n",
    "    #if iFile.name.endswith(\"_sentNoPunct.txt\"):\n",
    "        print(\"[Removing functions and single characters: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_sentNoPunct2.txt\"\n",
    "        \n",
    "        sentList = iFile.read().split('\\n')\n",
    "        \n",
    "        ifNotSomeOfTheAllowed = re.compile(\"[^(\\s+(,)\\s+)|(\\s+(.)\\s+)|(\\s+(a)\\s+)|(\\s+(i)\\s+)]\")\n",
    "        isNum = re.compile(\"[0-9]\")\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for sent in sentList:\n",
    "                    wordList = sent.split()\n",
    "                    for word in wordList:\n",
    "                        if (len(word) == 1):\n",
    "                            if ifNotSomeOfTheAllowed.match(word):\n",
    "                                #word = re.sub(ifNotSomeOfTheAllowed, \" \", item)\n",
    "                                if isNum.match(word):\n",
    "                                    #replace with NUMBER\n",
    "                                    oFile.write(\"NUMBER \")\n",
    "                                else:\n",
    "                                    oFile.write(\"OMITTED \")\n",
    "                            else:\n",
    "                                oFile.write(word+\" \")\n",
    "                        else:\n",
    "                            oFile.write(word+\" \")  \n",
    "                    oFile.write(\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "def prepForLabeling(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sentNoPunct2.txt\"):\n",
    "        print(\"[Removing functions and single characters: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        #print(\"BASE NAME: \", baseName)\n",
    "        \n",
    "        OFName = baseName + \".en_labels.txt\"\n",
    "        \n",
    "        sentList = iFile.read().split('\\n')\n",
    "        \n",
    "        try:\n",
    "            with open(OFName, \"w\") as oFile:\n",
    "                for sent in sentList:\n",
    "                    if not len(sent) == 0:\n",
    "                        oFile.write(\"|\"+sent+\"\\n\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CANNOT PROCESS (FileNotFoundError): \" + curFile.name)\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function #5 - partOfSpeechTag Tagging\n",
    "def POStag(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_noStopWordsALL.txt\"):\n",
    "        print(\"[Part of Speech tagging: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokPOStag.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:            \n",
    "            taggedTok = pos_tag(tokenList)\n",
    "            \n",
    "            for tok in taggedTok:\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\"}.union(string.ascii_lowercase)\n",
    "                if tok[0] in UNallowedPunct:\n",
    "                    # if token is a punctuation symbol, don't save in the output\n",
    "                    pass\n",
    "                else:\n",
    "                    #print(tok[0],',',tok[1])    # output: now , RB\n",
    "                    #print(tok)                  # output: ('now', 'RB')\n",
    "                    outpLine = tok[0]+\",\"+tok[1]\n",
    "                    oFile.write(outpLine+\"\\n\")\n",
    "                    #oFile.write(str(tok)+\"\\n\")\n",
    "                    \n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Outputs data as a tuple (Word,WordNetPOSTag,Stem)\n",
    "\n",
    "POS TAG: read line by line, split each line by (\",\"), write line[0] to the file with comma, following it\n",
    "STEM: take line[1] and compare POS tag, return the appropriate POS tag as per Wordnet, i.e. \n",
    "r for adverb\n",
    "a for adjective\n",
    "n for noun\n",
    "v for verb\n",
    "\"\"\"\n",
    "def stem(iFile, oPathNoExt):\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    if not iFile.name.endswith(\"_tokPOStag.txt\"):\n",
    "    #if not iFile.name.endswith(\"NounVerbAdjectiveAdverb.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Stemming: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_stemmedbyPOS.txt\"\n",
    "        \n",
    "        content = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:   \n",
    "           \n",
    "            for line in content:\n",
    "                curline = line.split(\",\")    #list with 2 elements\n",
    "                word = curline[0]\n",
    "                posTag = curline[1]\n",
    "                \n",
    "                stem = stemmer.stem(word)\n",
    "                \n",
    "                if(penn_to_wn(posTag) == \"a\"):\n",
    "                    oFile.write(word+\",a,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"n\"):\n",
    "                    oFile.write(word+\",n,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"r\"):\n",
    "                    oFile.write(word+\",r,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"v\"):\n",
    "                    oFile.write(word+\",v,\"+stem+\"\\n\")\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            oFile.close() \n",
    "            \n",
    "# Requires the output from stem() and more specifically the POS tag in order to know what \n",
    "# word to make out of the stem. If not specific, it assumes NOUN\n",
    "def lemmatize(iFile, oPathNoExt):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if not iFile.name.endswith(\"en_stemmedbyPOS.txt\"):\n",
    "    #if not iFile.name.endswith(\"NounVerbAdjectiveAdverb.en_stemmedbyPOS.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Lemmatizing: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_lemmatized.txt\"\n",
    "        \n",
    "        content = iFile.read().split()\n",
    "        \n",
    "        #print(\"Word | Stem | Lemma | LemmaPOS\")\n",
    "        #print(\"------------------------------\")\n",
    "        with open(OFName, \"w\") as oFile:    \n",
    "            for line in content:\n",
    "                curline = line.split(\",\")    #list with 2 elements\n",
    "                word = curline[0]\n",
    "                posTag = curline[1]\n",
    "                stem = curline[2]\n",
    "                \n",
    "                lemma = nltk.stem.WordNetLemmatizer().lemmatize(stem)\n",
    "                lemmaPOS = nltk.stem.WordNetLemmatizer().lemmatize(stem, 'v')\n",
    "                #print(word+\" | \"+stem+\" | \"+lemma+\" | \"+lemmaPOS)\n",
    "                oFile.write(lemmaPOS+\"\\n\")\n",
    "            \n",
    "            oFile.close()\n",
    "            \n",
    "def cleanup(iFile):\n",
    "    \"\"\"\n",
    "    if iFile.name.endswith(\"_sentNoPunctWithCommDot.txt\"):\n",
    "        if('.en' not in iFile.name):\n",
    "            try:\n",
    "                print(\"DELETING FILE: \",iFile.name)\n",
    "                os.remove(iFile.name)\n",
    "            except PermissionError as err:\n",
    "                print(\"SKIPPING: (FileInUse)\",err,\"\\n\")\n",
    "                pass\n",
    "    \"\"\"\n",
    "    if not iFile.name.endswith(\".en_labels.txt\"):\n",
    "        try:\n",
    "            print(\"DELETING FILE: \",iFile.name)\n",
    "            os.remove(iFile.name)\n",
    "        except PermissionError as err:\n",
    "            print(\"SKIPPING: (FileInUse)\",err,\"\\n\")\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_how-do-i-find-the-maximum-and-minimum-values-of-f-on-a-given-domain.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_how-fast-does-a-ball-move.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_how-fast-does-the-shadow-move.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-antidifferentiation-rule-corresponds-to-the-product-rule-in-reverse.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-does-dx-mean-by-itself.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-happens-when-i-use-thin-horizontal-rectangles-to-compute-area.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-an-antiderivative-of-sin-2n-1-x-cos-2n-x-dx.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-the-big-deal-about-the-fundamental-theorem-of-calculus.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-the-derivative-of-f-x-g-x.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-the-derivative-of-sine-and-cosine.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-the-difference-between-potential-and-actual-infinity.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-the-integral-of-x-2-from-x-0-to-1.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_what-is-the-meaning-of-the-derivative-of-the-derivative.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_why-is-calculus-going-to-be-so-much-fun.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\01_why-is-the-derivative-of-x-2-equal-to-2x.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_how-do-you-design-the-best-soup-can.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_what-is-an-antiderivative-for-e-x-2.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_what-is-an-antiderivative-for-x-n.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_what-is-the-area-between-the-graphs-of-y-x-2-and-y-1-x-2.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_what-is-the-derivative-of-1-2x-5-and-sqrt-x-2-0-0001.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_what-is-the-limit-of-sin-1-x.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_what-is-up-with-all-the-numerical-analysis-this-week.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_when-are-two-functions-the-same.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\02_why-does-f-x-0-imply-that-f-is-increasing.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\03_do-all-local-minimums-look-basically-the-same-when-you-zoom-in.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\03_how-can-i-approximate-sin-1.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\03_how-should-i-handle-the-endpoints-when-doing-u-substitution.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\03_what-is-logarithmic-differentiation.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\03_why-is-the-limit-of-2x-as-x-approaches-10-equal-to-20.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_bonus-how-does-one-prove-the-chain-rule.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_how-can-i-approximate-root-two.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_how-does-wiggling-x-affect-f-x.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_physically-why-is-the-fundamental-theorem-of-calculus-true.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_what-is-a-slope-field.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_what-is-the-integral-of-dx-1-cos-x.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_what-is-the-sum-of-the-first-k-perfect-squares.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\calculus1\\04_what-is-the-volume-of-a-thin-shell.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\01_file-formats.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\01_protecting-confidentiality-part-1.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\01_research-data-defined.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\02_backup.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\02_data-curation-standards-and-best-practices-part-1.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\02_what-are-the-drawbacks-of-sharing-data.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\03_data-management-across-the-research-lifecycle.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\03_data-management-plan-content.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\data-management\\03_how-does-good-data-management-add-value-to-research.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\01_1-1-introduction.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\02_additional-lecture-loops.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\02_additional-lecture-variables.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\04_3-1-transforms-part-1.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\08_4-3-preparing-and-playing-sound-fx.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\08_5-3-part-2-music-machine.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\10_2-5-djtube.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\12_1-4-running-apps-on-ios-and-android.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\digitalmedia\\14_3-5-building-audio-visualisers.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\embedded-operating-system\\01_input-output-devices.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\embedded-operating-system\\01_use-cases-of-micro-controller-platforms.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\embedded-operating-system\\02_the-contiki-system.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\embedded-operating-system\\04_the-modular-kernel.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\embedded-operating-system\\07_protothread-multithreading-and-code-sizes.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\03_1-2-desktop-icons.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\03_2-2-pen-and-ink-textures.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\05_3-3-shape-control-by-curves.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\05_4-3-spatial-key-framing.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\05_5-3-chairs.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\05_6-3-garments.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\interactive-computer-graphics\\05_7-3-actuated-puppet.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\iot-connectivity-security\\01_embeddable-webservers.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\iot-connectivity-security\\01_introduction-to-cps-security-and-privacy.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\iot-connectivity-security\\01_introduction-to-cps-web-connectivity.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\iot-connectivity-security\\01_m2m-connectivity-protocols.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\iot-connectivity-security\\01_symmetric-key-ciphers-and-wireless-lan-security.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\logic-introduction\\01_the-big-game.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\04_sensors.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\05_observers.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\05_other-robot-classes.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\05_stability.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\05_the-bouncing-ball.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\05_the-induced-mode.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\06_cruise-controllers.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\10_programming-simulation-lecture-2.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\10_programming-simulation-lecture-3.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\10_programming-simulation-lecture-4.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\10_programming-simulation-lecture-5.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\10_programming-simulation-lecture-6.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\10_programming-simulation-lecture-7.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\mobile-robot\\11_glue-lecture-1.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_2-2-nuclear-size-and-spin.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_2-5-beta-and-gamma-decay.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_2-9-nuclear-power.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_3-10-spectrometers-and-calorimeters.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_3-5-light-particles-in-matter.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_4-2-electromagnetic-scattering.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_5-1-elastic-electron-nucleon-scattering.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_6-1-particles-and-antiparticles.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_6-11-the-higgs-mechanism.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_6-6-the-z-boson.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_7-3-hunting-peaks.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\01_8-3-dark-energy.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\02_1-4a-rutherford-cross-section-optional.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\02_3-1a-cyclotron-frequency-optional.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\particle-physics\\03_1-2b-special-relativity-and-four-vectors-optional.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\01_1-continuous-system-super-particle-theorem.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\01_5-rigid-body-angular-momentum.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\01_module-4-introduction.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\03_1-1-gravity-gradient-torque-in-body-frame.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\04_9-example-dual-spinner-stability.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\05_3-torque-free-motion-general-inertia-case.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\06_4-vscmg-eom-variations.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\06_8-rigid-body-equations-of-motion.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\10_6-example-phase-space-plot-of-duffing-equation.en_sent.txt\n",
      "DELETING FILE:  C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Data for Crowdsourcing - UNLABELED\\spacecraft-dynamics-kinetics\\11_optional-review-rigid-body-equations-of-motion.en_sent.txt\n",
      "\n",
      "Total number of 224 TXT files found.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------- MAIN PROGRAM  -------------------------------------------------------\n",
    "\n",
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Selected files TRAIN\"\n",
    "#path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\PROCESSED\\Coursera Downloads PreProcessed\"\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            else: \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    #fileName = print(os.path.abspath(filePath))\n",
    "                    #curFile = open(filePath, 'r', encoding = \"ascii\")\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "\n",
    "                    try:\n",
    "                        fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                        #outpFileBase = open(FileExtRemoved + \"_PROC.txt\",\"w\")\n",
    "                        \n",
    "                        \"\"\"\n",
    "                        call each processing function here and pass it the file\n",
    "                        First argument: current input file\n",
    "                        Second argument: path without extension of the current file\n",
    "                        the path will be used to save the output file with the same name and same location\n",
    "                        but with different file ending based on what the fuunction did\n",
    "\n",
    "                        Functions take input files ending on:\n",
    "                        splitSentences()\t>>\t_Raw.txt\n",
    "                        sentTokenize()\t\t>>\t_sent.txt\n",
    "                        POStag()\t\t\t>>\t_noStopWordsALL.txt\n",
    "                        rmStopWords()\t\t>>\t_tokens.txt\n",
    "                        stem()\t\t\t\t>>\t_tokPOStag.txt\n",
    "                        lemmatize()\t\t\t>>\t_stemmed.txt\n",
    "                        rmPunctuationSent()\t>>\t_sent.txt\n",
    "                        stripMultiSpace\t\t>>\t_sentNoPunctWithCommDot\n",
    "                        rmSWSent \t\t\t>>\t_sentNoPunctEnhanced\n",
    "                        \"\"\"  \n",
    "                        #splitSentences(curFile, fileExtRemoved)\n",
    "                        #sentTokenize(curFile, fileExtRemoved)\n",
    "                        #rmStopWords(curFile, fileExtRemoved)\n",
    "                        #POStag(curFile, fileExtRemoved)\n",
    "                        #stem(curFile, fileExtRemoved)\n",
    "                        #lemmatize(curFile, fileExtRemoved)\n",
    "                        #rmPunctuationSent(curFile, fileExtRemoved)\n",
    "                        #stripMultiSpace(curFile)\n",
    "                        #rmSWSent(curFile, fileExtRemoved)\n",
    "                        #replaceNumsAndFunc(curFile, fileExtRemoved)\n",
    "                        #prepForLabeling(curFile, fileExtRemoved)\n",
    "                        \n",
    "                        curFile.close()\n",
    "                        #cleanup(curFile)\n",
    "                        \n",
    "                        # for sentence processing: run one after another: rmPunctuationSent(), replaceNumsAndFunc(),\n",
    "                        # prepForLabeling() and cleanup()\n",
    "                        \n",
    "                    except UnicodeEncodeError:\n",
    "                        print(\"CANNOT PROCESS (Encoding err): \" + curFile.name)\n",
    "                        pass\n",
    "                    except FileNotFoundError:\n",
    "                        print(\"CANNOT PROCESS (FileNotFoundError err): \" + curFile.name)\n",
    "                        pass\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "                    \n",
    "                    \n",
    "        \"\"\" #### USE ONLY TO CLEANUP DATA - DELETED SRT files\n",
    "        if filePath.endswith(\".srt\"):\n",
    "            curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "            curFile.close()\n",
    "            cleanup(curFile)\n",
    "        \"\"\" \n",
    "        \n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "---------- offtopic ---------\n",
    "\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
