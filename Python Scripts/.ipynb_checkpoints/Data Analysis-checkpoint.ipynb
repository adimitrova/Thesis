{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "#### Steps:\n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams\n",
    "\n",
    "---\n",
    "\n",
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "------\n",
    "\n",
    "#### Offtopic\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import pathlib\n",
    "from unidecode import unidecode\n",
    "import pprint\n",
    "from tabulate import tabulate\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "#### ---------------\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet.\n",
    "\n",
    "##### Term frequency\n",
    "\\\\( tf(t,d) = 0.5 + 0.5 * (\\frac{f_{t,d}}{f_{t',d}:t' \\in d}) \\\\) \n",
    "\n",
    "##### Inversed document frequency\n",
    "\\\\( idf(t,D) = log * (\\frac{N}{d \\in D  :  t \\in d}) \\\\)\n",
    "\n",
    "##### Computing tf-idf\n",
    "\\\\( tfidf(t,d,D) = tf(t,d) * idf(t,D) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TF-IDF with data\n",
    "\n",
    "#### TODO: Fix the input, it takes strings, and not files right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in docnames[]: 9\n",
      "Total number of files in doclist[]: 9\n"
     ]
    }
   ],
   "source": [
    "# traverse each folder and sub-folder\n",
    "# create an array of files to add each file in it\n",
    "# if the file is TXT, add to the array\n",
    "# create a String array of documents with the file of the array with files \n",
    "# so we can store the contents of each inside\n",
    "# read each line of each file and save to the strings\n",
    "# perform algorithms on the documents\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# WINDOWS\n",
    "#path = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\one file\"\n",
    "path = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\PROCESSED\\Course-data\\mobile-robot\\02_mobile-robots\\01_week-2\"\n",
    "\n",
    "# LINUX\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "docnames = []\n",
    "counter = 0\n",
    "\n",
    "# DOCUMENT LIST CONSISTS OF TEXTBLOB files. All input files need to be converted to TEXTBLOB \n",
    "# and then saved in this list in order for TF-IDF to work\n",
    "doclist = []\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            elif filePath.endswith(\"_lemmatized.txt\"): \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    docnames.append(curFile)\n",
    "                    \n",
    "                    fcontentTBlob = tb(curFile.read())\n",
    "                    #print(fcontentTBlob)\n",
    "                    doclist.append(fcontentTBlob)\n",
    "                    \n",
    "                    # bag of words processing:\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(\"Total number of files in docnames[]:\", len(docnames))\n",
    "print(\"Total number of files in doclist[]:\", len(docnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 terms in document 1 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\01_driving-robots-around.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: plan \t|\t TF-IDF: 0.02019\n",
      "\tTERM: behavy \t|\t TF-IDF: 0.01088\n",
      "\tTERM: adv \t|\t TF-IDF: 0.01009\n",
      "\tTERM: sun \t|\t TF-IDF: 0.00737\n",
      "\tTERM: understand \t|\t TF-IDF: 0.00737\n",
      "\tTERM: allow \t|\t TF-IDF: 0.00737\n",
      "\tTERM: discuss \t|\t TF-IDF: 0.00726\n",
      "\tTERM: target \t|\t TF-IDF: 0.00673\n",
      "\tTERM: chair \t|\t TF-IDF: 0.00673\n",
      "\tTERM: tre \t|\t TF-IDF: 0.00673\n",
      "\n",
      "Top 10 terms in document 2 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\02_differential-drive-robots.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: sub \t|\t TF-IDF: 0.03084\n",
      "\tTERM: paramet \t|\t TF-IDF: 0.01735\n",
      "\tTERM: input \t|\t TF-IDF: 0.0128\n",
      "\tTERM: dot \t|\t TF-IDF: 0.01237\n",
      "\tTERM: wheel \t|\t TF-IDF: 0.01014\n",
      "\tTERM: model \t|\t TF-IDF: 0.00838\n",
      "\tTERM: turn \t|\t TF-IDF: 0.00782\n",
      "\tTERM: vr \t|\t TF-IDF: 0.00771\n",
      "\tTERM: vl \t|\t TF-IDF: 0.00771\n",
      "\tTERM: rat \t|\t TF-IDF: 0.00722\n",
      "\n",
      "Top 10 terms in document 3 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\03_odometry.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: tick \t|\t TF-IDF: 0.01747\n",
      "\tTERM: count \t|\t TF-IDF: 0.0162\n",
      "\tTERM: extern \t|\t TF-IDF: 0.01381\n",
      "\tTERM: wheel \t|\t TF-IDF: 0.01173\n",
      "\tTERM: eiffel \t|\t TF-IDF: 0.0081\n",
      "\tTERM: tow \t|\t TF-IDF: 0.0081\n",
      "\tTERM: drift \t|\t TF-IDF: 0.0081\n",
      "\tTERM: encod \t|\t TF-IDF: 0.00801\n",
      "\tTERM: gps \t|\t TF-IDF: 0.00789\n",
      "\tTERM: dist \t|\t TF-IDF: 0.00739\n",
      "\n",
      "Top 10 terms in document 4 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\04_sensors.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: disk \t|\t TF-IDF: 0.03532\n",
      "\tTERM: skirt \t|\t TF-IDF: 0.02355\n",
      "\tTERM: obstac \t|\t TF-IDF: 0.0207\n",
      "\tTERM: abstract \t|\t TF-IDF: 0.01587\n",
      "\tTERM: chiper \t|\t TF-IDF: 0.01177\n",
      "\tTERM: glob \t|\t TF-IDF: 0.01177\n",
      "\tTERM: meet \t|\t TF-IDF: 0.00883\n",
      "\tTERM: grav \t|\t TF-IDF: 0.0086\n",
      "\tTERM: ring \t|\t TF-IDF: 0.00793\n",
      "\tTERM: red \t|\t TF-IDF: 0.00793\n",
      "\n",
      "Top 10 terms in document 5 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\05_behavior-based-robotics.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: er \t|\t TF-IDF: 0.01904\n",
      "\tTERM: tan \t|\t TF-IDF: 0.01564\n",
      "\tTERM: omeg \t|\t TF-IDF: 0.00984\n",
      "\tTERM: integr \t|\t TF-IDF: 0.00952\n",
      "\tTERM: pi \t|\t TF-IDF: 0.00828\n",
      "\tTERM: libr \t|\t TF-IDF: 0.00782\n",
      "\tTERM: kp \t|\t TF-IDF: 0.00782\n",
      "\tTERM: rady \t|\t TF-IDF: 0.00782\n",
      "\tTERM: eq \t|\t TF-IDF: 0.00762\n",
      "\tTERM: ref \t|\t TF-IDF: 0.00762\n",
      "\n",
      "Top 10 terms in document 6 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\07_the-grits-simulator.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: paramet \t|\t TF-IDF: 0.01916\n",
      "\tTERM: gain \t|\t TF-IDF: 0.0165\n",
      "\tTERM: sim \t|\t TF-IDF: 0.01532\n",
      "\tTERM: download \t|\t TF-IDF: 0.01312\n",
      "\tTERM: overshoot \t|\t TF-IDF: 0.01312\n",
      "\tTERM: goal \t|\t TF-IDF: 0.01025\n",
      "\tTERM: kheper \t|\t TF-IDF: 0.00958\n",
      "\tTERM: entir \t|\t TF-IDF: 0.00874\n",
      "\tTERM: stabl \t|\t TF-IDF: 0.00874\n",
      "\tTERM: jp \t|\t TF-IDF: 0.00874\n",
      "\n",
      "Top 10 terms in document 7 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\08_obstacle-avoidance.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: goal \t|\t TF-IDF: 0.02838\n",
      "\tTERM: obstac \t|\t TF-IDF: 0.02594\n",
      "\tTERM: avoid \t|\t TF-IDF: 0.01901\n",
      "\tTERM: obst \t|\t TF-IDF: 0.01245\n",
      "\tTERM: cho \t|\t TF-IDF: 0.01212\n",
      "\tTERM: blend \t|\t TF-IDF: 0.01037\n",
      "\tTERM: analys \t|\t TF-IDF: 0.01037\n",
      "\tTERM: combin \t|\t TF-IDF: 0.00909\n",
      "\tTERM: ad \t|\t TF-IDF: 0.00909\n",
      "\tTERM: pur \t|\t TF-IDF: 0.00909\n",
      "\n",
      "Top 10 terms in document 8 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\09_glue-lecture-2.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: guy \t|\t TF-IDF: 0.01485\n",
      "\tTERM: omeg \t|\t TF-IDF: 0.01315\n",
      "\tTERM: feedback \t|\t TF-IDF: 0.0122\n",
      "\tTERM: angul \t|\t TF-IDF: 0.00877\n",
      "\tTERM: mus \t|\t TF-IDF: 0.00813\n",
      "\tTERM: model \t|\t TF-IDF: 0.00725\n",
      "\tTERM: walk \t|\t TF-IDF: 0.00693\n",
      "\tTERM: find \t|\t TF-IDF: 0.00658\n",
      "\tTERM: equ \t|\t TF-IDF: 0.00658\n",
      "\tTERM: okay \t|\t TF-IDF: 0.00621\n",
      "\n",
      "Top 10 terms in document 9 | <_io.TextIOWrapper name='C:\\\\Users\\\\a.dimitrova\\\\Desktop\\\\TEST data\\\\PROCESSED\\\\Course-data\\\\mobile-robot\\\\02_mobile-robots\\\\01_week-2\\\\10_programming-simulation-lecture-2.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: estim \t|\t TF-IDF: 0.02482\n",
      "\tTERM: funct \t|\t TF-IDF: 0.02357\n",
      "\tTERM: assign \t|\t TF-IDF: 0.01241\n",
      "\tTERM: retriev \t|\t TF-IDF: 0.01241\n",
      "\tTERM: cod \t|\t TF-IDF: 0.01088\n",
      "\tTERM: cur \t|\t TF-IDF: 0.01088\n",
      "\tTERM: dist \t|\t TF-IDF: 0.01067\n",
      "\tTERM: superv \t|\t TF-IDF: 0.00993\n",
      "\tTERM: week \t|\t TF-IDF: 0.00906\n",
      "\tTERM: angl \t|\t TF-IDF: 0.00873\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ TF-IDF --------------------------------------------------------\n",
    "\n",
    "# arrays to hold the terms found in text and also a custom list to test domain-specific terms\n",
    "exportedList = []\n",
    "ownList = {\"data management\",\"database\",\"example\",\"iot\",\"lifecycle\",\"bloom\",\"filter\",\"integrity\",\n",
    "           \"java\",\"pattern\",\"design pattern\",\"svm\",\"Support vector machine\",\"knn\",\"k-nearest neighbors\",\"machine learning\"}\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.column_headers = [\"TERM\", \"TF-IDF\"]\n",
    "\n",
    "topNwords = 10;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topNwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "        #print(table.append_row([term, round(score, 5)]))\n",
    "        print(\"\\tTERM: {} \\t|\\t TF-IDF: {}\".format(term, round(score, 5)))\n",
    "        exportedList.append(term)\n",
    "        #print tabulate([term, round(score, 5)], headers=['tTERM', 'TF-IDF'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- EXPORTED TERMS in WORDNET ----------\n",
      "\n",
      " 'm : NO SYNSETS\n",
      "\n",
      "\n",
      " interv : NO SYNSETS\n",
      "\n",
      "\n",
      " differenty : NO SYNSETS\n",
      "\n",
      "\n",
      " definit : NO SYNSETS\n",
      "\n",
      "\n",
      " limit\n",
      "-  limit.n.01  |  the greatest possible degree of something\n",
      "-  terminus_ad_quem.n.01  |  final or latest limiting point\n",
      "-  limit.n.03  |  as far as something can go\n",
      "-  limit.n.04  |  the boundary of a specific area\n",
      "-  limit.n.05  |  the mathematical value toward which a function goes as the independent variable approaches infinity\n",
      "-  limit.n.06  |  the greatest amount of something that is possible or allowed\n",
      "-  restrict.v.03  |  place limits on (extent or access)\n",
      "-  limit.v.02  |  restrict or confine, \"I limit you to two visits to the pub a day\"\n",
      "-  specify.v.02  |  decide upon or fix definitely\n",
      "\n",
      " tang\n",
      "-  nip.n.05  |  a tart spicy quality\n",
      "-  tang.n.02  |  the imperial dynasty of China from 618 to 907\n",
      "-  relish.n.03  |  the taste experience when a savoury condiment is taken into the mouth\n",
      "-  bladderwrack.n.02  |  a common rockweed used in preparing kelp and as manure\n",
      "-  serrated_wrack.n.01  |  brown algae seaweed with serrated edges\n",
      "-  tang.n.06  |  any of various coarse seaweeds\n",
      "-  sea_tangle.n.01  |  any of various kelps especially of the genus Laminaria\n",
      "\n",
      " lin\n",
      "-  lin.n.01  |  United States sculptor and architect whose public works include the memorial to veterans of the Vietnam War in Washington (born in 1959)\n",
      "\n",
      " know\n",
      "-  know.n.01  |  the fact of being aware of information that is known to few people\n",
      "-  know.v.01  |  be cognizant or aware of a fact or a specific piece of information; possess knowledge or information about\n",
      "-  know.v.02  |  know how to do or perform something\n",
      "-  know.v.03  |  be aware of the truth of something; have a belief or faith in something; regard as true beyond any doubt\n",
      "-  know.v.04  |  be familiar or acquainted with a person or an object\n",
      "-  know.v.05  |  have firsthand knowledge of states, situations, emotions, or sensations\n",
      "-  acknowledge.v.06  |  accept (someone) to be what is claimed or accept his power and authority\n",
      "-  know.v.07  |  have fixed in the mind\n",
      "-  sleep_together.v.01  |  have sexual intercourse with\n",
      "-  know.v.09  |  know the nature or character of\n",
      "-  know.v.10  |  be able to distinguish, recognize as being different\n",
      "-  know.v.11  |  perceive as familiar\n",
      "\n",
      " slop\n",
      "-  slop.n.01  |  wet feed (especially for pigs) consisting of mostly kitchen waste mixed with water or skimmed or sour milk\n",
      "-  slop.n.02  |  deep soft mud in water or slush\n",
      "-  slop.n.03  |  (usually plural) waste water from a kitchen or bathroom or chamber pot that has to be emptied by hand\n",
      "-  slop.n.04  |  (usually plural) weak or watery unappetizing food or drink\n",
      "-  treacle.n.02  |  writing or music that is excessively sweet and sentimental\n",
      "-  spill.v.01  |  cause or allow (a liquid substance) to run or flow from a container\n",
      "-  squelch.v.03  |  walk through mud or mire\n",
      "-  slop.v.03  |  ladle clumsily\n",
      "-  slop.v.04  |  feed pigs\n",
      "\n",
      " mov : NO SYNSETS\n",
      "\n",
      "\n",
      " effect\n",
      "-  consequence.n.01  |  a phenomenon that follows and is caused by some previous phenomenon\n",
      "-  impression.n.02  |  an outward appearance\n",
      "-  effect.n.03  |  an impression (especially one that is artificial or contrived)\n",
      "-  effect.n.04  |  the central meaning or theme of a speech or literary work\n",
      "-  effect.n.05  |  (of a law) having legal validity\n",
      "-  effect.n.06  |  a symptom caused by an illness or a drug\n",
      "-  effect.v.01  |  produce\n",
      "-  effect.v.02  |  act so as to bring into existence\n",
      "\n",
      " difinit : NO SYNSETS\n",
      "\n",
      "\n",
      " allow\n",
      "-  let.v.01  |  make it possible through a specific action or lack of action for something to happen\n",
      "-  permit.v.01  |  consent to, give permission\n",
      "-  allow.v.03  |  let have\n",
      "-  allow.v.04  |  give or assign a resource to a particular person or cause\n",
      "-  leave.v.06  |  make a possibility or provide opportunity for; permit to be attainable or cause to remain\n",
      "-  allow.v.06  |  allow or plan for a certain possibility; concede the truth or validity of something\n",
      "-  admit.v.05  |  afford possibility\n",
      "-  give_up.v.11  |  allow the other (baseball) team to score\n",
      "-  allow.v.09  |  grant as a discount or in exchange\n",
      "-  allow.v.10  |  allow the presence of or allow (an activity) without opposing or prohibiting\n",
      "\n",
      " exact\n",
      "-  demand.v.03  |  claim as due or just\n",
      "-  claim.v.05  |  take as an undesirable consequence of some event or state of affairs\n",
      "-  exact.a.01  |  marked by strict and particular and complete accordance with fact\n",
      "-  accurate.s.02  |  (of ideas, images, representations, expressions) characterized by perfect conformity to fact or truth; strictly correct\n",
      "\n",
      " want\n",
      "-  privation.n.01  |  a state of extreme poverty\n",
      "-  lack.n.01  |  the state of needing something that is absent or unavailable\n",
      "-  need.n.02  |  anything that is necessary but lacking\n",
      "-  wish.n.01  |  a specific feeling of desire\n",
      "-  desire.v.01  |  feel or have a desire for; want strongly\n",
      "-  want.v.02  |  have need of\n",
      "-  want.v.03  |  hunt or look for; want for a particular reason\n",
      "-  want.v.04  |  wish or demand the presence of\n",
      "-  want.v.05  |  be without, lack; be deficient in\n",
      "\n",
      "\n",
      "------- CUSTOM TERMS in WORDNET (also domain specific) ----------\n",
      "\n",
      " knn : NO SYNSETS\n",
      "\n",
      "\n",
      " svm : NO SYNSETS\n",
      "\n",
      "\n",
      " bloom\n",
      "-  blooming.n.01  |  the organic process of bearing flowers\n",
      "-  flower.n.02  |  reproductive organ of angiosperm plants especially one having showy or colorful parts\n",
      "-  bloom.n.03  |  the best time of youth\n",
      "-  bloom.n.04  |  a rosy color (especially in the cheeks) taken as a sign of good health\n",
      "-  flower.n.03  |  the period of greatest prosperity or productivity\n",
      "-  efflorescence.n.04  |  a powdery deposit on a surface\n",
      "-  bloom.v.01  |  produce or yield flowers\n",
      "\n",
      " design pattern : NO SYNSETS\n",
      "\n",
      "\n",
      " example\n",
      "-  example.n.01  |  an item of information that is typical of a class or group\n",
      "-  model.n.07  |  a representative form or pattern\n",
      "-  exemplar.n.01  |  something to be imitated\n",
      "-  example.n.04  |  punishment intended as a warning to others\n",
      "-  case.n.01  |  an occurrence of something\n",
      "-  exercise.n.04  |  a task performed or problem solved in order to develop skill or understanding\n",
      "\n",
      " java\n",
      "-  java.n.01  |  an island in Indonesia to the south of Borneo; one of the world's most densely populated regions\n",
      "-  coffee.n.01  |  a beverage consisting of an infusion of ground coffee beans\n",
      "-  java.n.03  |  a platform-independent object-oriented programming language\n",
      "\n",
      " lifecycle : NO SYNSETS\n",
      "\n",
      "\n",
      " integrity\n",
      "-  integrity.n.01  |  an undivided or unbroken completeness or totality with nothing wanting\n",
      "-  integrity.n.02  |  moral soundness\n",
      "\n",
      " k-nearest neighbors : NO SYNSETS\n",
      "\n",
      "\n",
      " iot : NO SYNSETS\n",
      "\n",
      "\n",
      " data management : NO SYNSETS\n",
      "\n",
      "\n",
      " pattern\n",
      "-  form.n.03  |  a perceptual structure\n",
      "-  practice.n.01  |  a customary way of operation or behavior\n",
      "-  design.n.04  |  a decorative or artistic work\n",
      "-  convention.n.02  |  something regarded as a normative example\n",
      "-  pattern.n.05  |  a model considered worthy of imitation\n",
      "-  blueprint.n.01  |  something intended as a guide for making something else\n",
      "-  traffic_pattern.n.01  |  the path that is prescribed for an airplane that is preparing to land at an airport\n",
      "-  radiation_pattern.n.01  |  graphical representation (in polar or Cartesian coordinates) of the spatial distribution of radiation from an antenna as a function of angle\n",
      "-  model.v.01  |  plan or create according to a model or models\n",
      "-  pattern.v.02  |  form a pattern\n",
      "\n",
      " machine learning : NO SYNSETS\n",
      "\n",
      "\n",
      " filter\n",
      "-  filter.n.01  |  device that removes something from whatever passes through it\n",
      "-  filter.n.02  |  an electrical device that alters the frequency spectrum of signals passing through it\n",
      "-  filter.v.01  |  remove by passing through a filter\n",
      "-  percolate.v.05  |  pass through\n",
      "-  trickle.v.01  |  run or flow slowly, as in drops or in an unsteady stream\n",
      "\n",
      " database\n",
      "-  database.n.01  |  an organized body of related information\n",
      "\n",
      " Support vector machine : NO SYNSETS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------- NLTK, WORDNET -------------------------------------------\n",
    "print(\"\\n\\n------- EXPORTED TERMS in WORDNET ----------\") \n",
    "for word in exportedList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "\n",
    "print(\"\\n\\n------- CUSTOM TERMS in WORDNET (also domain specific) ----------\")    \n",
    "for word in ownList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words, and all the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "\n",
    "# BoF\n",
    "def bagOfWords(iFile):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    data_corpus = []\n",
    "    #X = vectorizer.fit_transform(data_corpus) \n",
    "    #print(X.toarray())\n",
    "    #print(vectorizer.get_feature_names())    \n",
    "    \n",
    "    if iFile.name.endswith(\"_lemmatized.txt\"):\n",
    "        print(\"\\n[Bag of words: ]\\t\" + os.path.basename(iFile.name))\n",
    "        baseName = iFile.name.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_tokPOStag.txt\"\n",
    "        fileContent = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            text = \"\"\n",
    "            for line in fileContent:\n",
    "                text += line+\" \"\n",
    "                findRealTerm(iFile,line)\n",
    "            data_corpus.append(text)\n",
    "        \n",
    "        #print(data_corpus)\n",
    "        vector = vectorizer.fit_transform(data_corpus) \n",
    "        #print(vector.toarray())\n",
    "        #print(vectorizer.get_feature_names())    \n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# ----------------------------------------- FINISH ---------------------------------------------\n",
    "\n",
    "# Match the lemma to first found word from file _lemmatized.txt for that file\n",
    "def findRealTerm(iFile, lemmaIn):   \n",
    "    # take the path of the input file and look for the file ending on \"_stemmedbyPOS.txt\"\n",
    "    # split each line into 3, look in line[3] for the first match of the current lemma\n",
    "    # when found, take line[0] which is the full word and return that word\n",
    "    # exit the function\n",
    "    #print(\"TODO findteam\")\n",
    "    \n",
    "    #current_dir = pathlib.Path(iFile).parent\n",
    "    #fileExtRemoved = os.path.splitext(os.path.abspath(iFile.path))[0]\n",
    "    path = os.path.splitext(os.path.abspath(iFile.path))[1]\n",
    "    #iFileCurDir = path.lastIndexOf(\"\\\\\")\n",
    "    print(path)\n",
    "    \n",
    "    baseName = iFile.name.split(\".en\", 1)[0]\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(baseName+'.en_stemmedbyPOS.txt'):\n",
    "                print(file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Bag of words: ]\t01_driving-robots-around.en_lemmatized.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-0fcec90e2803>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[1;31m# bag of words processing:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                     \u001b[0mbagOfWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-cb110516b158>\u001b[0m in \u001b[0;36mbagOfWords\u001b[1;34m(iFile)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileContent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mfindRealTerm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mdata_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-cb110516b158>\u001b[0m in \u001b[0;36mfindRealTerm\u001b[1;34m(iFile, lemmaIn)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m#current_dir = pathlib.Path(iFile).parent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m#fileExtRemoved = os.path.splitext(os.path.abspath(iFile.path))[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;31m#iFileCurDir = path.lastIndexOf(\"\\\\\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'path'"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\PROCESSED\\Course-data\\mobile-robot\\02_mobile-robots\\01_week-2\"\n",
    "\n",
    "# LINUX\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            elif filePath.endswith(\"_lemmatized.txt\"): \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    \n",
    "                    # bag of words processing:\n",
    "                    bagOfWords(curFile)\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(\"Total number of files in docnames[]:\", len(docnames))\n",
    "print(\"Total number of files in doclist[]:\", len(docnames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "### NOTES\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**TF-IDF** doesn't output the necessary result, I need n-grams selected as a combined keyword and these are often very general words like `for example` or `key concept` etc. in order to classify the text into the GOAL element. \n",
    "\n",
    "**TextBlob** provides options for n-grams and also connection to WordNet ontology which could be useful, so will look more into it.\n",
    "\n",
    "**WordNet** finds multiple definitions and synsets (synonyms) for most of the general words, however if provided specific e.g. computer science algorithm names, or specific terms, it doesn find any synonyms, nor descriptions of any of them.\n",
    "\n",
    "**Wikipedia** recognized some of the terms, but not all. For instance if we give it KNN it doesn't find anything, but if we give it K-nearest neighbour, if finds it. This is how the name is in Wikipedia, so that may be the reason. But on Google first returned result for KNN is this article. Same for SVM and Support vector machine. I've modified the script to return \"NO DESCRIPTION or DISAMBIGUATION\" everytime if finds nopthing ot if there's a disambiguation error, otherwise it wouldn continue checking the rest of the terms. So now it skips the error. \n",
    " \n",
    "**Full list** of identified key words so far [HERE](https://docs.google.com/spreadsheets/d/1Dj4UAh6U5jAelcsz-gDCdDE9JRVhwaNei0Ctn8m0Ui4/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
