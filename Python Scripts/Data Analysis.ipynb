{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "#### Steps:\n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams\n",
    "\n",
    "---\n",
    "\n",
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "------\n",
    "\n",
    "#### Offtopic\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "#### ---------------\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet.\n",
    "\n",
    "##### Term frequency\n",
    "\\\\( tf(t,d) = 0.5 + 0.5 * (\\frac{f_{t,d}}{f_{t',d}:t' \\in d}) \\\\) \n",
    "\n",
    "##### Inversed document frequency\n",
    "\\\\( idf(t,D) = log * (\\frac{N}{d \\in D  :  t \\in d}) \\\\)\n",
    "\n",
    "##### Computing tf-idf\n",
    "\\\\( tfidf(t,d,D) = tf(t,d) * idf(t,D) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TF-IDF with data\n",
    "\n",
    "#### TODO: Fix the input, it takes strings, and not files right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in docnames[]: 4\n",
      "Total number of files in doclist[]: 4\n"
     ]
    }
   ],
   "source": [
    "# traverse each folder and sub-folder\n",
    "# create an array of files to add each file in it\n",
    "# if the file is TXT, add to the array\n",
    "# create a String array of documents with the file of the array with files \n",
    "# so we can store the contents of each inside\n",
    "# read each line of each file and save to the strings\n",
    "# perform algorithms on the documents\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "path = \"/media/sf_Shared_Folder/TEST/RAW/03_the-end-of-limits/03_infinity-how-can-i-work-with-that\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "docnames = []\n",
    "counter = 0\n",
    "\n",
    "# DOCUMENT LIST CONSISTS OF TEXTBLOB files. All input files need to be converted to TEXTBLOB \n",
    "# and then saved in this list in order for TF-IDF to work\n",
    "doclist = []\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            elif filePath.endswith(\"_tokens.txt\"): \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    docnames.append(curFile)\n",
    "                    \n",
    "                    fcontentTBlob = tb(curFile.read())\n",
    "                    #print(fcontentTBlob)\n",
    "                    doclist.append(fcontentTBlob)\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(\"Total number of files in docnames[]:\", len(docnames))\n",
    "print(\"Total number of files in doclist[]:\", len(docnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 terms in document 1 | <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/RAW/03_the-end-of-limits/03_infinity-how-can-i-work-with-that/01_why-is-there-an-x-so-that-f-x-x.en_tokens.txt' mode='r' encoding='ISO-8859-1'>\n",
      "+---------+--------+\n",
      "|  TERM   | TF-IDF |\n",
      "+---------+--------+\n",
      "| cosine  | 0.015  |\n",
      "+---------+--------+\n",
      "|  input  | 0.011  |\n",
      "+---------+--------+\n",
      "|  fixed  | 0.007  |\n",
      "+---------+--------+\n",
      "|  point  | 0.006  |\n",
      "+---------+--------+\n",
      "| between | 0.005  |\n",
      "+---------+--------+\n",
      "\n",
      "Top 5 terms in document 2 | <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/RAW/03_the-end-of-limits/03_infinity-how-can-i-work-with-that/02_what-does-lim-f-x-infinity-mean.en_tokens.txt' mode='r' encoding='ISO-8859-1'>\n",
      "+----------+--------+\n",
      "|   TERM   | TF-IDF |\n",
      "+----------+--------+\n",
      "|  cosine  | 0.015  |\n",
      "+----------+--------+\n",
      "|  input   | 0.011  |\n",
      "+----------+--------+\n",
      "|  fixed   | 0.007  |\n",
      "+----------+--------+\n",
      "|  point   | 0.006  |\n",
      "+----------+--------+\n",
      "| between  | 0.005  |\n",
      "+----------+--------+\n",
      "| squared  | 0.014  |\n",
      "+----------+--------+\n",
      "|  within  | 0.006  |\n",
      "+----------+--------+\n",
      "|  close   | 0.005  |\n",
      "+----------+--------+\n",
      "|  three   | 0.004  |\n",
      "+----------+--------+\n",
      "| positive | 0.004  |\n",
      "+----------+--------+\n",
      "\n",
      "Top 5 terms in document 3 | <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/RAW/03_the-end-of-limits/03_infinity-how-can-i-work-with-that/03_what-is-the-limit-f-x-as-x-approaches-infinity.en_tokens.txt' mode='r' encoding='ISO-8859-1'>\n",
      "+----------+--------+\n",
      "|   TERM   | TF-IDF |\n",
      "+----------+--------+\n",
      "|  cosine  | 0.015  |\n",
      "+----------+--------+\n",
      "|  input   | 0.011  |\n",
      "+----------+--------+\n",
      "|  fixed   | 0.007  |\n",
      "+----------+--------+\n",
      "|  point   | 0.006  |\n",
      "+----------+--------+\n",
      "| between  | 0.005  |\n",
      "+----------+--------+\n",
      "| squared  | 0.014  |\n",
      "+----------+--------+\n",
      "|  within  | 0.006  |\n",
      "+----------+--------+\n",
      "|  close   | 0.005  |\n",
      "+----------+--------+\n",
      "|  three   | 0.004  |\n",
      "+----------+--------+\n",
      "| positive | 0.004  |\n",
      "+----------+--------+\n",
      "|   over   | 0.005  |\n",
      "+----------+--------+\n",
      "|    2     | 0.004  |\n",
      "+----------+--------+\n",
      "| talking  | 0.004  |\n",
      "+----------+--------+\n",
      "| divided  | 0.004  |\n",
      "+----------+--------+\n",
      "|   100    | 0.004  |\n",
      "+----------+--------+\n",
      "\n",
      "Top 5 terms in document 4 | <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/RAW/03_the-end-of-limits/03_infinity-how-can-i-work-with-that/04_why-is-infinity-not-a-number.en_tokens.txt' mode='r' encoding='ISO-8859-1'>\n",
      "+------------+--------+\n",
      "|    TERM    | TF-IDF |\n",
      "+------------+--------+\n",
      "|   cosine   | 0.015  |\n",
      "+------------+--------+\n",
      "|   input    | 0.011  |\n",
      "+------------+--------+\n",
      "|   fixed    | 0.007  |\n",
      "+------------+--------+\n",
      "|   point    | 0.006  |\n",
      "+------------+--------+\n",
      "|  between   | 0.005  |\n",
      "+------------+--------+\n",
      "|  squared   | 0.014  |\n",
      "+------------+--------+\n",
      "|   within   | 0.006  |\n",
      "+------------+--------+\n",
      "|   close    | 0.005  |\n",
      "+------------+--------+\n",
      "|   three    | 0.004  |\n",
      "+------------+--------+\n",
      "|  positive  | 0.004  |\n",
      "+------------+--------+\n",
      "|    over    | 0.005  |\n",
      "+------------+--------+\n",
      "|     2      | 0.004  |\n",
      "+------------+--------+\n",
      "|  talking   | 0.004  |\n",
      "+------------+--------+\n",
      "|  divided   | 0.004  |\n",
      "+------------+--------+\n",
      "|    100     | 0.004  |\n",
      "+------------+--------+\n",
      "| difference | 0.007  |\n",
      "+------------+--------+\n",
      "| seventeen  | 0.005  |\n",
      "+------------+--------+\n",
      "|    x^2     | 0.004  |\n",
      "+------------+--------+\n",
      "|  writing   | 0.003  |\n",
      "+------------+--------+\n",
      "| including  | 0.003  |\n",
      "+------------+--------+\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ TF-IDF --------------------------------------------------------\n",
    "\n",
    "# arrays to hold the terms found in text and also a custom list to test domain-specific terms\n",
    "exportedList = []\n",
    "ownList = {\"data management\",\"database\",\"example\",\"iot\",\"lifecycle\",\"bloom\",\"filter\",\"integrity\",\n",
    "           \"java\",\"pattern\",\"design pattern\",\"svm\",\"Support vector machine\",\"knn\",\"k-nearest neighbors\",\"machine learning\"}\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.column_headers = [\"TERM\", \"TF-IDF\"]\n",
    "\n",
    "topNwords = 5;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topNwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "         table.append_row([term, round(score, 5)]) \n",
    "         exportedList.append(term)\n",
    "    \n",
    "    print(table)\n",
    "#    print(exportedWords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- EXPORTED TERMS in WORDNET ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1107: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/lexnames'>\n",
      "  for i, line in enumerate(self.open('lexnames')):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/index.adj'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/index.adv'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/index.verb'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/index.noun'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/adj.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/adv.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/verb.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/corpus/reader/wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='/home/ani/nltk_data/corpora/wordnet/noun.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cosine\n",
      "-  cosine.n.01  |  ratio of the adjacent side to the hypotenuse of a right-angled triangle\n",
      "\n",
      " input\n",
      "-  input_signal.n.01  |  signal going into an electronic system\n",
      "-  remark.n.01  |  a statement that expresses a personal opinion or belief or adds information\n",
      "-  stimulation.n.02  |  any stimulating information or event; acts to arouse action\n",
      "-  input.n.04  |  a component of production; something that goes into the production of output\n",
      "-  input.v.01  |  enter (data or a program) into a computer\n",
      "\n",
      " fixed\n",
      "-  repair.v.01  |  restore by replacing a part or putting together what is torn or broken\n",
      "-  fasten.v.01  |  cause to be firmly attached\n",
      "-  specify.v.02  |  decide upon or fix definitely\n",
      "-  cook.v.02  |  prepare for eating by applying heat\n",
      "-  pay_back.v.02  |  take vengeance on or get even\n",
      "-  fix.v.06  |  set or place definitely\n",
      "-  fix.v.07  |  kill, preserve, and harden (tissue) in order to prepare for microscopic study\n",
      "-  fixate.v.03  |  make fixed, stable or stationary\n",
      "-  sterilize.v.02  |  make infertile\n",
      "-  fix.v.10  |  influence an event or its outcome by illegal means\n",
      "-  situate.v.02  |  put (something somewhere) firmly\n",
      "-  fix.v.12  |  make ready or suitable or equip in advance for a particular purpose or for some use, event, etc\n",
      "-  fixed.s.01  |  (of a number) having a fixed and unchanging value\n",
      "-  fixed.s.02  |  fixed and unmoving\n",
      "-  fixed.a.03  |  securely placed or fastened or set\n",
      "-  fixed.s.04  |  incapable of being changed or moved or undone; e.g. \"frozen prices\"\n",
      "\n",
      " point\n",
      "-  point.n.01  |  a geometric element that has position but no extension\n",
      "-  point.n.02  |  the precise location of something; a spatially limited location\n",
      "-  point.n.03  |  a brief version of the essential meaning of something\n",
      "-  detail.n.01  |  an isolated fact that is considered separately from the whole\n",
      "-  degree.n.02  |  a specific identifiable position in a continuum or series or especially in a process\n",
      "-  point.n.06  |  an instant of time\n",
      "-  point.n.07  |  the object of an activity\n",
      "-  point.n.08  |  a V shape\n",
      "-  point.n.09  |  a very small circular shape\n",
      "-  point.n.10  |  the unit of counting in scoring a game or contest\n",
      "-  point.n.11  |  a promontory extending out into a large body of water\n",
      "-  item.n.01  |  a distinct part that can be specified separately in a group of things that could be enumerated on a list\n",
      "-  point.n.13  |  a style in speech or writing that arrests attention and has a penetrating or convincing quality or effect\n",
      "-  point.n.14  |  an outstanding characteristic\n",
      "-  point.n.15  |  sharp end\n",
      "-  compass_point.n.01  |  any of 32 horizontal directions indicated on the card of a compass\n",
      "-  point.n.17  |  a linear unit used to measure the size of type; approximately 1/72 inch\n",
      "-  point.n.18  |  one percent of the total principal of a loan; it is paid at the time the loan is made and is independent of the interest on the loan\n",
      "-  period.n.07  |  a punctuation mark (.) placed at the end of a declarative sentence to indicate a full stop or after abbreviations\n",
      "-  point.n.20  |  a V-shaped mark at one end of an arrow pointer\n",
      "-  decimal_point.n.01  |  the dot at the left of a decimal fraction\n",
      "-  point.n.22  |  the property of a shape that tapers to a sharp tip\n",
      "-  point.n.23  |  a distinguishing or individuating characteristic\n",
      "-  point.n.24  |  the gun muzzle's direction\n",
      "-  point.n.25  |  a wall socket\n",
      "-  distributor_point.n.01  |  a contact in the distributor; as the rotor turns its projecting arm contacts them and current flows to the spark plugs\n",
      "-  indicate.v.02  |  indicate a place, direction, person, or thing; either spatially or figuratively\n",
      "-  orient.v.01  |  be oriented\n",
      "-  charge.v.17  |  direct into a position for use\n",
      "-  steer.v.01  |  direct the course; determine the direction of travelling\n",
      "-  bespeak.v.01  |  be a signal for or a symptom of\n",
      "-  luff.v.01  |  sail close to the wind\n",
      "-  point.v.07  |  mark (Hebrew words) with diacritics\n",
      "-  point.v.08  |  mark with diacritics\n",
      "-  point.v.09  |  mark (a psalm text) to indicate the points at which the music changes\n",
      "-  point.v.10  |  be positionable in a specified manner\n",
      "-  target.v.01  |  intend (something) to move towards a certain goal\n",
      "-  point.v.12  |  indicate the presence of (game) by standing and pointing with the muzzle\n",
      "-  sharpen.v.07  |  give a point to\n",
      "-  point.v.14  |  repair the joints of bricks\n",
      "\n",
      " between\n",
      "-  between.r.01  |  in the interval\n",
      "-  between.r.02  |  in between\n",
      "\n",
      " squared\n",
      "-  square.v.01  |  make square\n",
      "-  square.v.02  |  raise to the second power\n",
      "-  square.v.03  |  cause to match, as of ideas or acts\n",
      "-  square.v.04  |  position so as to be square\n",
      "-  square.v.05  |  be compatible with\n",
      "-  square.v.06  |  pay someone and settle a debt\n",
      "-  feather.v.03  |  turn the paddle; in canoeing\n",
      "-  feather.v.04  |  turn the oar, while rowing\n",
      "-  squared.s.01  |  having been made square\n",
      "\n",
      " within\n",
      "-  inside.r.02  |  on the inside\n",
      "\n",
      " close\n",
      "-  stopping_point.n.01  |  the temporal end; the concluding time\n",
      "-  conclusion.n.08  |  the last section of a communication\n",
      "-  finale.n.03  |  the concluding part of any performance\n",
      "-  close.v.01  |  move so that an opening or passage is obstructed; make shut\n",
      "-  close.v.02  |  become closed\n",
      "-  close_up.v.01  |  cease to operate or cause to cease operating\n",
      "-  close.v.04  |  finish or terminate (meetings, speeches, etc.)\n",
      "-  conclude.v.04  |  come to a close\n",
      "-  close.v.06  |  complete a business deal, negotiation, or an agreement\n",
      "-  close.v.07  |  be priced or listed when trading stops\n",
      "-  close.v.08  |  engage at close quarters\n",
      "-  close.v.09  |  cause a window or an application to disappear on a computer desktop\n",
      "-  close.v.10  |  change one's body stance so that the forward shoulder and foot are closer to the intended point of impact\n",
      "-  close.v.11  |  come together, as if in an embrace\n",
      "-  close.v.12  |  draw near\n",
      "-  close.v.13  |  bring together all the elements or parts of\n",
      "-  close.v.14  |  bar access to\n",
      "-  close.v.15  |  fill or stop up\n",
      "-  close_up.v.03  |  unite or bring into contact or bring together the edges of\n",
      "-  close.v.17  |  finish a game in baseball by protecting a lead\n",
      "-  close.a.01  |  at or within a short distance in space or time or having elements near each other\n",
      "-  close.a.02  |  close in relevance or relationship\n",
      "-  near.a.01  |  not far distant in time or space or degree or circumstances\n",
      "-  close.s.04  |  rigorously attentive; strict and thorough\n",
      "-  close.s.05  |  marked by fidelity to an original\n",
      "-  close.s.06  |  (of a contest or contestants) evenly matched\n",
      "-  close.s.07  |  crowded\n",
      "-  airless.s.01  |  lacking fresh air\n",
      "-  close.s.09  |  of textiles\n",
      "-  close.s.10  |  strictly confined or guarded\n",
      "-  close.s.11  |  confined to specific persons\n",
      "-  close.s.12  |  fitting closely but comfortably\n",
      "-  close.s.13  |  used of hair or haircuts\n",
      "-  cheeseparing.s.01  |  giving or spending with reluctance\n",
      "-  close.s.15  |  inclined to secrecy or reticence about divulging information\n",
      "-  near.r.01  |  near in time or place or relationship\n",
      "-  close.r.02  |  in an attentive manner\n",
      "\n",
      " three\n",
      "-  three.n.01  |  the cardinal number that is the sum of one and one and one\n",
      "-  trey.n.02  |  one of four playing cards in a deck having three pips\n",
      "-  three.s.01  |  being one more than two\n",
      "\n",
      " positive\n",
      "-  positive.n.01  |  the primary form of an adjective or adverb; denotes a quality without qualification, comparison, or relation to increase or diminution\n",
      "-  positive.n.02  |  a film showing a photographic image whose tones correspond to those of the original subject\n",
      "-  positive.a.01  |  characterized by or displaying affirmation or acceptance or certainty etc.\n",
      "-  convinced.s.01  |  persuaded of; very sure\n",
      "-  plus.s.02  |  involving advantage or good\n",
      "-  positive.a.04  |  indicating existence or presence of a suspected condition or pathogen\n",
      "-  positive.s.05  |  formally laid down or imposed\n",
      "-  incontrovertible.s.01  |  impossible to deny or disprove\n",
      "-  positivist.a.01  |  of or relating to positivism\n",
      "-  positive.a.08  |  reckoned, situated or tending in the direction which naturally or arbitrarily is taken to indicate increase or progress or onward motion\n",
      "-  positive.s.09  |  greater than zero\n",
      "-  positive.s.10  |  having a positive charge\n",
      "-  cocksure.s.01  |  marked by excessive confidence\n",
      "\n",
      " over\n",
      "-  over.n.01  |  (cricket) the division of play during which six balls are bowled at the batsman by one player from the other team from the same end of the pitch\n",
      "-  complete.s.05  |  having come or been brought to a conclusion\n",
      "-  over.r.01  |  at or to a point across intervening space etc.\n",
      "-  over.r.02  |  throughout an area\n",
      "-  over.r.03  |  throughout a period of time\n",
      "-  over.r.04  |  beyond the top or upper surface or edge; forward from an upright position; \n",
      "-  all_over.r.01  |  over the entire area\n",
      "\n",
      " 2\n",
      "-  two.n.01  |  the cardinal number that is the sum of one and one or a numeral representing this number\n",
      "-  two.s.01  |  being one more than one\n",
      "\n",
      " talking\n",
      "-  talk.n.01  |  an exchange of ideas via conversation\n",
      "-  talk.v.01  |  exchange thoughts; talk with\n",
      "-  talk.v.02  |  express in speech\n",
      "-  speak.v.03  |  use language\n",
      "-  spill.v.05  |  reveal information\n",
      "-  spill_the_beans.v.01  |  divulge confidential information or secrets\n",
      "-  lecture.v.01  |  deliver a lecture or talk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " divided\n",
      "-  divide.v.01  |  separate into parts or portions\n",
      "-  divide.v.02  |  perform a division\n",
      "-  separate.v.01  |  act as a barrier between; stand between\n",
      "-  separate.v.12  |  come apart\n",
      "-  separate.v.07  |  make a division or separation\n",
      "-  separate.v.02  |  force, take, or pull apart\n",
      "-  divided.a.01  |  separated into parts or pieces\n",
      "-  divided.s.02  |  having a median strip or island between lanes of traffic moving in opposite directions\n",
      "-  divided.s.03  |  distributed in portions (often equal) on the basis of a plan or purpose\n",
      "\n",
      " 100\n",
      "-  hundred.n.01  |  ten 10s\n",
      "-  hundred.s.01  |  being ten more than ninety\n",
      "\n",
      " difference\n",
      "-  difference.n.01  |  the quality of being unlike or dissimilar\n",
      "-  deviation.n.01  |  a variation that deviates from the standard or norm\n",
      "-  dispute.n.01  |  a disagreement or argument about something important\n",
      "-  difference.n.04  |  a significant change\n",
      "-  remainder.n.03  |  the number that remains after subtraction; the number that when added to the subtrahend gives the minuend\n",
      "\n",
      " seventeen\n",
      "-  seventeen.n.01  |  the cardinal number that is the sum of sixteen and one\n",
      "-  seventeen.s.01  |  being one more than sixteen\n",
      "\n",
      " x^2 : NO SYNSETS\n",
      "\n",
      "\n",
      " writing\n",
      "-  writing.n.01  |  the act of creating written works\n",
      "-  writing.n.02  |  the work of a writer; anything expressed in letters of the alphabet (especially when considered from the point of view of style and effect)\n",
      "-  writing.n.03  |  (usually plural) the collected work of an author\n",
      "-  writing.n.04  |  letters or symbols that are written or imprinted on a surface to represent the sounds or words of a language\n",
      "-  writing.n.05  |  the activity of putting something in written form\n",
      "-  write.v.01  |  produce a literary work\n",
      "-  write.v.02  |  communicate or express by writing\n",
      "-  publish.v.03  |  have (one's written work) issued for publication\n",
      "-  write.v.04  |  communicate (with) in writing\n",
      "-  write.v.05  |  communicate by letter\n",
      "-  compose.v.02  |  write music\n",
      "-  write.v.07  |  mark or trace on a surface\n",
      "-  write.v.08  |  record data on a computer\n",
      "-  spell.v.03  |  write or name the letters that comprise the conventionally accepted form of (a word or part of a word)\n",
      "-  write.v.10  |  create code, write a computer program\n",
      "\n",
      " including\n",
      "-  include.v.01  |  have as a part, be made up out of\n",
      "-  include.v.02  |  consider as part of something\n",
      "-  include.v.03  |  add as part of something else; put in as part of a set, group, or category\n",
      "-  admit.v.03  |  allow participation in or the right to be part of; permit to exercise the rights, functions, and responsibilities of\n",
      "\n",
      "\n",
      "------- CUSTOM TERMS in WORDNET (also domain specific) ----------\n",
      "\n",
      " svm : NO SYNSETS\n",
      "\n",
      "\n",
      " knn : NO SYNSETS\n",
      "\n",
      "\n",
      " bloom\n",
      "-  blooming.n.01  |  the organic process of bearing flowers\n",
      "-  flower.n.02  |  reproductive organ of angiosperm plants especially one having showy or colorful parts\n",
      "-  bloom.n.03  |  the best time of youth\n",
      "-  bloom.n.04  |  a rosy color (especially in the cheeks) taken as a sign of good health\n",
      "-  flower.n.03  |  the period of greatest prosperity or productivity\n",
      "-  efflorescence.n.04  |  a powdery deposit on a surface\n",
      "-  bloom.v.01  |  produce or yield flowers\n",
      "\n",
      " iot : NO SYNSETS\n",
      "\n",
      "\n",
      " machine learning : NO SYNSETS\n",
      "\n",
      "\n",
      " design pattern : NO SYNSETS\n",
      "\n",
      "\n",
      " database\n",
      "-  database.n.01  |  an organized body of related information\n",
      "\n",
      " filter\n",
      "-  filter.n.01  |  device that removes something from whatever passes through it\n",
      "-  filter.n.02  |  an electrical device that alters the frequency spectrum of signals passing through it\n",
      "-  filter.v.01  |  remove by passing through a filter\n",
      "-  percolate.v.05  |  pass through\n",
      "-  trickle.v.01  |  run or flow slowly, as in drops or in an unsteady stream\n",
      "\n",
      " data management : NO SYNSETS\n",
      "\n",
      "\n",
      " java\n",
      "-  java.n.01  |  an island in Indonesia to the south of Borneo; one of the world's most densely populated regions\n",
      "-  coffee.n.01  |  a beverage consisting of an infusion of ground coffee beans\n",
      "-  java.n.03  |  a platform-independent object-oriented programming language\n",
      "\n",
      " integrity\n",
      "-  integrity.n.01  |  an undivided or unbroken completeness or totality with nothing wanting\n",
      "-  integrity.n.02  |  moral soundness\n",
      "\n",
      " lifecycle : NO SYNSETS\n",
      "\n",
      "\n",
      " Support vector machine : NO SYNSETS\n",
      "\n",
      "\n",
      " k-nearest neighbors : NO SYNSETS\n",
      "\n",
      "\n",
      " example\n",
      "-  example.n.01  |  an item of information that is typical of a class or group\n",
      "-  model.n.07  |  a representative form or pattern\n",
      "-  exemplar.n.01  |  something to be imitated\n",
      "-  example.n.04  |  punishment intended as a warning to others\n",
      "-  case.n.01  |  an occurrence of something\n",
      "-  exercise.n.04  |  a task performed or problem solved in order to develop skill or understanding\n",
      "\n",
      " pattern\n",
      "-  form.n.03  |  a perceptual structure\n",
      "-  practice.n.01  |  a customary way of operation or behavior\n",
      "-  design.n.04  |  a decorative or artistic work\n",
      "-  convention.n.02  |  something regarded as a normative example\n",
      "-  pattern.n.05  |  a model considered worthy of imitation\n",
      "-  blueprint.n.01  |  something intended as a guide for making something else\n",
      "-  traffic_pattern.n.01  |  the path that is prescribed for an airplane that is preparing to land at an airport\n",
      "-  radiation_pattern.n.01  |  graphical representation (in polar or Cartesian coordinates) of the spatial distribution of radiation from an antenna as a function of angle\n",
      "-  model.v.01  |  plan or create according to a model or models\n",
      "-  pattern.v.02  |  form a pattern\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------- NLTK, WORDNET -------------------------------------------\n",
    "print(\"\\n\\n------- EXPORTED TERMS in WORDNET ----------\") \n",
    "for word in exportedList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "\n",
    "print(\"\\n\\n------- CUSTOM TERMS in WORDNET (also domain specific) ----------\")    \n",
    "for word in ownList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "### NOTES\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**TF-IDF** doesn't output the necessary result, I need n-grams selected as a combined keyword and these are often very general words like `for example` or `key concept` etc. in order to classify the text into the GOAL element. \n",
    "\n",
    "**TextBlob** provides options for n-grams and also connection to WordNet ontology which could be useful, so will look more into it.\n",
    "\n",
    "**WordNet** finds multiple definitions and synsets (synonyms) for most of the general words, however if provided specific e.g. computer science algorithm names, or specific terms, it doesn find any synonyms, nor descriptions of any of them.\n",
    "\n",
    "**Wikipedia** recognized some of the terms, but not all. For instance if we give it KNN it doesn't find anything, but if we give it K-nearest neighbour, if finds it. This is how the name is in Wikipedia, so that may be the reason. But on Google first returned result for KNN is this article. Same for SVM and Support vector machine. I've modified the script to return \"NO DESCRIPTION or DISAMBIGUATION\" everytime if finds nopthing ot if there's a disambiguation error, otherwise it wouldn continue checking the rest of the terms. So now it skips the error. \n",
    " \n",
    "**Full list** of identified key words so far [HERE](https://docs.google.com/spreadsheets/d/1Dj4UAh6U5jAelcsz-gDCdDE9JRVhwaNei0Ctn8m0Ui4/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
