{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "#### Steps:\n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams\n",
    "\n",
    "---\n",
    "\n",
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "------\n",
    "\n",
    "#### Offtopic\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import pathlib\n",
    "from unidecode import unidecode\n",
    "import pprint\n",
    "from tabulate import tabulate\n",
    "\n",
    "import scipy\n",
    "import numpy\n",
    "import sklearn\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "#### ---------------\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet.\n",
    "\n",
    "##### Term frequency\n",
    "\\\\( tf(t,d) = 0.5 + 0.5 * (\\frac{f_{t,d}}{f_{t',d}:t' \\in d}) \\\\) \n",
    "\n",
    "##### Inversed document frequency\n",
    "\\\\( idf(t,D) = log * (\\frac{N}{d \\in D  :  t \\in d}) \\\\)\n",
    "\n",
    "##### Computing tf-idf\n",
    "\\\\( tfidf(t,d,D) = tf(t,d) * idf(t,D) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TF-IDF with data\n",
    "\n",
    "#### TODO: Fix the input, it takes strings, and not files right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse each folder and sub-folder\n",
    "# create an array of files to add each file in it\n",
    "# if the file is TXT, add to the array\n",
    "# create a String array of documents with the file of the array with files \n",
    "# so we can store the contents of each inside\n",
    "# read each line of each file and save to the strings\n",
    "# perform algorithms on the documents\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# WINDOWS\n",
    "#path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\one file\"\n",
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\"\n",
    "\n",
    "# LINUX\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "docnames = []\n",
    "counter = 0\n",
    "\n",
    "# DOCUMENT LIST CONSISTS OF TEXTBLOB files. All input files need to be converted to TEXTBLOB \n",
    "# and then saved in this list in order for TF-IDF to work\n",
    "doclist = []\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            elif filePath.endswith(\"_lemmatized.txt\"): \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    docnames.append(curFile)\n",
    "                    \n",
    "                    fcontentTBlob = tb(curFile.read())\n",
    "                    #print(fcontentTBlob)\n",
    "                    doclist.append(fcontentTBlob)\n",
    "                    \n",
    "                    # bag of words processing:\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(\"Total number of files in docnames[]:\", len(docnames))\n",
    "print(\"Total number of files in doclist[]:\", len(docnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------ TF-IDF --------------------------------------------------------\n",
    "\n",
    "# arrays to hold the terms found in text and also a custom list to test domain-specific terms\n",
    "exportedList = []\n",
    "ownList = {\"data management\",\"database\",\"example\",\"iot\",\"lifecycle\",\"bloom\",\"filter\",\"integrity\",\n",
    "           \"java\",\"pattern\",\"design pattern\",\"svm\",\"Support vector machine\",\"knn\",\"k-nearest neighbors\",\"machine learning\"}\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.column_headers = [\"TERM\", \"TF-IDF\"]\n",
    "\n",
    "topNwords = 10;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topNwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "        #print(table.append_row([term, round(score, 5)]))\n",
    "        print(\"\\tTERM: {} \\t|\\t TF-IDF: {}\".format(term, round(score, 5)))\n",
    "        exportedList.append(term)\n",
    "        #print tabulate([term, round(score, 5)], headers=['tTERM', 'TF-IDF'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------- NLTK, WORDNET -------------------------------------------\n",
    "print(\"\\n\\n------- EXPORTED TERMS in WORDNET ----------\") \n",
    "for word in exportedList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "\n",
    "print(\"\\n\\n------- CUSTOM TERMS in WORDNET (also domain specific) ----------\")    \n",
    "for word in ownList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words, and all the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "\n",
    "# BoF\n",
    "def bagOfWords(iFilePath,iPOSfPath,choice):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    data_corpus = []  \n",
    "    \n",
    "    if iFilePath.endswith(\"_lemmatized.txt\"):\n",
    "        print(\"[Bag of words: ]\\t\" + iFilePath+\"\\n\")\n",
    "\n",
    "        baseName = iFilePath.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_FullLemTerm.txt\" \n",
    "        \n",
    "        try:\n",
    "            iLemmaFile = open(iFilePath, 'r', encoding = \"ISO-8859-1\")  # open lemma file in read mode\n",
    "            LemmafileCont = iLemmaFile.read().split()   # read file content and save it into the string variable\n",
    "\n",
    "            text = \"\"\n",
    "            for line in LemmafileCont:\n",
    "                fullTerm = findRealTerm(line,iPOSfPath)\n",
    "                text += fullTerm+\" \"\n",
    "                \n",
    "                if(choice == 0):\n",
    "                    pass\n",
    "                elif(choice == 1):\n",
    "                    with open(OFName, \"w\") as oFile:\n",
    "                        oFile.write(fullTerm+\"\\n\")\n",
    "                else:\n",
    "                    print(\"Invalid output file option.. 0 - NO file, 1 - SAVE file\")\n",
    "                    break\n",
    "                \n",
    "                continue\n",
    "            data_corpus.append(text)\n",
    "            \n",
    "            #print(data_corpus)\n",
    "            vector = vectorizer.fit_transform(data_corpus).todense() \n",
    "            #print(vector.toarray())\n",
    "            #print(vectorizer.get_feature_names())  \n",
    "\n",
    "            #array = vector.toarray()\n",
    "            #featureNames = vectorizer.get_feature_names()\n",
    "            \n",
    "            print(vectorizer.vocabulary_)\n",
    "            \n",
    "            #features = vectorizer.fit_transform(data_corpus)\n",
    "            #print(vectorizer.vocabulary_)\n",
    "\n",
    "            for f in vector:\n",
    "                print(\"Euclidean distances: \\n\")\n",
    "                print(euclidean_distances(vector[0], f))\n",
    "            \n",
    "        finally:\n",
    "            iLemmaFile.close()              \n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Match the lemma to first found word from file _lemmatized.txt for that file\n",
    "def findRealTerm(lemmaIn, POSfilePath):   \n",
    "    # take the path of the input file and look for the file ending on \"_stemmedbyPOS.txt\"\n",
    "    # split each line into 3, look in line[3] for the first match of the current lemma\n",
    "    # when found, take line[0] which is the full word and return that word\n",
    "    # exit the function\n",
    "\n",
    "    #print(\"Searching for term\")\n",
    "    res = \"\"\n",
    "    try:\n",
    "        iPOSFile = open(POSfilePath, 'r', encoding = \"ISO-8859-1\")  # open POS file in read mode\n",
    "        posfileCont = iPOSFile.read().split()   # read file content and save it into the string variable\n",
    "    \n",
    "        for line in posfileCont:\n",
    "            line = line.split(\",\")\n",
    "\n",
    "            word = line[0]\n",
    "            lemma = line[2]\n",
    "\n",
    "            if lemma == lemmaIn:\n",
    "                #print(\"WORD: \", word, \" || LEMMA: \", lemma)\n",
    "                res = word\n",
    "                break\n",
    "    \n",
    "    finally:\n",
    "        iPOSFile.close()\n",
    "        \n",
    "    if res == \"\":\n",
    "        res = \"NOT FOUND: \"+lemmaIn\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\01_driving-robots-around.en_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\02_differential-drive-robots.en_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\03_odometry.en_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: ResourceWarning: unclosed file <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\test_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "processing.. C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\\01_driving-robots-around.en_stemmedbyPOS.txt\n",
      "[Bag of words: ]\tC:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\\01_driving-robots-around.en_lemmatized.txt\n",
      "\n",
      "{'welcome': 187, 'module': 105, 'course': 35, 'control': 34, 'mobile': 103, 'robots': 135, 'introduced': 81, 'theory': 163, 'way': 185, 'dealing': 37, 'systems': 159, 'general': 63, 'not': 111, 'found': 60, 'go': 66, 'key': 82, 'target': 161, 'application': 10, 'start': 152, 'simple': 144, 'question': 129, 'drive': 43, 'point': 123, 'case': 26, 'ball': 15, 'yellow': 195, 'sun': 156, 'first': 55, 'need': 108, 'understand': 174, 'even': 48, 'answer': 9, 'well': 188, 'obviously': 114, 'measure': 100, 'goal': 67, 'somehow': 148, 'turn': 172, 'actions': 3, 've': 178, 'taken': 160, 'information': 77, 'feeding': 53, 'things': 164, 'design': 38, 'kind': 83, 'know': 84, 'little': 89, 'bite': 20, 'already': 6, 'also': 7, 'discuss': 41, 'gain': 62, 'world': 193, 'sensors': 141, 'models': 104, 'sufficient': 155, 'level': 86, 'abstraction': 2, 'reason': 133, 'rich': 134, 'enough': 46, 'trust': 170, 'based': 16, 'something': 149, 'finally': 54, 'order': 116, 'useful': 176, 'able': 0, 'predict': 125, 'influence': 75, 'particular': 118, 'look': 92, 'differential': 40, 'allow': 5, 'internal': 80, 'state': 153, 'instance': 78, 'will': 190, 'advanced': 4, 'perception': 120, 'give': 65, 'type': 173, 'infor': 76, 'mation': 97, 'want': 184, 'whenever': 189, 'try': 171, 'facts': 52, 'really': 132, 'embrace': 45, 'fundamentally': 61, 'unknown': 175, 'chair': 27, 'building': 23, 'tree': 169, 'forest': 58, 'plan': 122, 'exactly': 50, 'second': 138, 'people': 119, 'move': 107, 'wind': 191, 'makes': 95, 'blow': 22, 'sway': 157, 'changing': 28, 'dynamic': 44, 'bad': 14, 'idea': 72, 'produce': 127, 'complicated': 32, 'monolithic': 106, 'everything': 49, 'instead': 79, 'lot': 93, 'times': 166, 'divide': 42, 'task': 162, 'chunks': 29, 'individual': 74, 'get': 64, 'may': 98, 'shows': 143, 'environment': 47, 'switch': 158, 'avoid': 12, 'primitive': 126, 'blocks': 21, 'view': 180, 'called': 24, 'behaviors': 19, 'concepts': 33, 'quite': 130, 'mention': 101, 'handful': 71, 'standard': 151, 'indeed': 73, 'see': 139, 'to': 167, 'means': 99, 'waypoint': 186, 'location': 90, 'obstacles': 113, 'absolutely': 1, 'crucial': 36, 'slam': 147, 'office': 115, 'straight': 154, 'lines': 88, 'walls': 183, 'follow': 56, 'wall': 182, 'track': 168, 'forth': 59, 'beha': 18, 'viors': 181, 'would': 194, 'like': 87, 'video': 179, 'developing': 39, 'working': 192, 'camera': 25, 'map': 96, 'putting': 128, 'thinking': 165, 'new': 110, 'long': 91, 'saw': 137, 'spending': 150, 'large': 85, 'amount': 8, 're': 131, 'running': 136, 'following': 57, 'pops': 124, 'obstacle': 112, 'sitting': 145, 'clear': 30, 'goes': 68, 'back': 13, 'example': 51, 'segway': 140, 'never': 109, 'mind': 102, 'graphs': 70, 'lower': 94, 'part': 117, 'picture': 121, 'arc': 11, 'various': 177, 'sizes': 146, 'shapes': 142, 'become': 17, 'good': 69, 'combine': 31}\n",
      "Euclidean distances: \n",
      "\n",
      "[[0.]]\n",
      "\n",
      "\n",
      "processing.. C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\\02_differential-drive-robots.en_stemmedbyPOS.txt\n",
      "[Bag of words: ]\tC:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\\02_differential-drive-robots.en_lemmatized.txt\n",
      "\n",
      "{'order': 109, 'design': 36, 'behaviors': 12, 'controllers': 28, 'robots': 131, 'inevitably': 75, 'need': 104, 'models': 101, 'actually': 1, 'behave': 11, 'not': 105, 'found': 63, 'go': 67, 'start': 149, 'common': 26, 'differential': 37, 'drive': 46, 'mobile': 100, 'wheeled': 186, 'turn': 171, 'rates': 126, 'make': 95, 'move': 102, 'reason': 128, 'extremely': 55, 'fact': 56, 'khepera': 82, 'using': 176, 'quiet': 123, 'lot': 93, 'course': 30, 'typically': 172, 'caster': 22, 'back': 9, 'way': 184, 'work': 188, 'right': 130, 'velocity': 179, 'leave': 86, 'ca': 16, 'instance': 78, 'straight': 152, 'ahead': 2, 'slower': 143, 'towards': 165, 'direction': 39, 'able': 0, 'round': 133, 'let': 87, 'kind': 83, 'see': 137, 'look': 92, 'like': 88, 'well': 185, 'cartoon': 20, 'circle': 24, 'black': 15, 'rectangles': 129, 'supposed': 155, 'first': 61, 'thing': 162, 'know': 85, 'dimensions': 38, 've': 178, 'saying': 136, 'good': 68, 'exactly': 53, 'particular': 113, 'parameters': 112, 'dont': 42, 'friction': 64, 'coeficcient': 25, 'case': 21, 'base': 10, 'meaning': 97, 'far': 57, 'away': 8, 'call': 17, 'also': 3, 'radius': 125, 'eaning': 48, 'big': 13, 'capital': 18, 'luckily': 94, 'inherently': 76, 'easy': 49, 'measure': 98, 'take': 158, 'ruler': 134, 'will': 187, 'play': 116, 'little': 91, 'bite': 14, 'role': 132, 'trying': 170, 'want': 183, 'end': 50, 'day': 32, 'signals': 139, 'disposal': 40, 'sub': 154, 'inputs': 77, 'system': 157, 'states': 150, 'draw': 45, 'triangle': 168, 'stress': 153, 'care': 19, 'position': 118, 'heading': 69, 'phi': 115, 'orientation': 110, 'connect': 27, 'somehow': 145, 'transition': 166, 'kinematics': 84, 'instead': 79, 'spending': 148, 'minutes': 99, 'deriving': 34, 'voila': 181, 'tells': 160, 'vr': 182, 'vl': 180, 'translates': 167, 'dot': 44, 'change': 23, 'gives': 66, 'terms': 161, 'mapping': 96, 'problem': 120, 'cumbersome': 31, 'unnatural': 175, 'think': 163, 'various': 177, 'asked': 7, 'get': 65, 'door': 43, 'probably': 119, 'oing': 106, 'fast': 58, 'instructions': 80, 'however': 71, 'implement': 73, 'something': 146, 'unicycle': 173, 'overcomes': 111, 'issue': 81, 'dealing': 33, 'unintuitive': 174, 'sense': 138, 'talk': 159, 'speed': 147, 'quickly': 122, 'angular': 4, 'omega': 108, 'natural': 103, 'feel': 59, 'dynamics': 47, 'follows': 62, 'cosine': 29, 'put': 121, 'equal': 51, 'line': 89, 'similarly': 140, 'sine': 142, 'highly': 70, 'quite': 124, 'deserves': 35, 'patented': 114, 'sweethearts': 156, 'okay': 107, 'together': 164, 'trick': 169, 'find': 60, 'identify': 72, 'divide': 41, 'simply': 141, 'equation': 52, 'real': 127, 'linear': 90, 'solve': 144, 'explicitly': 54, 'point': 117, 'apart': 6, 'indeed': 74, 'running': 135, 'step': 151, 'anything': 5, 'world': 189}\n",
      "Euclidean distances: \n",
      "\n",
      "[[0.]]\n",
      "\n",
      "\n",
      "processing.. C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\\03_odometry.en_stemmedbyPOS.txt\n",
      "[Bag of words: ]\tC:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\\03_odometry.en_lemmatized.txt\n",
      "\n",
      "{'model': 123, 'need': 127, 'way': 224, 'knowing': 101, 'robot': 166, 'state': 191, 'xy': 233, 'fine': 69, 'meaning': 120, 'position': 144, 'heading': 82, 'odometry': 132, 'obtain': 131, 'pose': 143, 'information': 92, 'question': 150, 'actually': 3, 'get': 75, 'well': 225, 'number': 130, 'different': 43, 'end': 56, 'day': 38, 'absolutely': 1, 'sensors': 177, 'possibilities': 145, 'use': 218, 'kind': 100, 'external': 62, 'would': 230, 'measuring': 121, 'something': 187, 'environment': 58, 'instance': 94, 'pretend': 146, 'see': 174, 'landmark': 102, 'let': 107, 'say': 171, 'eiffel': 53, 'tower': 207, 'start': 190, 'moving': 124, 'around': 8, 'keep': 99, 'track': 208, 'able': 0, 'least': 105, 'relative': 161, 'right': 164, 'seem': 175, 'make': 117, 'ultrasound': 215, 'infrared': 93, 'cameras': 16, 'laser': 103, 'scanners': 173, 'tell': 198, 'type': 214, 'course': 36, 'gps': 78, 'internally': 97, 'immediately': 87, 'give': 76, 'things': 200, 'forth': 71, 'however': 85, 'running': 169, 'indoors': 91, 'certainly': 19, 'signals': 181, 'lot': 114, 'times': 203, 'alone': 4, 'enough': 57, 'phi': 139, 'high': 84, 'level': 108, 'fidelity': 66, 'couple': 35, 'sitting': 185, 'helping': 83, 'could': 33, 'compass': 21, 'figure': 67, 'orientation': 136, 'self': 176, 'respecting': 162, 'accelerometers': 2, 'gyroscopes': 80, 'finding': 68, 'far': 65, 've': 219, 'traveled': 209, 'derived': 42, 'degree': 40, 'wheel': 227, 'encoders': 55, 'tick': 202, 'counts': 34, 'many': 118, 'basically': 13, 'revolutions': 163, 'amount': 6, 'like': 109, 'discuss': 46, 'little': 111, 'not': 129, 'found': 72, 'bite': 15, 'reason': 157, 'indeed': 90, 'working': 228, 'referential': 159, 'drive': 51, 'awhile': 12, 'importantly': 88, 'much': 125, 'trusted': 212, 'distance': 47, 'leave': 106, 'following': 70, 'assumption': 9, 'go': 77, 'arc': 7, 'turning': 213, 'constant': 28, 'rate': 153, 'velocity': 220, 'ohm': 133, 'short': 179, 'scales': 172, 'correct': 29, 'unknown': 216, 'case': 17, 'quicker': 151, 'interested': 96, 'center': 18, 'dc': 39, 'luckily': 115, 'simply': 183, 'dl': 48, 'dr': 49, 'particularly': 137, 'picky': 141, 'showing': 180, 'geometry': 74, 'comes': 20, 'instead': 95, 'readily': 155, 'available': 10, 'want': 222, 'look': 113, 'connect': 26, 'mobile': 122, 'fact': 63, 'road': 165, 'interval': 98, 'compute': 24, 'new': 128, 'primes': 148, 'similarly': 182, 'update': 217, 'sine': 184, 'cosine': 31, 'term': 199, 'even': 59, 'quite': 152, 'experiments': 61, 'think': 201, 'order': 135, 'pi': 140, 'total': 205, 'beginning': 14, 'system': 194, 'writing': 231, 'delta': 41, 'take': 196, 'old': 134, 'subtract': 193, 'trick': 210, 'easily': 52, 'either': 54, 'mapping': 119, 'saw': 170, 'previous': 147, 'slide': 186, 'fair': 64, 'major': 116, 'disclaimer': 45, 'must': 126, 'ta': 195, 'daa': 37, 'drift': 50, 'imprecise': 89, 'real': 156, 'source': 188, 'probably': 149, 'trouble': 211, 'video': 221, 'cou': 32, 'rses': 168, 'teach': 197, 'recently': 158, 'competing': 22, 'lines': 110, 'wave': 223, 'points': 142, 'lay': 104, 'pda': 138, 'regulator': 160, 'whack': 226, 'top': 204, 'consequence': 27, 'spinning': 189, 'touching': 206, 'grind': 79, 'completely': 23, 'confused': 25, 'idea': 86, 'world': 229, 'example': 60, 'rather': 154, 'severe': 178, 'wrong': 232, 'direction': 44, 'longer': 112, 'correspond': 30, 'happening': 81, 'always': 5, 'aware': 11, 'full': 73, 'story': 192, 'robust': 167}\n",
      "Euclidean distances: \n",
      "\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\"\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\"\n",
    "\n",
    "# LINUX\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "counter = 0\n",
    "POSfiles = []\n",
    "\n",
    "# --- Collecting the POS files\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        curFilePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(curFilePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # create a list of files for POS so that it can be sent along with BoF to look for the right file and terms\n",
    "            if curFilePath.endswith(\"_stemmedbyPOS.txt\"):\n",
    "                curFile = open(curFilePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                baseName = os.path.basename(curFile.name.split(\".en\", 1)[0])\n",
    "                curFilePOS = baseName+\".en_stemmedbyPOS.txt\"\n",
    "                POSfiles.append(curFilePOS)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "            \n",
    "# --- processing Lemmatized files with Algos\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        curFilePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(curFilePath):\n",
    "            pass\n",
    "\n",
    "        else:                \n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if curFilePath.endswith(\"_lemmatized.txt\"): \n",
    "                counter += 1\n",
    "                try:\n",
    "                    # need this only to extract the file path and send it to the algorithm later. Send path, !not file!\n",
    "                    tempFile = open(curFilePath, 'r', encoding = \"ISO-8859-1\")                    \n",
    "\n",
    "                    baseName = tempFile.name.split(\".en\", 1)[0]\n",
    "                    POSfilePath = baseName+\".en_stemmedbyPOS.txt\"\n",
    "\n",
    "                    if os.path.basename(POSfilePath) in POSfiles:\n",
    "                        print(\"\\n\\nprocessing.. \" + POSfilePath)\n",
    "                        \n",
    "                        # ---------- bag of words processing: ------------\n",
    "                        # last index is whether an output file to be saved or not. 0 - NO, 1 - YES\n",
    "                        bagOfWords(curFilePath,POSfilePath,0)  \n",
    "                        \n",
    "                finally:\n",
    "                    tempFile.close()\n",
    "            else:\n",
    "                pass\n",
    "#print(\"Total number of POS Files[]:\", len(POSfiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "### NOTES\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**TF-IDF** doesn't output the necessary result, I need n-grams selected as a combined keyword and these are often very general words like `for example` or `key concept` etc. in order to classify the text into the GOAL element. \n",
    "\n",
    "**TextBlob** provides options for n-grams and also connection to WordNet ontology which could be useful, so will look more into it.\n",
    "\n",
    "**WordNet** finds multiple definitions and synsets (synonyms) for most of the general words, however if provided specific e.g. computer science algorithm names, or specific terms, it doesn find any synonyms, nor descriptions of any of them.\n",
    "\n",
    "**Wikipedia** recognized some of the terms, but not all. For instance if we give it KNN it doesn't find anything, but if we give it K-nearest neighbour, if finds it. This is how the name is in Wikipedia, so that may be the reason. But on Google first returned result for KNN is this article. Same for SVM and Support vector machine. I've modified the script to return \"NO DESCRIPTION or DISAMBIGUATION\" everytime if finds nopthing ot if there's a disambiguation error, otherwise it wouldn continue checking the rest of the terms. So now it skips the error. \n",
    " \n",
    "**Full list** of identified key words so far [HERE](https://docs.google.com/spreadsheets/d/1Dj4UAh6U5jAelcsz-gDCdDE9JRVhwaNei0Ctn8m0Ui4/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
