{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "#### Steps:\n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams\n",
    "\n",
    "---\n",
    "\n",
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "------\n",
    "\n",
    "#### Offtopic\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import pathlib\n",
    "from unidecode import unidecode\n",
    "import pprint\n",
    "from tabulate import tabulate\n",
    "\n",
    "import scipy\n",
    "import numpy\n",
    "import sklearn\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "#### ---------------\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet.\n",
    "\n",
    "##### Term frequency\n",
    "\\\\( tf(t,d) = 0.5 + 0.5 * (\\frac{f_{t,d}}{f_{t',d}:t' \\in d}) \\\\) \n",
    "\n",
    "##### Inversed document frequency\n",
    "\\\\( idf(t,D) = log * (\\frac{N}{d \\in D  :  t \\in d}) \\\\)\n",
    "\n",
    "##### Computing tf-idf\n",
    "\\\\( tfidf(t,d,D) = tf(t,d) * idf(t,D) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TF-IDF with data\n",
    "\n",
    "#### TODO: Fix the input, it takes strings, and not files right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in docnames[]: 3\n",
      "Total number of files in doclist[]: 3\n"
     ]
    }
   ],
   "source": [
    "# traverse each folder and sub-folder\n",
    "# create an array of files to add each file in it\n",
    "# if the file is TXT, add to the array\n",
    "# create a String array of documents with the file of the array with files \n",
    "# so we can store the contents of each inside\n",
    "# read each line of each file and save to the strings\n",
    "# perform algorithms on the documents\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# WINDOWS\n",
    "#path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\one file\"\n",
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Course test files\"\n",
    "\n",
    "# LINUX\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "docnames = []\n",
    "counter = 0\n",
    "\n",
    "# DOCUMENT LIST CONSISTS OF TEXTBLOB files. All input files need to be converted to TEXTBLOB \n",
    "# and then saved in this list in order for TF-IDF to work\n",
    "doclist = []\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            elif filePath.endswith(\"_lemmatized.txt\"): \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    docnames.append(curFile)\n",
    "                    \n",
    "                    fcontentTBlob = tb(curFile.read())\n",
    "                    #print(fcontentTBlob)\n",
    "                    doclist.append(fcontentTBlob)\n",
    "                    \n",
    "                    # bag of words processing:\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "print(\"Total number of files in docnames[]:\", len(docnames))\n",
    "print(\"Total number of files in doclist[]:\", len(docnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 terms in document 1 | <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\01_driving-robots-around.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: plan \t|\t TF-IDF: 0.00544\n",
      "\tTERM: goal \t|\t TF-IDF: 0.00454\n",
      "\tTERM: mod \t|\t TF-IDF: 0.00363\n",
      "\tTERM: sun \t|\t TF-IDF: 0.00272\n",
      "\tTERM: understand \t|\t TF-IDF: 0.00272\n",
      "\tTERM: allow \t|\t TF-IDF: 0.00272\n",
      "\tTERM: adv \t|\t TF-IDF: 0.00272\n",
      "\tTERM: build \t|\t TF-IDF: 0.00272\n",
      "\tTERM: switch \t|\t TF-IDF: 0.00272\n",
      "\tTERM: introduc \t|\t TF-IDF: 0.00181\n",
      "\n",
      "Top 10 terms in document 2 | <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\02_differential-drive-robots.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: sub \t|\t TF-IDF: 0.01138\n",
      "\tTERM: dot \t|\t TF-IDF: 0.00854\n",
      "\tTERM: paramet \t|\t TF-IDF: 0.0064\n",
      "\tTERM: input \t|\t TF-IDF: 0.0064\n",
      "\tTERM: car \t|\t TF-IDF: 0.00285\n",
      "\tTERM: vr \t|\t TF-IDF: 0.00285\n",
      "\tTERM: vl \t|\t TF-IDF: 0.00285\n",
      "\tTERM: unicyc \t|\t TF-IDF: 0.00285\n",
      "\tTERM: spee \t|\t TF-IDF: 0.00285\n",
      "\tTERM: omeg \t|\t TF-IDF: 0.00285\n",
      "\n",
      "Top 10 terms in document 3 | <_io.TextIOWrapper name='C:\\\\Users\\\\ani\\\\Desktop\\\\Course data Thesis\\\\Course test files\\\\03_odometry.en_lemmatized.txt' mode='r' encoding='ISO-8859-1'>\n",
      "\tTERM: tick \t|\t TF-IDF: 0.00874\n",
      "\tTERM: encod \t|\t TF-IDF: 0.00801\n",
      "\tTERM: extern \t|\t TF-IDF: 0.0051\n",
      "\tTERM: dist \t|\t TF-IDF: 0.0051\n",
      "\tTERM: count \t|\t TF-IDF: 0.00437\n",
      "\tTERM: gps \t|\t TF-IDF: 0.00291\n",
      "\tTERM: interest \t|\t TF-IDF: 0.00291\n",
      "\tTERM: comput \t|\t TF-IDF: 0.00291\n",
      "\tTERM: eiffel \t|\t TF-IDF: 0.00218\n",
      "\tTERM: tow \t|\t TF-IDF: 0.00218\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ TF-IDF --------------------------------------------------------\n",
    "\n",
    "# arrays to hold the terms found in text and also a custom list to test domain-specific terms\n",
    "exportedList = []\n",
    "ownList = {\"data management\",\"database\",\"example\",\"iot\",\"lifecycle\",\"bloom\",\"filter\",\"integrity\",\n",
    "           \"java\",\"pattern\",\"design pattern\",\"svm\",\"Support vector machine\",\"knn\",\"k-nearest neighbors\",\"machine learning\"}\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.column_headers = [\"TERM\", \"TF-IDF\"]\n",
    "\n",
    "topNwords = 10;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topNwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "        #print(table.append_row([term, round(score, 5)]))\n",
    "        print(\"\\tTERM: {} \\t|\\t TF-IDF: {}\".format(term, round(score, 5)))\n",
    "        exportedList.append(term)\n",
    "        #print tabulate([term, round(score, 5)], headers=['tTERM', 'TF-IDF'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------- EXPORTED TERMS in WORDNET ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1107: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\lexnames'>\n",
      "  for i, line in enumerate(self.open('lexnames')):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\index.adj'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\index.adv'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\index.noun'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1159: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\index.verb'>\n",
      "  for i, line in enumerate(self.open('index.%s' % suffix)):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\adj.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\adv.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\noun.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n",
      "C:\\Users\\ani\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1209: ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\ani\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\wordnet\\\\verb.exc'>\n",
      "  for line in self.open('%s.exc' % suffix):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " plan\n",
      "-  plan.n.01  |  a series of steps to be carried out or goals to be accomplished\n",
      "-  design.n.02  |  an arrangement scheme\n",
      "-  plan.n.03  |  scale drawing of a structure\n",
      "-  plan.v.01  |  have the will and intention to carry out some action\n",
      "-  plan.v.02  |  make plans for something\n",
      "-  plan.v.03  |  make or work out a plan for; devise\n",
      "-  design.v.04  |  make a design of; plan out in systematic, often graphic form\n",
      "\n",
      " goal\n",
      "-  goal.n.01  |  the state of affairs that a plan is intended to achieve and that (when achieved) terminates behavior intended to achieve it\n",
      "-  finish.n.04  |  the place designated as the end (as of a race or journey)\n",
      "-  goal.n.03  |  game equipment consisting of the place toward which players of a game try to advance a ball or puck in order to score points\n",
      "-  goal.n.04  |  a successful attempt at scoring\n",
      "\n",
      " mod\n",
      "-  mod.n.01  |  a British teenager or young adult in the 1960s; noted for their clothes consciousness and opposition to the rockers\n",
      "-  mod.s.01  |  relating to a recently developed fashion or style; \n",
      "\n",
      " sun\n",
      "-  sun.n.01  |  the star that is the source of light and heat for the planets in the solar system\n",
      "-  sunlight.n.01  |  the rays of the sun\n",
      "-  sun.n.03  |  a person considered as a source of warmth or energy or glory etc\n",
      "-  sun.n.04  |  any star around which a planetary system revolves\n",
      "-  sunday.n.01  |  first day of the week; observed as a day of rest and worship by most Christians\n",
      "-  sun.v.01  |  expose one's body to the sun\n",
      "-  sun.v.02  |  expose to the rays of the sun or affect by exposure to the sun\n",
      "\n",
      " understand\n",
      "-  understand.v.01  |  know and comprehend the nature or meaning of\n",
      "-  understand.v.02  |  perceive (an idea or situation) mentally\n",
      "-  understand.v.03  |  make sense of a language\n",
      "-  understand.v.04  |  believe to be the case\n",
      "-  sympathize.v.02  |  be understanding of\n",
      "\n",
      " allow\n",
      "-  let.v.01  |  make it possible through a specific action or lack of action for something to happen\n",
      "-  permit.v.01  |  consent to, give permission\n",
      "-  allow.v.03  |  let have\n",
      "-  allow.v.04  |  give or assign a resource to a particular person or cause\n",
      "-  leave.v.06  |  make a possibility or provide opportunity for; permit to be attainable or cause to remain\n",
      "-  allow.v.06  |  allow or plan for a certain possibility; concede the truth or validity of something\n",
      "-  admit.v.05  |  afford possibility\n",
      "-  give_up.v.11  |  allow the other (baseball) team to score\n",
      "-  allow.v.09  |  grant as a discount or in exchange\n",
      "-  allow.v.10  |  allow the presence of or allow (an activity) without opposing or prohibiting\n",
      "\n",
      " adv : NO SYNSETS\n",
      "\n",
      "\n",
      " build\n",
      "-  physique.n.01  |  constitution of the human body\n",
      "-  human_body.n.01  |  alternative names for the body of a human being\n",
      "-  construct.v.01  |  make by combining materials and parts\n",
      "-  build_up.v.02  |  form or accumulate steadily\n",
      "-  build.v.03  |  build or establish something abstract\n",
      "-  build.v.04  |  improve the cleansing action of\n",
      "-  build.v.05  |  order, supervise, or finance the construction of\n",
      "-  build.v.06  |  give form to, according to a plan\n",
      "-  build.v.07  |  be engaged in building\n",
      "-  build.v.08  |  found or ground\n",
      "-  build_up.v.04  |  bolster or strengthen\n",
      "-  build.v.10  |  develop and grow\n",
      "\n",
      " switch\n",
      "-  switch.n.01  |  control consisting of a mechanical or electrical or electronic device for making or breaking or changing the connections in a circuit\n",
      "-  substitution.n.01  |  an event in which one thing is substituted for another\n",
      "-  switch.n.03  |  hairpiece consisting of a tress of false hair; used by women to give shape to a coiffure\n",
      "-  switch.n.04  |  railroad track having two movable rails and necessary connections; used to turn a train from one track to another or to store rolling stock\n",
      "-  switch.n.05  |  a flexible implement used as an instrument of punishment\n",
      "-  switch.n.06  |  a basketball maneuver; two defensive players shift assignments so that each guards the player usually guarded by the other\n",
      "-  switch.n.07  |  the act of changing one thing or position for another\n",
      "-  switch_over.v.01  |  change over, change around, as to a new order or sequence\n",
      "-  trade.v.04  |  exchange or give (something) in exchange for\n",
      "-  switch.v.03  |  lay aside, abandon, or leave for another\n",
      "-  switch.v.04  |  make a shift in or exchange of; then we switched\"\n",
      "-  throw.v.06  |  cause to go on or to be engaged or set in operation\n",
      "-  switch.v.06  |  flog with or as if with a flexible rod\n",
      "-  interchange.v.04  |  reverse (a direction, attitude, or course of action)\n",
      "\n",
      " introduc : NO SYNSETS\n",
      "\n",
      "\n",
      " sub\n",
      "-  bomber.n.03  |  a large sandwich made of a long crusty roll split lengthwise and filled with meats and cheese (and tomato and onion and lettuce and condiments); different names are used in different sections of the United States\n",
      "-  submarine.n.01  |  a submersible warship usually armed with torpedoes\n",
      "-  substitute.v.02  |  be a substitute\n",
      "\n",
      " dot\n",
      "-  point.n.09  |  a very small circular shape\n",
      "-  department_of_transportation.n.01  |  the United States federal department that institutes and coordinates national transportation programs; created in 1966\n",
      "-  dot.n.03  |  the shorter of the two telegraphic signals used in Morse code\n",
      "-  acid.n.02  |  street name for lysergic acid diethylamide\n",
      "-  dot.v.01  |  scatter or intersperse like dots or studs\n",
      "-  scatter.v.03  |  distribute loosely\n",
      "-  dot.v.03  |  make a dot or dots\n",
      "-  dot.v.04  |  mark with a dot\n",
      "\n",
      " paramet : NO SYNSETS\n",
      "\n",
      "\n",
      " input\n",
      "-  input_signal.n.01  |  signal going into an electronic system\n",
      "-  remark.n.01  |  a statement that expresses a personal opinion or belief or adds information\n",
      "-  stimulation.n.02  |  any stimulating information or event; acts to arouse action\n",
      "-  input.n.04  |  a component of production; something that goes into the production of output\n",
      "-  input.v.01  |  enter (data or a program) into a computer\n",
      "\n",
      " car\n",
      "-  car.n.01  |  a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "-  car.n.02  |  a wheeled vehicle adapted to the rails of railroad\n",
      "-  car.n.03  |  the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "-  car.n.04  |  where passengers ride up and down\n",
      "-  cable_car.n.01  |  a conveyance for passengers or freight on a cable railway\n",
      "\n",
      " vr : NO SYNSETS\n",
      "\n",
      "\n",
      " vl : NO SYNSETS\n",
      "\n",
      "\n",
      " unicyc : NO SYNSETS\n",
      "\n",
      "\n",
      " spee : NO SYNSETS\n",
      "\n",
      "\n",
      " omeg : NO SYNSETS\n",
      "\n",
      "\n",
      " tick\n",
      "-  tick.n.01  |  a metallic tapping sound\n",
      "-  tick.n.02  |  any of two families of small parasitic arachnids with barbed proboscis; feed on blood of warm-blooded animals\n",
      "-  check_mark.n.01  |  a mark indicating that something has been noted or completed etc.\n",
      "-  tick.n.04  |  a light mattress\n",
      "-  click.v.02  |  make a clicking or ticking sound\n",
      "-  tick.v.02  |  make a sound like a clock or a timer\n",
      "-  tick.v.03  |  sew\n",
      "-  check.v.06  |  put a check mark on or near or next to\n",
      "\n",
      " encod : NO SYNSETS\n",
      "\n",
      "\n",
      " extern\n",
      "-  extern.n.01  |  a nonresident doctor or medical student; connected with a hospital but not living there\n",
      "\n",
      " dist : NO SYNSETS\n",
      "\n",
      "\n",
      " count\n",
      "-  count.n.01  |  the total number counted\n",
      "-  count.n.02  |  the act of counting; reciting numbers in ascending order\n",
      "-  count.n.03  |  a nobleman (in various countries) having rank equal to a British earl\n",
      "-  count.v.01  |  determine the number or amount of\n",
      "-  count.v.02  |  have weight; have import, carry weight\n",
      "-  consider.v.04  |  show consideration for; take into account\n",
      "-  count.v.04  |  name or recite the numbers in ascending order\n",
      "-  count.v.05  |  put into a group\n",
      "-  count.v.06  |  include as if by counting\n",
      "-  count.v.07  |  have a certain value or carry a certain weight\n",
      "-  count.v.08  |  have faith or confidence in\n",
      "-  reckon.v.06  |  take account of\n",
      "\n",
      " gps\n",
      "-  global_positioning_system.n.01  |  a navigational system involving satellites and computers that can determine the latitude and longitude of a receiver on Earth by computing the time difference for signals from different satellites to reach the receiver\n",
      "\n",
      " interest\n",
      "-  interest.n.01  |  a sense of concern with and curiosity about someone or something\n",
      "-  sake.n.01  |  a reason for wanting something done\n",
      "-  interest.n.03  |  the power of attracting or holding one's attention (because it is unusual or exciting etc.)\n",
      "-  interest.n.04  |  a fixed charge for borrowing money; usually a percentage of the amount borrowed\n",
      "-  interest.n.05  |  (law) a right or legal share of something; a financial involvement with something\n",
      "-  interest.n.06  |  (usually plural) a social group whose members control some field of activity and who have common aims\n",
      "-  pastime.n.01  |  a diversion that occupies one's time and thoughts (usually pleasantly)\n",
      "-  interest.v.01  |  excite the curiosity of; engage the interest of\n",
      "-  concern.v.02  |  be on the mind of\n",
      "-  matter_to.v.01  |  be of importance or consequence\n",
      "\n",
      " comput : NO SYNSETS\n",
      "\n",
      "\n",
      " eiffel\n",
      "-  eiffel.n.01  |  French engineer who constructed the Eiffel Tower (1832-1923)\n",
      "\n",
      " tow\n",
      "-  tow.n.01  |  the act of hauling something (as a vehicle) by means of a hitch or rope\n",
      "-  tow.v.01  |  drag behind\n",
      "\n",
      "\n",
      "------- CUSTOM TERMS in WORDNET (also domain specific) ----------\n",
      "\n",
      " Support vector machine : NO SYNSETS\n",
      "\n",
      "\n",
      " design pattern : NO SYNSETS\n",
      "\n",
      "\n",
      " iot : NO SYNSETS\n",
      "\n",
      "\n",
      " integrity\n",
      "-  integrity.n.01  |  an undivided or unbroken completeness or totality with nothing wanting\n",
      "-  integrity.n.02  |  moral soundness\n",
      "\n",
      " java\n",
      "-  java.n.01  |  an island in Indonesia to the south of Borneo; one of the world's most densely populated regions\n",
      "-  coffee.n.01  |  a beverage consisting of an infusion of ground coffee beans\n",
      "-  java.n.03  |  a platform-independent object-oriented programming language\n",
      "\n",
      " pattern\n",
      "-  form.n.03  |  a perceptual structure\n",
      "-  practice.n.01  |  a customary way of operation or behavior\n",
      "-  design.n.04  |  a decorative or artistic work\n",
      "-  convention.n.02  |  something regarded as a normative example\n",
      "-  pattern.n.05  |  a model considered worthy of imitation\n",
      "-  blueprint.n.01  |  something intended as a guide for making something else\n",
      "-  traffic_pattern.n.01  |  the path that is prescribed for an airplane that is preparing to land at an airport\n",
      "-  radiation_pattern.n.01  |  graphical representation (in polar or Cartesian coordinates) of the spatial distribution of radiation from an antenna as a function of angle\n",
      "-  model.v.01  |  plan or create according to a model or models\n",
      "-  pattern.v.02  |  form a pattern\n",
      "\n",
      " k-nearest neighbors : NO SYNSETS\n",
      "\n",
      "\n",
      " example\n",
      "-  example.n.01  |  an item of information that is typical of a class or group\n",
      "-  model.n.07  |  a representative form or pattern\n",
      "-  exemplar.n.01  |  something to be imitated\n",
      "-  example.n.04  |  punishment intended as a warning to others\n",
      "-  case.n.01  |  an occurrence of something\n",
      "-  exercise.n.04  |  a task performed or problem solved in order to develop skill or understanding\n",
      "\n",
      " lifecycle : NO SYNSETS\n",
      "\n",
      "\n",
      " machine learning : NO SYNSETS\n",
      "\n",
      "\n",
      " database\n",
      "-  database.n.01  |  an organized body of related information\n",
      "\n",
      " data management : NO SYNSETS\n",
      "\n",
      "\n",
      " svm : NO SYNSETS\n",
      "\n",
      "\n",
      " bloom\n",
      "-  blooming.n.01  |  the organic process of bearing flowers\n",
      "-  flower.n.02  |  reproductive organ of angiosperm plants especially one having showy or colorful parts\n",
      "-  bloom.n.03  |  the best time of youth\n",
      "-  bloom.n.04  |  a rosy color (especially in the cheeks) taken as a sign of good health\n",
      "-  flower.n.03  |  the period of greatest prosperity or productivity\n",
      "-  efflorescence.n.04  |  a powdery deposit on a surface\n",
      "-  bloom.v.01  |  produce or yield flowers\n",
      "\n",
      " knn : NO SYNSETS\n",
      "\n",
      "\n",
      " filter\n",
      "-  filter.n.01  |  device that removes something from whatever passes through it\n",
      "-  filter.n.02  |  an electrical device that alters the frequency spectrum of signals passing through it\n",
      "-  filter.v.01  |  remove by passing through a filter\n",
      "-  percolate.v.05  |  pass through\n",
      "-  trickle.v.01  |  run or flow slowly, as in drops or in an unsteady stream\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------- NLTK, WORDNET -------------------------------------------\n",
    "print(\"\\n\\n------- EXPORTED TERMS in WORDNET ----------\") \n",
    "for word in exportedList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "\n",
    "print(\"\\n\\n------- CUSTOM TERMS in WORDNET (also domain specific) ----------\")    \n",
    "for word in ownList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words, and all the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "\n",
    "# BoF\n",
    "def bagOfWords(iFile,POSfileIn):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    data_corpus = []  \n",
    "    \n",
    "    if iFile.name.endswith(\"_lemmatized.txt\"):\n",
    "        print(\"\\n[Bag of words: ]\\t\" + os.path.basename(iFile.name))\n",
    "\n",
    "        baseName = iFile.name.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_tokPOStag.txt\" \n",
    "        \n",
    "        LemmafileCont = iFile.read().split()\n",
    "        #POSfileCont = POSfileIn.read().split()\n",
    "       \n",
    "        listLemmas = []\n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            for line in LemmafileCont:\n",
    "                LemmafileCont.append(line)\n",
    "                \n",
    "            listLemmas = findRealTerm(listLemmas,POSfileIn)\n",
    "            \n",
    "            text = \"\"\n",
    "            for line in LemmafileCont:\n",
    "                #fullTerm = findRealTerm(iFile,line,POSfileList)\n",
    "                #print(fullTerm)\n",
    "                #text = text,\" \",fullTerm,\" \"\n",
    "                #fullT = findRealTerm(line,POSfileIn)\n",
    "                #findRealTerm(line,POSfileIn)   # Look for the full term from the lemma\n",
    "                text += line+\" \"\n",
    "                #pass\n",
    "                continue\n",
    "            data_corpus.append(text)\n",
    "        \n",
    "        #print(data_corpus)\n",
    "        vector = vectorizer.fit_transform(data_corpus) \n",
    "        #print(vector.toarray())\n",
    "        #print(vectorizer.get_feature_names())  \n",
    "        \n",
    "        array = vector.toarray()\n",
    "        featureNames = vectorizer.get_feature_names()\n",
    "        \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# ----------------------------------------- FINISH ---------------------------------------------\n",
    "\n",
    "# Match the lemma to first found word from file _lemmatized.txt for that file\n",
    "def findRealTerm(listLemmas, POSfileIn):   \n",
    "    # take the path of the input file and look for the file ending on \"_stemmedbyPOS.txt\"\n",
    "    # split each line into 3, look in line[3] for the first match of the current lemma\n",
    "    # when found, take line[0] which is the full word and return that word\n",
    "    # exit the function\n",
    "    #print(\"Looking for term for lemma: \" + lemmaIn)\n",
    "\n",
    "    #print(\"Searching for term\")\n",
    "    #res = \"\"\n",
    "    resList = []\n",
    "    posfileCont = POSfileIn.read().split()\n",
    "\n",
    "    for curLemma in listLemmas:\n",
    "        for line in posfileCont:\n",
    "            line = line.split(\",\")\n",
    "            #print(len(line))\n",
    "\n",
    "            word = line[0]\n",
    "            lemma = line[2]\n",
    "\n",
    "            if lemma == curLemma:\n",
    "                print(\"WORD: \", word, \" for LEMMA: \", lemma)\n",
    "                res = res.append(word)\n",
    "\n",
    "    return resList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/one file/01_what-is-the-definition-of-derivative.en_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/one file/04_how-does-wiggling-x-affect-f-x.en_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/one file/NounVerbAdjectiveAdverb.en_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:37: ResourceWarning: unclosed file <_io.TextIOWrapper name='/media/sf_Shared_Folder/TEST/one file/test_stemmedbyPOS.txt' mode='r' encoding='ISO-8859-1'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Bag of words: ]\t01_what-is-the-definition-of-derivative.en_lemmatized.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2909\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2910\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2911\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-cb46d4a50de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                             \u001b[0mlookFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOSfileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#IMPORTANT ENCODING! UTF8 DOESN'T WORK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                             \u001b[0mbagOfWords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlookFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                             \u001b[0;31m#findRealTerm(curFile,\"wiggl\",lookFile)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-83c43b2e1904>\u001b[0m in \u001b[0;36mbagOfWords\u001b[0;34m(iFile, POSfileIn)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLemmafileCont\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mLemmafileCont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#path = r\"C:\\Users\\a.dimitrova\\Desktop\\TEST data\\PROCESSED\\Course-data\\mobile-robot\\02_mobile-robots\\01_week-2\"\n",
    "path = \"/media/sf_Shared_Folder/TEST/one file\"\n",
    "\n",
    "# LINUX\n",
    "#path = \"/media/sf_Shared_Folder/TEST/one file\" # FEW TEST FILES\n",
    "#path = \"/media/sf_Shared_Folder/TEST/RAW\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "counter = 0\n",
    "POSfiles = []\n",
    "\n",
    "# --- Collecting the POS files\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # create a list of files for POS so that it can be sent along with BoF to look for the right file and terms\n",
    "            if filePath.endswith(\"_stemmedbyPOS.txt\"):\n",
    "                curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                baseName = os.path.basename(curFile.name.split(\".en\", 1)[0])\n",
    "                curFilePOS = baseName+\".en_stemmedbyPOS.txt\"\n",
    "                POSfiles.append(curFilePOS)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "            \n",
    "# --- processing Lemmatized files with Algos\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:                \n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if filePath.endswith(\"_lemmatized.txt\"): \n",
    "                try: \n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    \n",
    "                    baseName = curFile.name.split(\".en\", 1)[0]\n",
    "                    POSfileName = baseName+\".en_stemmedbyPOS.txt\"\n",
    "                    if os.path.basename(POSfileName) in POSfiles:\n",
    "                        \n",
    "                        try:\n",
    "                            lookFile = open(POSfileName, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                            bagOfWords(curFile,lookFile)\n",
    "                            #findRealTerm(curFile,\"wiggl\",lookFile)\n",
    "                        # bag of words processing:\n",
    "                        #findRealTerm(curFile,item,POSfile)\n",
    "                            \n",
    "                    #unifyLemmas(curFile,POSfiles)\n",
    "                        finally: \n",
    "                            lookFile.close()\n",
    "                finally: \n",
    "                    curFile.close()            \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "#print(\"Total number of POS Files[]:\", len(POSfiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "### NOTES\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**TF-IDF** doesn't output the necessary result, I need n-grams selected as a combined keyword and these are often very general words like `for example` or `key concept` etc. in order to classify the text into the GOAL element. \n",
    "\n",
    "**TextBlob** provides options for n-grams and also connection to WordNet ontology which could be useful, so will look more into it.\n",
    "\n",
    "**WordNet** finds multiple definitions and synsets (synonyms) for most of the general words, however if provided specific e.g. computer science algorithm names, or specific terms, it doesn find any synonyms, nor descriptions of any of them.\n",
    "\n",
    "**Wikipedia** recognized some of the terms, but not all. For instance if we give it KNN it doesn't find anything, but if we give it K-nearest neighbour, if finds it. This is how the name is in Wikipedia, so that may be the reason. But on Google first returned result for KNN is this article. Same for SVM and Support vector machine. I've modified the script to return \"NO DESCRIPTION or DISAMBIGUATION\" everytime if finds nopthing ot if there's a disambiguation error, otherwise it wouldn continue checking the rest of the terms. So now it skips the error. \n",
    " \n",
    "**Full list** of identified key words so far [HERE](https://docs.google.com/spreadsheets/d/1Dj4UAh6U5jAelcsz-gDCdDE9JRVhwaNei0Ctn8m0Ui4/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
