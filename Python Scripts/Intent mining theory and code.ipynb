{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How do researchers deal with it:\n",
    "- [Word embeddings Wiki](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "- [Gensim Python library](https://en.wikipedia.org/wiki/Gensim)\n",
    "- [Inference Rules Wiki](https://en.wikipedia.org/wiki/Rule_of_inference)\n",
    "\n",
    "### LSTM: \n",
    "- [Long-Short term memory (LSTM)](https://www.datacamp.com/community/tutorials/lstm-python-stock-market#lstm)\n",
    "- [Learn via example](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "\n",
    "### Literature:\n",
    "- [Intent extraction from social media texts using sequential segmentation and deep learning models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8119461) uses CRFs and Bi-LSTM for intent extraction from texts from social media in 2 categories - Cosmetics and Tourism. Look into these algos\n",
    "    - Citation: \n",
    "`@INPROCEEDINGS{8119461, \n",
    "author={T. L. Luong and M. S. Cao and D. T. Le and X. H. Phan}, \n",
    "booktitle={2017 9th International Conference on Knowledge and Systems Engineering (KSE)}, \n",
    "title={Intent extraction from social media texts using sequential segmentation and deep learning models}, \n",
    "year={2017}, \n",
    "pages={215-220}, \n",
    "doi={10.1109/KSE.2017.8119461}, \n",
    "month={Oct},}`\n",
    "\n",
    "\n",
    "- In [Semantic Indexing for Recorded Educational Lecture Videos](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1598977) they extracted scripts from videos with timestamps on each word and cluster them in order to allow for finding of the exact position of a particular thing in the video. They also use a retrieval method to find “example”, “explanation”, “overview”, “repetition”, “exercise” for a particular word or topic word. \n",
    "    - Citation: `@INPROCEEDINGS{1598977, \n",
    "author={S. Repp and M. Meinel}, \n",
    "booktitle={Fourth Annual IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOMW'06)}, \n",
    "title={Semantic indexing for recorded educational lecture videos}, \n",
    "year={2006}, \n",
    "pages={5 pp.-245}, \n",
    "month={March},}`\n",
    "\n",
    "\n",
    "- In [Olex: Effective Rule Learning for Text Categorization](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4641927) Sees the problem as a text classification task and applied Inference Rules onto it. Not particularly for intent mining, but for different categories, similar to what I have. The inference rules are of the form: \\begin{equation}If \\space T_1 \\space or \\space \\dots \\space T_n \\space occurs \\space in \\space document \\space d,\\space and \\space none \\space of \\space T_{n+1} \\dots T_{n+m} \\space occurs \\space in \\space d, \\space then \\space classify \\space d \\space under \\space category \\space C \\end{equation}  This includes `one` positive literal and `0+` negative literals and temrs are `n-grams`\n",
    "\n",
    "    - Citation `@ARTICLE{4641927, \n",
    "author={P. Rullo and V. L. Policicchio and C. Cumbo and S. Iiritano}, \n",
    "journal={IEEE Transactions on Knowledge and Data Engineering}, \n",
    "title={Olex: Effective Rule Learning for Text Categorization}, \n",
    "year={2009}, \n",
    "volume={21}, \n",
    "number={8}, \n",
    "pages={1118-1132}, \n",
    "doi={10.1109/TKDE.2008.206}, \n",
    "ISSN={1041-4347}, \n",
    "month={Aug},}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Rules method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Theory\n",
    "### General Rules\n",
    "1. If sentence has no label, proceed with label search.\n",
    "2. If no label can be assigned, assign the last applied labeled from a previous sentence\n",
    "3. We need priority as we often get results from more than one label and therefore we need to put weight on the results from some of the rules over others. \n",
    "\n",
    "-----------------------------------\n",
    "### ALL RULES\n",
    "- [Priority] [Status] [Label] <<< [RULE]\n",
    "- [1] [✔] EX_1 <<< `example` || OR `for instance` || `assume` || `suppose` || `imagine` || `as` || `simulation` || `diagram` \n",
    "- [2] [✔] EX_2 <<< `Let's` __&&__ try || think || see || pick || take a look || say ..\n",
    "- [2] [✔] CD_1 <<< `Let's` __&&__ look at || make || put || do || start || prove || evaluate || back || try || just __AND NO__ `example` || `assume` || `suppose` || `imagine` || `diagram`\n",
    "- [4] [✔] CD_2 <<< `in other words` || `basically` __AND NO__ `should` || `have to` || `must` \n",
    "- [4] [✔] CD_3 <<< `so` is the first word in the sentence __&&__ `it's` || `i'm`\n",
    "- [3] [✔] CD_4 <<< `so this is` || `actually` __AND NO__ `example` || `summary` || `next` || `last`\n",
    "- [1] [✔] CD_5 <<< `means` || `mean` || `given` || `define` || `explain` __AND NO__ Present continuous tense (going to)\n",
    "- [3] [✔] CD_6 <<< `what if` __AND NO__ `example` || `instance`\n",
    "- [1] [✔] SM_1 <<< `Let's` __&&__ summarize || `recap` \n",
    "- [4] [✔] SM_2 <<< `in other words` __&&__ past tense\n",
    "- [3] [✔] SM_3 <<< `this week` || `this lesson` || `today` __&&__ present tense || future tense\n",
    "- [1] [✔] SM_4 <<< `later` || `next time` || `last time` || `summary` || `summarize` || `here is` || `here are` || `discuss` || `next` \n",
    "- [4] [✔] SM_5 <<< if (lineNr < 10 `OR` lineNr > fileLinesNr - 10) __&&__ (past tense) =>> (within the first or last 10 lines + past tense)\n",
    "- [2] [✔] SM_6 <<< `going to` __&&__ `look` || `see` || `be` || `think` || `explain` || `explained` __AND NO__ present tense (&& future or past)\n",
    "- [1] [✔] AP_1 <<< `in other words` __&&__ `should` || `could` || `would` [✔]\n",
    "- [1] [✔] AP_2 <<< `encourage` || `step` || `first` || `finally` || `second` || `should` || `could` || `would` || `best practice(s)` || `need to` || `homework` || `you can` || `make sure`\n",
    "- [2] [✔] AP_3 <<< `if` __&&__ `use` || `can` || `should` || `could` || `want`\n",
    "- [1] [✔] CM_1 <<< `called` __&&__ concept\n",
    "- [2] [✔] CM_2 <<< `what is` .. __&&__ concept\n",
    "- [3] [✔] CM_3 <<< `theorem` || `algorithm` || `method` || `let's use` || `theory`\n",
    "- [1] [✔] CM_4 <<< first occurence of the terms in the title of the file\n",
    "- [ ] [**X**] CM_5 <<< `let's` __&&__ `use` - **REPEATS CM_3** \n",
    "\n",
    "Based on manual analysis:\n",
    "Label prioritization:\n",
    "- CM == EX   =>    EX\n",
    "- CM == SM   =>    SM\n",
    "- AP == CD   =>    CD\n",
    "- SM == AP   =>    AP\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "### Logical expressions (copy/paste in thesis later - LATEX style)\n",
    "#### ♦ EXAMPLE\n",
    "1. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"let's\" \\in d \\space) \\space \\land (\"try\" \\in d \\space \\lor \"see\" \\in d \\space \\lor \"think\" \\in d \\space \\lor \"pick\" \\in d \\space \\lor \"say\" \\in d) \\end{equation} \n",
    "\n",
    "2. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"example\" \\in d \\space) \\lor (\"for \\space instance\" \\in d) \\space \\lor (\"suppose\" \\in d) \\space \\lor (\"assume\" \\in d) \\space \\lor (\"includes\" \\in d) \\space \\lor (\"imagine\" \\in d) \\space \\end{equation} \n",
    "\n",
    "Latex Formula Formatter: https://www.codecogs.com/eqnedit.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pseudocode\n",
    "\n",
    "`Disregard all sentences that have NL label, totally ignore, then: [✔]\n",
    "    if sentence has no label:\n",
    "        for line in text:\n",
    "            if lineNR < 10 OR lineNR > nrOfLines-10:\n",
    "                for word in line:\n",
    "                    if (60%+ of the words on the line are in PAST TENSE):\n",
    "                        go over the SM rules  (append res to curSentLabels)\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in PRESENT TENSE):\n",
    "                        go over the CD rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in FUTURE TENSE):\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "            if lineNR > 10 AND lineNR < nrOfLines-10:\n",
    "                go over the EX rules  (append res to curSentLabels)\n",
    "                go over the CD rules  (append res to curSentLabels)\n",
    "                go over the AP rules  (append res to curSentLabels)\n",
    "                go over the CM rules  (append res to curSentLabels)\n",
    "        Count the labels with a special method for this: [✔]\n",
    "            if all rules fail to assign a label, i.e. if all labels return count 0:\n",
    "                search for the last labeled sentence:\n",
    "                    assign its label to the current sentence`                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### [THEORY] Checking the tense of the verbs in the sentence\n",
    "\n",
    "- [All POS Tags](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "NLTK PPOS TAGS for verbs:\n",
    "\n",
    "- VBD\t\tverb, past tense\t\t\t\t\t`took`      +1 if checked for PAST, else 0\n",
    "- VBN\t\tverb, past participle\t\t\t\t`taken`     +1 if checked for PAST, else 0\n",
    "- VB\t\tverb, base form\t\t\t\t\t\t`take`      +0.5\n",
    "- VBG\t\tverb, gerund/present participle\t\t`taking`    +0.5\n",
    "- VBP\t\tverb, sing. present, non-3d\t\t\t`take`      +1 if checked for PRESENT, else 0\n",
    "- VBZ\t\tverb, 3rd person sing. present\t\t`takes`     +1 if checked for PRESENT, else 0\n",
    "- MD        modal verb (will, shall)            `will`      +1 if checked for FUTURE, else 0\n",
    "\n",
    "\n",
    "**Simply counting verbs isn't enough, artificial boost is added if certain types of verbs are present so that they\n",
    "form a specific English tense. All listed below:**\n",
    "\n",
    "- **Past perfect**: `VBD`(had) + `VBN`(been) ------- BUT NO VBG(-ing/gerund)\n",
    "- **Past continuous tense**: `VBD`(was/were) + `VBG`(-ing/gerund)\n",
    "- **Past perfect continuous**: `VBD`(had) + `VBN`(been) + `VBG`(-ing/gerund)\n",
    "- **PRESENT perfect**: `VBP`(have) + `VBN`(been) ------- BUT NO VBG(-ing/gerund)\n",
    "- **PRESENT perfect continuous**: `VBP`(have) + `VBN`(been) + `VBG`(-ing/gerund)\n",
    "- **PRESENT continuous**: `VBP`(is/are) + `VBG`(-ing/gerund)\n",
    "- **Future continuous**: `MD`(WILL) + `VBG`(-ing/gerund)\n",
    "- **Future perfect**: `MD`(will) + `VB`(have) + `VBN`(PP)\n",
    "- **Future perfect continuous**: `MD`(will) + `VBN`(been) + `VB`(have) + `VBG`(-ing/gerund)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Inference Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import dis\n",
    "import time\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for present / past / future tense\n",
    "\n",
    "All return a number which is the percentage of likelihood that the sentence is of certain tense\n",
    "\n",
    "**Version 1** checked also for the more complicated tenses such as perfect, continuous etc of each present, past, future by assigning addiotnal scores according to whether all criteria is covered, in order to make the output higher and ==> more certain.\n",
    "\n",
    "**Version 2** only counts the types of verbs for each tense\n",
    "- checkPRES(sentence)\n",
    "- checkPAST(sentence)\n",
    "- checkFUTURE(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ======================================== PAST TENSE CHECK ==============================================\n",
    "\n",
    "puncDict = [\",\",\".\",\";\",\"-\",\"_\",\"`\",\"'\",\"?\",\"!\",\":\"]\n",
    "\n",
    "def checkPAST2(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, pastTense, grammarCase, VBGgerund, VBNpp, VBDHad = 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "#            print(tag)\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBD\": \n",
    "                pastTense += 1\n",
    "                VBDHad += 1\n",
    "            elif str(tag[1]) == \"VBN\": \n",
    "                pastTense += 1\n",
    "                VBNpp += 1\n",
    "            elif str(tag[1]) == \"VBG\": VBGgerund += 1\n",
    "            else: pass\n",
    "    \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense\n",
    "    # TODO re-evaluate points\n",
    "    #  1   1   0.5\n",
    "    # VBD VBN  VBG\n",
    "    #print(\"VBD \",VBDHad,\" VBN \",VBNpp,\" VBG \",VBGgerund)\n",
    "    if VBDHad > 0 and VBNpp > 0 and VBGgerund == 0: grammarCase += 2                   #Past perfect \n",
    "    elif VBDHad > 0 and VBNpp == 0 and VBGgerund > 0: grammarCase += 1.5                #Past continuous tense \n",
    "    elif VBDHad > 0 and VBNpp > 0 and VBGgerund > 0: grammarCase += 2.5                  #Past perfect continuous\n",
    "    else: grammarCase += 0\n",
    "        \n",
    "    if pastTense == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        #grammarCase = grammarCase * 0.1 \n",
    "        # big / small = 100 / x\n",
    "        #calculate verbs over words\n",
    "        ratio_verbs = len(text) / verbs\n",
    "        perc_verbs= (100 / ratio_verbs) \n",
    "\n",
    "        #calculate past tense verbs over all verbs\n",
    "        ratio_PastVerbs = verbs / pastTense\n",
    "        perc_PastVerbsOverVerbs = (100 / ratio_PastVerbs)\n",
    "\n",
    "        #calculate past tense verbs over all words\n",
    "        ratio_PastVerbsOverWords = len(text) / pastTense\n",
    "        perc_VerbsPastTenseOverWords = 100 / ratio_PastVerbsOverWords\n",
    "\n",
    "        \"\"\"\n",
    "        if perc_PastVerbsOverVerbs >= 30:\n",
    "            return perc_PastVerbsOverVerbs\n",
    "        else:\n",
    "            print(\"{0:.2f}\".format(perc_PastVerbsOverVerbs))\n",
    "            return \"NPAST\"\n",
    "        \"\"\"\n",
    "\n",
    "        if not grammarCase == 0:\n",
    "            return grammarCase * 100\n",
    "        else:\n",
    "            if perc_PastVerbsOverVerbs >= 30:\n",
    "                return perc_PastVerbsOverVerbs\n",
    "            else:\n",
    "                #print(\"{0:.2f}\".format(perc_PastVerbsOverVerbs))\n",
    "                return -1\n",
    "            \n",
    "            \n",
    "# ========================================= PRESENT TENSE CHECK =============================================\n",
    "    \n",
    "def checkPRESENT2(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, presTenseVerbs, grammarCase, VBGgerund, VBPhave, VBNbeen = 0, 0, 0, 0, 0, 0 \n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBP\": \n",
    "                presTenseVerbs += 1\n",
    "                VBPhave += 1\n",
    "            elif str(tag[1]) == \"VBN\": \n",
    "                VBNbeen += 1\n",
    "            elif str(tag[1]) == \"VBG\":\n",
    "                presTenseVerbs += 1\n",
    "                VBGgerund += 1\n",
    "            elif str(tag[1]) == \"VB\":\n",
    "                presTenseVerbs += 1\n",
    "            elif str(tag[1]) == \"VBZ\":\n",
    "                presTenseVerbs += 1\n",
    "            else: pass\n",
    "                \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense.\n",
    "    #print(\"VBG \",VBGgerund,\" VBP \",VBPhave,\" VBN \",VBNbeen)\n",
    "    # TODO re-evaluate points\n",
    "    # 0.5   1    0\n",
    "    # VBG  VBP  VBN\n",
    "    \n",
    "    if VBGgerund == 0 and VBPhave > 0 and VBNbeen > 0: grammarCase += 1                   # PRESENT perfect\n",
    "    elif VBGgerund > 0 and VBPhave > 0 and VBNbeen == 0: grammarCase += 1.5               # PRESENT continuous \n",
    "    elif VBGgerund > 0 and VBPhave > 0 and VBNbeen > 0: grammarCase += 1.5                # PRESENT perfect continuous\n",
    "    else: grammarCase += 0\n",
    "    \n",
    "    #grammarCase = grammarCase * 0.1\n",
    "    #calculate past tense verbs over all verbs\n",
    "    if presTenseVerbs == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_PresVerbs = verbs / presTenseVerbs\n",
    "        perc_PresVerbsOverVerbs = (100 / ratio_PresVerbs)\n",
    "\n",
    "        if not grammarCase == 0:\n",
    "            return grammarCase * 100\n",
    "        else:\n",
    "            if perc_PresVerbsOverVerbs >= 30:\n",
    "                return perc_PresVerbsOverVerbs\n",
    "            else:\n",
    "                #print(\"{0:.2f}\".format(perc_PresVerbsOverVerbs))\n",
    "                return -1\n",
    "\n",
    "\n",
    "# ======================================= FUTURE TENSE CHECK ===============================================\n",
    "\n",
    "def checkFUTURE2(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, futureTenseWords, grammarCase, MDmodal, VBGgerund, VBhave, VBNpp = 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBG\":\n",
    "                VBGgerund += 1\n",
    "                futureTenseWords += 1\n",
    "            elif str(tag[1]) == \"VB\":\n",
    "                VBhave += 1\n",
    "            elif str(tag[1]) == \"VBN\":\n",
    "                VBNpp += 1\n",
    "        if str(tag[1]) == \"MD\" and str(tag[0]) == \"will\" or str(tag[0]) == \"shall\":\n",
    "            MDmodal += 1\n",
    "            futureTenseWords += 1\n",
    "                \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense.\n",
    "    #print(\"VBG \",VBGgerund,\" VBP \",VBPhave,\" VBN \",VBNbeen)\n",
    "    # TODO re-evaluate points\n",
    "    # 0.5   0    0    1\n",
    "    # VBG   VB  VBN   MD\n",
    "    \n",
    "    if MDmodal > 0 and VBhave > 0 and VBNpp > 0: grammarCase += 1                         # Fut. perf.    MD/VB/VBN\n",
    "    elif MDmodal > 0 and VBGgerund > 0: grammarCase += 1.5                                # Fut. cont.  MD/VBG\n",
    "    elif MDmodal > 0 and VBhave > 0 and VBNpp > 0 and VBGgerund > 0: grammarCase += 1.5   # Fut. perf. cont. MD/VB/VBN/VBG\n",
    "    else: grammarCase += 0\n",
    "    \n",
    "    #grammarCase = grammarCase * 1.0\n",
    "    #calculate past tense verbs over all verbs\n",
    "    if futureTenseWords == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_FutureVerbs = verbs / futureTenseWords\n",
    "        perc_FutureWordsOverVerbs = (100 / ratio_FutureVerbs)\n",
    "\n",
    "        if not grammarCase == 0:\n",
    "            return grammarCase * 100\n",
    "        else:\n",
    "            if perc_FutureWordsOverVerbs >= 30:\n",
    "                return perc_FutureWordsOverVerbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Version 2 (USE THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ================================= SECOND VERSION OF TENSE CHECK ========================================\n",
    "\n",
    "# ======================================== PAST TENSE CHECK ==============================================\n",
    "\n",
    "puncDict = [\",\",\".\",\";\",\"-\",\"_\",\"`\",\"'\",\"?\",\"!\",\":\"]\n",
    "\n",
    "def checkPAST(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, pastTense, VBGgerund = 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "#            print(tag)\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBD\": pastTense += 1\n",
    "            elif str(tag[1]) == \"VBN\": pastTense += 1\n",
    "            elif str(tag[1]) == \"VBG\": pastTense += 1\n",
    "            else: pass\n",
    "        \n",
    "    if pastTense == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        #calculate past tense verbs over all verbs\n",
    "        ratio_PastVerbs = verbs / pastTense\n",
    "        perc_PastVerbsOverVerbs = (100 / ratio_PastVerbs)\n",
    "        \n",
    "        return perc_PastVerbsOverVerbs\n",
    "            \n",
    "            \n",
    "# ========================================= PRESENT TENSE CHECK =============================================\n",
    "    \n",
    "def checkPRESENT(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, presTenseVerbs = 0, 0 \n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBP\": presTenseVerbs += 1\n",
    "            elif str(tag[1]) == \"VBG\":\n",
    "                presTenseVerbs += 1\n",
    "            elif str(tag[1]) == \"VBZ\": presTenseVerbs += 1\n",
    "            else: pass\n",
    "            \n",
    "    if presTenseVerbs == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_PresVerbs = verbs / presTenseVerbs\n",
    "        perc_PresVerbsOverVerbs = (100 / ratio_PresVerbs)\n",
    "        \n",
    "        return perc_PresVerbsOverVerbs\n",
    "\n",
    "\n",
    "# ======================================= FUTURE TENSE CHECK ===============================================\n",
    "\n",
    "def checkFUTURE(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, futureTenseWords, grammarCase = 0, 0, 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBG\": futureTenseWords += 1\n",
    "        if str(tag[1]) == \"MD\" and str(tag[0]) == \"will\" or str(tag[0]) == \"shall\":\n",
    "            futureTenseWords += 1\n",
    "            \n",
    "    if futureTenseWords == 0:\n",
    "        return -1\n",
    "    else:\n",
    "        ratio_FutureVerbs = verbs / futureTenseWords\n",
    "        perc_FutureWordsOverVerbs = (100 / ratio_FutureVerbs)\n",
    "        \n",
    "        return perc_FutureWordsOverVerbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "that means we will create this next time\n",
    "\"\"\"\n",
    "\n",
    "# -------- check sent for PAST tense\n",
    "print(\"Past tense chance: {} %\".format(int(checkPAST(sentence))))\n",
    "\n",
    "# -------- check sent for PRESENT tense    \n",
    "print(\"Present tense chance: {} %\".format(int(checkPRESENT(sentence))))\n",
    "\n",
    "# -------- check sent for FUTURE tense\n",
    "print(\"Future tense chance: {} %\".format(int(checkFUTURE(sentence))))\n",
    "\n",
    "print(\"i am going to eat a kiwi\".find(\"to\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Functions:\n",
    "- **avg**(list)\n",
    "- **get_ngrams**(text, n)\n",
    "- **title_getConcept**(oFileNoExt)\n",
    "- **getPerLabelMax**(tupleList)\n",
    "- **getFinalLabelTupleList**(curSentLabels)\n",
    "- **getFinalLabelDict**(resDict)    #NOT working\n",
    "- **checkPriority**(funcName)\n",
    "- **updatePriority**(ruleFunc, funcName)\n",
    "- **termSeen**(listOfTermsSeen, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    // TODO\\n    When this returns 0, we can update the value for the respective term in titleTermsSeen from 0 to 1 and \\n    we can call CM_4 for each term and it will work only if the term\\'s value is 0, \\n    so for each sentence:\\n        check dictionary, if value is 0, call CM_4\\n        if CM_4 returns a value \"CM\", then update the dictionary for that value with 1\\n        else don\\'t call CM_4 at all\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def avg(listOfItems):\n",
    "    return sum(listOfItems, 0.0) / len(listOfItems)\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# WORKS - takes a file name\n",
    "# USE to find key words from tghe title and find the first occurence of the concepts in the text and label as CM\n",
    "def title_getConcept(oFileNoExt):\n",
    "    \n",
    "    # remove all surrounding stuff like my naming convention etc from the title \n",
    "    mainTitlelist = oFileNoExt.split(\"_\")\n",
    "    maintitle = mainTitlelist[1]\n",
    "    punct = {'_','-','.'}\n",
    "    finaltitle = \"\"\n",
    "    \n",
    "    # extract the final title, i.e. the main part of the title\n",
    "    for word in maintitle.split():\n",
    "        for letter in word:\n",
    "            if letter in punct:\n",
    "                finaltitle += \" \"\n",
    "                pass\n",
    "            else:\n",
    "                finaltitle += letter\n",
    "    \n",
    "    title_keywords = []\n",
    "    for word in finaltitle.split(\" \"):\n",
    "        if word == 'en':\n",
    "             pass\n",
    "        else:\n",
    "            title_keywords.append(word)\n",
    "    \n",
    "    return title_keywords  # returns a list of keywords\n",
    "\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# since more then one rule may return the same label and summing up the results can give incorrect result, we will\n",
    "# only get the highest value per label for the final comparison for label output\n",
    "def getPerLabelMax(tupleList):\n",
    "    maxCD, maxAP, maxEX, maxCM, maxSM = 0,0,0,0,0\n",
    "    uniqueValueList = []\n",
    "    \n",
    "    for item in tupleList:\n",
    "        label = item[0]\n",
    "        score = item[1]\n",
    "        \n",
    "        if label == 'NOLBL': pass\n",
    "        if label == 'CD': \n",
    "            if score > maxCD: maxCD = score\n",
    "        if label == 'CM': \n",
    "            if score > maxCM: maxCM = score\n",
    "        if label == 'AP': \n",
    "            if score > maxAP: maxAP = score\n",
    "        if label == 'SM': \n",
    "            if score > maxSM: maxSM = score\n",
    "        if label == 'EX': \n",
    "            if score > maxEX: maxEX = score\n",
    "                \n",
    "    uniqueValueList.append(tuple((\"CD\", maxCD)))\n",
    "    uniqueValueList.append(tuple((\"CM\", maxCM)))\n",
    "    uniqueValueList.append(tuple((\"AP\", maxAP)))\n",
    "    uniqueValueList.append(tuple((\"SM\", maxSM)))\n",
    "    uniqueValueList.append(tuple((\"EX\", maxEX)))\n",
    "    \n",
    "    return uniqueValueList\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# WORKS \n",
    "# USE at the end to export only one label per sentence - the one with the majority vote\n",
    "# NOTE: equal case is NOT considered, so it may crash\n",
    "def getFinalLabelTupleList(curSentLabels):\n",
    "    uniqueList = getPerLabelMax(curSentLabels)\n",
    "    #print(\"final list before comparison: \", uniqueList)\n",
    "    \n",
    "    maxLabel = \"\"\n",
    "    maxCount = 0\n",
    "    labels = []\n",
    "    \n",
    "    # getting the max value\n",
    "    for item in uniqueList:\n",
    "        label = item[0]\n",
    "        count = item[1]\n",
    "        \n",
    "        if count > maxCount:\n",
    "            maxCount = count\n",
    "            maxLabel = label\n",
    "        elif count == maxCount:\n",
    "            \"\"\" PRIORITIES of LABELS over each other\n",
    "            CM == EX => EX [✔]\n",
    "            CM == SM => SM [✔]\n",
    "            AP == CD => CD [✔]\n",
    "            SM == AP => AP [✔]\n",
    "            \"\"\"\n",
    "            if label == 'CM' and maxLabel == 'EX': pass\n",
    "            elif label == 'EX' and maxLabel == 'CM': maxLabel = label\n",
    "            elif label == 'SM' and maxLabel == 'CM': maxLabel = label\n",
    "            elif label == 'CM' and maxLabel == 'SM': pass\n",
    "            elif label == 'CD' and maxLabel == 'AP': pass\n",
    "            elif label == 'AP' and maxLabel == 'CD': maxLabel = label\n",
    "            elif label == 'AP' and maxLabel == 'AP': maxLabel = label\n",
    "            elif label == 'SM' and maxLabel == 'AP': pass\n",
    "            #elif label == 'SM' and maxLabel == 'CD': maxLabel = label\n",
    "            #elif label == 'CD' and maxLabel == 'SM': pass\n",
    "            #elif label == 'EX' and maxLabel == 'CD': maxLabel = label\n",
    "            #elif label == 'CD' and maxLabel == 'EX': pass\n",
    "                \n",
    "    return maxLabel\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def getFinalLabelDict(resDict):     # gets the list of assigned labels after all rules have been checked\n",
    "    #print(resDict)\n",
    "    EX,AP,CD,CM,SM = 0, 0, 0, 0, 0\n",
    "    \n",
    "    numericalDict = {}\n",
    "    # counting the labels returned from the rules checks\n",
    "    for value in resDict:\n",
    "        if value == \"EX\": numericalDict[\"EX\"] += 1\n",
    "        elif value == \"AP\": numericalDict[\"AP\"] += 1\n",
    "        elif value == \"CD\": numericalDict[\"CD\"] += 1\n",
    "        elif value == \"CM\": numericalDict[\"CM\"] += 1\n",
    "        elif value == \"SM\": numericalDict[\"SM\"] += 1\n",
    "        else: pass   #pass NOLBL items\n",
    "        \n",
    "    maxValueLabel = getKeyWithMaxVal(numericalDict)\n",
    "\n",
    "    return maxValueLabel\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# checks the rule name and since the rules have priority returns int based on that: where higher priority = more points\n",
    "def checkPriority(funcName):\n",
    "    res = 0\n",
    "    if funcName in (\"EX_1\", \"CD_5\", \"SM_1\", \"SM_4\", \"AP_1\", \"AP_2\", \"CM_1\", \"CM_4\"): res = 4    # priority 1 (highest)\n",
    "    if funcName in (\"EX_2\", \"CD_1\", \"SM_6\", \"AP_3\", \"CM_2\"): res = 3    # priority 2\n",
    "    if funcName in (\"CD_4\", \"CD_6\", \"SM_3\", \"CM_3\"): res = 2    # priority 3\n",
    "    if funcName in (\"SM_5\", \"SM_2\", \"CD_3\", \"CD_2\"): res = 1     # priority 4 (lowest)\n",
    "    \n",
    "    return res\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# gets function and sentence, checkswhat the function returns and if the result is not NOLBL,\n",
    "# returns a TUPLE to be added to a dictionary as (KEY-VALUE): e.g. (\"EX_1\", 4)\n",
    "def updatePriority(ruleFunc, funcName):\n",
    "    keyValPair = ()\n",
    "    points = 0\n",
    "    \n",
    "    if not ruleFunc == \"NOLBL\":\n",
    "        points = checkPriority(funcName)\n",
    "        #print(funcName,\" (\",ruleFunc,points,\")\")\n",
    "    \n",
    "    keyValPair = (ruleFunc,points)\n",
    "    return keyValPair    # returns a tuple\n",
    "\n",
    "\n",
    "\n",
    "### -----------------------------\n",
    "# figure out how to detect whether a term from the title has been seen in a file or not.\n",
    "\n",
    "def termSeen(listOfTermsSeen, term):\n",
    "    if term in listOfTermsSeen: return 1\n",
    "    else: return 0\n",
    "   \n",
    "\"\"\"\n",
    "    // TODO\n",
    "    When this returns 0, we can update the value for the respective term in titleTermsSeen from 0 to 1 and \n",
    "    we can call CM_4 for each term and it will work only if the term's value is 0, \n",
    "    so for each sentence:\n",
    "        check dictionary, if value is 0, call CM_4\n",
    "        if CM_4 returns a value \"CM\", then update the dictionary for that value with 1\n",
    "        else don't call CM_4 at all\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Rules implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##### RULES #####\n",
    "\n",
    "### ------------------EX--------------------\n",
    "def EX_1(sent):\n",
    "    nrOfWordsFound = 0\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    wordsEX = {\"example\", \"examples\", \"for instance\", \"assume\", \"sketch\", \"chart\", \"cartoon\", \"suppose\", \"imagine\", \"as\", \"simulation\", \"diagram\", \"might have\", \"may have\", \"want\", \"draw\"}\n",
    "    \n",
    "    for wd in wordsEX:\n",
    "        splitwords = wd.split()\n",
    "        if len(splitwords) == 2:\n",
    "            if wd in bigrams:\n",
    "                nrOfWordsFound += 1\n",
    "        else:\n",
    "            if wd in monograms:\n",
    "                nrOfWordsFound += 1\n",
    "        \n",
    "    if nrOfWordsFound > 0:\n",
    "        return \"EX\"\n",
    "    else: \n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### ------------------EX--------------------\n",
    "    \n",
    "def EX_2(sent):\n",
    "    mainWordNR = 0      # Let's\n",
    "    secondaryWordsNR = 0     \n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    wordsEX2 = {\"try\", \"think\", \"see\", \"pick\", \"take a look\", \"say\"}\n",
    "    \n",
    "    if (\"let's\" or \"let 's\") in bigrams: mainWordNR += 1\n",
    "    \n",
    "    for wd in wordsEX2:\n",
    "        splitwords = wd.split()\n",
    "        if len(splitwords) == 3:\n",
    "            if wd in trigrams:\n",
    "                secondaryWordsNR += 1\n",
    "        if wd in monograms:\n",
    "                secondaryWordsNR += 1\n",
    "        \n",
    "    #print(,\" \", \" \",)\n",
    "    #print(mainWordNR,\" \",secondaryWordsNR)\n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0:\n",
    "        return \"EX\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### -------------------CD-------------------\n",
    "\n",
    "def CD_1(sent):\n",
    "    mainWordNR = 0      # Let's     # main word looking for in conjunction with one or more of the secondary words\n",
    "    secondaryWordsNR = 0 \n",
    "    negWordsNR = 0      # words that must NOT occur for the label to apply, i.e. this should stay at ZERO\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    secondaryWords = {\"look at\", \"make\", \"put\", \"do\", \"start\", \"prove\", \"back\", \"try\", \"just\", \"be\", \"take\", \"bring\"}\n",
    "    negWords = {\"example\", \"examples\", \"diagram\", \"assume\", \"imagine\", \"suppose\"}\n",
    "    \n",
    "    if (\"let's\" or \"let 's\") in bigrams: mainWordNR += 1\n",
    "    \n",
    "    for wd in secondaryWords:\n",
    "        splitwords = wd.split()\n",
    "        if len(splitwords) == 2:\n",
    "            if wd in bigrams:\n",
    "                secondaryWordsNR += 1\n",
    "        else:\n",
    "            if wd in monograms:\n",
    "                secondaryWordsNR += 1\n",
    "    \n",
    "    for wd in negWords:\n",
    "        if wd in monograms:\n",
    "                negWordsNR += 1\n",
    "    \n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0 and negWordsNR == 0:\n",
    "        return \"CD\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CD_2(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if (\"in other words\" or \"basically\") in monograms and not (\"should\" or \"must\") in monograms or not \"have to\" in bigrams and checkPRESENT(sent) >= 50:\n",
    "        return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def CD_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if monograms[0] == \"so\" and (\"it 's\" or \"it's\") or (\"i 'm\" or \"i'm\") in bigrams: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def CD_4(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    if \"so this is\" in trigrams or \"actually\" in monograms and not ('example' or 'summary' or 'next' or 'last') in monograms: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------   \n",
    "\n",
    "def CD_5(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if (\"mean\" or \"given\" or \"define\") in monograms and checkPRESENT(sent) >= 50 or \"going to\" in bigrams: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "    \n",
    "    \n",
    "def CD_6(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"what if\" in bigrams and not (\"example\" or \"instance\") in monograms: return \"CD\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "    \n",
    "### --------------------------------------\n",
    "def SM_1(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if (\"let's\" or \"let 's\") in bigrams and \"summarize\" or \"recap\" in monograms: return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_2(sent):\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if \"in other words\" in trigrams and (checkFUTURE(sent) or checkPAST(sent)) >= 40: return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if ((\"this week\" or \"this lesson\") in bigrams or \"today\" in monograms) and (checkPRESENT(sent) or checkFUTURE(sent)) >= 50: \n",
    "        return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_4(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    wordsSM = {'later' , 'next time' , 'last time' , 'summary' , 'summarize' , \n",
    "             'here is' , 'here are' , 'discuss' , 'next', 'recap'}\n",
    "    \n",
    "    for wd in wordsSM:\n",
    "        splitwords = wd.split()\n",
    "        if len(splitwords) == 2:\n",
    "            if wd in bigrams: return \"SM\"\n",
    "        elif wd in monograms: return \"SM\"\n",
    "        else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def SM_5(sent, lineNR, NrOfLines):\n",
    "    if lineNR < 15 or lineNR > NrOfLines - 15 and checkPAST(sent) > 40: return \"SM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "    \n",
    "def SM_6(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    wordsSM6 = {\"look\", \"see\", \"be\", \"think\", \"explain\"} \n",
    "    \n",
    "    if \"going to\" in bigrams:\n",
    "        for wd in wordsSM6:\n",
    "            if wd in monograms and checkPRESENT(sent) < 30 and (checkFUTURE(sent) or checkPAST(sent)) > 40 : return \"SM\"\n",
    "            else: return \"NOLBL\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def AP_1(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if \"in other words\" in trigrams and (\"should\" or \"would\" or \"could\") in monograms: return \"AP\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def AP_2(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    wordsAP2 = {'encourage' , 'step' , 'first' , 'finally' , 'second' , 'should' ,\n",
    "             'could' , 'would' , 'best practice', 'need to', 'good idea', 'homework', 'you can', 'make sure'}\n",
    "    \n",
    "    res = \"\"\n",
    "    for wd in wordsAP2:\n",
    "        splitwords = wd.split()\n",
    "        if len(splitwords) == 2 and wd in bigrams: res = \"AP\"\n",
    "        elif len(splitwords) == 1 and wd in monograms: res = \"AP\"\n",
    "        else: \n",
    "            if not res == \"AP\": res = \"NOLBL\"\n",
    "    \n",
    "    if res == \"\": return \"NOLBL\"  \n",
    "    else: return res\n",
    "   \n",
    "    ### --------------------------------------\n",
    "\n",
    "def AP_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    \n",
    "    if \"if\" in monograms and (\"use\" or \"can\" or \"should\", \"could\" or \"want\") in monograms: return \"AP\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CM_1(sent, origLbl):\n",
    "    #mainConcepts = title_getConcept(oFileNoExt)\n",
    "    monograms = get_ngrams(sent, 1)    \n",
    "    \n",
    "    if \"called\" in monograms and origLbl == \"CM\": return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CM_2(sent, oFileNoExt):\n",
    "    mainConcepts = title_getConcept(oFileNoExt)\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"what is\" in bigrams:\n",
    "        for wd in mainConcepts:\n",
    "            if wd in monograms: return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "    \n",
    "### --------------------------------------\n",
    "\n",
    "def CM_3(sent):\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if (\"theory\" or \"theorem\" or \"algorithm\" or \"method\") in monograms or (\"let's\" or \"let 's\") in bigrams and \"use\" in monograms: return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CM_4(sent, termsSeenList, term):\n",
    "    seen = -1\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    \n",
    "    if term in sent and not originalLabel == \"EX\": \n",
    "        if term not in termsSeenList:\n",
    "            termsSeenList.append(term)\n",
    "            seen = 1\n",
    "    else: seen = 0\n",
    "\n",
    "    #print(term,\"\\t\",seen)\n",
    "    if seen == 0: return \"CM\"\n",
    "    else: return \"NOLBL\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main program logic, called from the file traverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------- STUPID TEST EXAMPLE ------------------------------------\n",
    "testSent = \"this week,in week nine,we're still doing applications of the derivative,but it's not so much word problems anymore\"\n",
    "#res = EX_2(testSent)\n",
    "#print(res)\n",
    "# ----------- disregard the above test ---------\n",
    "\n",
    "ignore_dict = ['inaudible','OMITTED','NUMBER','sound','music','laughter','yeah','blank_audio']\n",
    "\n",
    "text = \"\"\"EX|so here's a example we want to run through \n",
    "EX|here,have some principle inertias that are NUMBER,NUMBER,and NUMBER \n",
    "CD|now,just by the numbers,a little bit unfortunate,the intermediate one if NUMBER beneath the max and is NUMBER above the min,so it's plus and minus NUMBER \n",
    "CD|this is why when we look at these wheels,the critical wheel speeds,where things go stable unstable,it happens to be symmetric \n",
    "CD|but,generally these inertias aren't split evenly like that \n",
    "CD|so generally those critical wheel speeds are not necessarily symmetric,all right \n",
    "EX|this example will have them symmetric \n",
    "EX|the wheel inertia is ten kilograms,just to make a lot of math easy \n",
    "EX|spacecraft is to spin about NUMBER rpm,about b1,so that's our omega e1 that you have \n",
    "EX|and so now,we're trying to figure out how fast do we have to spin this wheel at least to guarantee linear stability of the system \n",
    "CD|so you want both bracketed terms to be either positive or negative \n",
    "CD|now,those two bracketed terms could be positive,that's the set \n",
    "CD|that's where we have i1 must be greater than these other two terms here \n",
    "CD|in that case,the left bracket and the right bracket are both positive,and then we have a stable system \n",
    "CD|set,actually just simply reverses this,greater than becomes a less than \n",
    "CD|this is the condition that makes the first and second bracket go negative and that is also a stable configuration,all right \n",
    "CD|so now,we want to look at what is the range of spin rates \n",
    "CD|so the way like to think of this is just look at the spin axis \n",
    "CD|we have zero \n",
    "CD|and then,can go positive spin rate and negative spin rate \n",
    "CD|and we're going to put in inequality condition to start to go it has to be at least to the right of this mark \n",
    "CD|and it has to be at least to the left of this mark,so what's left \n",
    "CD|the union or intersections of these different areas,that's how solve this stuff at least \n",
    "CD|so if we're looking at this,in this problem if the wheel speed is zero,is this system going to be stable \n",
    "NL|brian \n",
    "CD|they'd take a look at the inequalities again \n",
    "NL|no,no \n",
    "SM|if the wheel speed is zero,there's no longer a dual spin,there's simply a single rigid body \n",
    "CD|and we're spinning about b1 dual spin\n",
    "CD|about which inertia axis are we spinning then \n",
    "CD|it's not going to be stable \n",
    "CD|all right,because now we have NUMBER \n",
    "CD|so b1 is an axis of intermediate inertia in this particular example,right \n",
    "CD|so we know up front,zero can not be included in my solution space \n",
    "CD|have to,if this is going to work,have to have something positive or negative \n",
    "CD|and we'll see how that drops out \n",
    "CD|so if we do this,here's the two inequality conditions \n",
    "EX|the second one,for example here,this is one set \n",
    "CD|you can say okay,i2 minus,you can bring this term over,that gives you i2 minus i1,divide by this \n",
    "CD|iw,i2 minus i1,gives you minus NUMBER divided by NUMBER \n",
    "CD|no,it gets you minus NUMBER divided by ten,gives you minus five \n",
    "CD|so by bringing this over,omega hat has to be bigger than minus five \n",
    "CD|if'm just going to draw that here,so let's say,here's the minus five part,that means,what did say,less than \n",
    "CD|all right,thinking too many steps ahead,okay \n",
    "CD|no,it's greater than \n",
    "CD|okay,so that means we would have to be somewhere greater than minus five \n",
    "CD|but that's only one condition \n",
    "CD|now the other condition,here'm just going through the math a little bit more steps,right \n",
    "CD|i'm bringing this condition and bringing this part over to the left hand side \n",
    "CD|i1 back over to the right hand side \n",
    "CD|divide by iw2,here it's i3 minus i1 \n",
    "CD|you plug in the NUMBERs,which you do something similar in the homework,so'm just not going to \n",
    "CD|you know how to do this algebra \n",
    "EX|then omega hat as to be greater than plus five \n",
    "EX|so if we look at this now and say,okay,there's also a plus five,and here's the other point \n",
    "CD|this is the domain that makes the second bracket go to positive \n",
    "CM|what is the actual solution space then for stability \n",
    "NL|inaudible great \n",
    "CD|so that union between both,right \n",
    "CD|because we need both of them to be positive \n",
    "CD|that's going to be here,all right \n",
    "CD|outs greater than \n",
    "CD|is that the only domain,though \n",
    "NL|carlo,what do you think \n",
    "CD|will be less than minus five \n",
    "CD|less than minus five,now talk me through why that it's also domain correct \n",
    "CD|because the second set of equations doesn't give us that answer \n",
    "CD|yep,the first set always has greater than\n",
    "CD|you've identified two points where each bracketed term flips signs \n",
    "CD|once you've found those,you've really found,instead of greater than,it's all less than \n",
    "CD|so instead of always being to the right of,it's always going to be all to the left of \n",
    "CD|and,again,the same unions \n",
    "CD|so if you did that to make both brackets negative,if pick a different color,let me make blue for negative right \n",
    "CD|then you would have to be here or here and the union of that is just going to be here \n",
    "CD|so you can see as expected the origin is not included in the solution space \n",
    "CD|because as brian was pointing out,this is a single rigid body spinning about axis remediated inertia \n",
    "CD|we know it's not stable,but if the wheel spins up enough then at some point,you'll notice with this wheel spinning,you're spinning oars is an oblate body,you're spinning by axis of max inertia \n",
    "CD|that wheel by itself,the way it's defined \n",
    "CD|it's always going to be stable,right \n",
    "CD|so if that wheel spins fast enough,one way to think of this is the stability of that wheel is going to overcome the instability of the spacecraft spin and that's how we stabilize it \n",
    "CD|but you have to get to some minimum amount where it's just equal and then you have to be greater than that,hopefully,quite a bit greater that it will increase your stiffness and your response time as well,right \n",
    "CD|so either you can go positive or you can go negative and that will get you there \n",
    "CD|now,so good \n",
    "EX|so in this case,have,this is one of the wheel's speed,the critical speeds,you could use or it has to be bigger than that really,that's what you have to write \n",
    "CD|not equal,but probably bigger than that \n",
    "CD|or it could also be less than minus NUMBER in this case \n",
    "CD|there's two sets,both brackets positive,both brackets negative \n",
    "CD|so if'm giving you some problem like this though,here am saying,we are spinning about major axis \n",
    "CD|so in this case,b1 is axis of maximum inertia and without a rotor spin,so the origin,we would have to be stable,which is what am showing \n",
    "CD|if we are spinning up more about the spacecraft by definition,already has a positive spin,about b1 \n",
    "CD|that is how we derived to all this stuff \n",
    "CD|that's how picked b1 \n",
    "CD|and now,the wheel is also spinning about that \n",
    "CD|if the spacecraft is max inertia stable,the wheel is max inertia stable and they're both spinning about positive b1,it's always just going to be stable \n",
    "CD|more and more stable \n",
    "CD|if you start to spin it the opposite direction though,even though the space craft by itself without that wheel will stable,it has the speed momentum that helps stabilize it \n",
    "CD|what happens with the wheel,if it's in the opposite direction,it's going to start to pull out the total momentum gets reduced,actually \n",
    "CD|if this is spinning at NUMBER rpm positive and the wheel is NUMBER rpm negative,you're pulling out momentum \n",
    "CD|and in fact,the momentum perspective,at some point it acts like the separatrix,it acts like the unstable intermediate axis motion and you will drive a system unstable \n",
    "CD|but the good news is no matter what axis,principle axis,you want to spin nominally,we can always find a wheel speed that will stabilize it \n",
    "CD|laugh the bad news is no matter how stable the spacecraft was before you touched it,once you touch it,you have the capability to drive it unstable \n",
    "AP|so make sure you have the right real speeds otherwise your sponsor will not be happy \n",
    "AP|at some point again,if you go fast enough,then the wheel momentum will dominate and it's very,very stable \n",
    "CD|whatever the spacecraft's doing is almost noise \n",
    "CM|so with all duo spinners to extremes infinity real speeds always stable \n",
    "CD|but in between,there is a region,a finite zone,that is unstable \n",
    "CD|so for an intermediate axis speed,which we looked at \n",
    "CD|and in our problem,we had plus and minus five,that was just because of the inertias,it doesn't have to be symmetric \n",
    "CD|if you see something like this that excludes the origin,right away could say,this must be spinning about an axis of intermediate inertia with a dual-spinner \n",
    "CD|just from the solution space \n",
    "CD|and,if you have an axis of a least inertia,you're spinning about this region that you shouldn't be spinning about is actually in the positive spin direction and just all comes out of the mathematics then \n",
    "NL|jordan \n",
    "CD|sorry,missed how can it be stable if you're spinning it less than negative five \n",
    "CD|both bracketed terms \n",
    "CD|because the two brackets have to either be positive,and that was the argument for the black hash lines \n",
    "CD|but we found the two critical points \n",
    "CD|if we also less than those critical points,then both brackets both become negative \n",
    "NL|okay \n",
    "CD|and that's why this is also a possible answer,out of the mathematics \n",
    "NL|thanks \n",
    "NL|yeah,good \n",
    "NL|so anyway,but this gives you now a quick solution space \n",
    "AP|you can do these easy homework with this yourself,to come up with it \n",
    "SM|but just remember the extremums are always included \n",
    "CD|the origin is only included for a max and min inertia case and then you kind of look at the pattern \n",
    "CD|if see the pattern,know right away what type of spin we're doing \"\"\"\n",
    "\n",
    "\n",
    "text2 = \"\"\"SM|welcome to my last additional video,which is about doing things many,many times \n",
    "SM|this is something called a loop \n",
    "SM|before we get to loops,let's talk about why we want to do things many,many times \n",
    "EX|simplest reason is that we might want to draw lots of things that are very similar \n",
    "EX|a bunch of concentric circles,a set of lines forming a grid,these are the examples i'm going to use \n",
    "EX|but also,things get much,much more complex than that \n",
    "EX|i mean,you might have a game with lots of enemies you want to draw \n",
    "EX|or in the case of this week's example,we're creating complex shapes made out of many similar simple shapes \n",
    "EX|and,in a sense,a grid or concentric circles is a good starting point \n",
    "EX|here,this is a sketch that gives us,draws a bunch of lines across the screen  \"\"\"\n",
    "\n",
    "curSentLabels = []\n",
    "sentences = text2.split(\"\\n\")\n",
    "originalLabel = \"\"\n",
    "res = {}\n",
    "lineNR = 0\n",
    "totalLines = len(text2)\n",
    "prevLabeledSent = \"\"\n",
    "prevLabeledSentLABEL = \"\"\n",
    "finalLabelSent = \"\"\n",
    "\n",
    "titleTermList = title_getConcept(\"04_9-example-dual-spinner-stability.en_labels.txt\")\n",
    "#print(titleTermList)\n",
    "termsSeenList = []   # will hold values for main terms\n",
    "\n",
    "for sent in sentences:\n",
    "    lineNR += 1\n",
    "    splitSent = sent.split(\"|\")\n",
    "    originalLabel = splitSent[0]\n",
    "    sent = splitSent[1]\n",
    "    termInCurSent = \"\"\n",
    "    \n",
    "    if originalLabel == \"NL\":\n",
    "        pass\n",
    "    else:\n",
    "        curSentLabels.append(tuple(updatePriority(SM_1(sent),\"SM_1\")))\n",
    "        curSentLabels.append(tuple(updatePriority(SM_2(sent),\"SM_2\")))\n",
    "        curSentLabels.append(tuple(updatePriority(SM_3(sent),\"SM_3\")))\n",
    "        curSentLabels.append(tuple(updatePriority(SM_4(sent),\"SM_4\")))\n",
    "        curSentLabels.append(tuple(updatePriority(SM_5(sent, lineNR, totalLines),\"SM_5\")))\n",
    "        curSentLabels.append(tuple(updatePriority(SM_6(sent),\"SM_6\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CD_1(sent),\"CD_1\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CD_2(sent),\"CD_2\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CD_3(sent),\"CD_3\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CD_4(sent),\"CD_4\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CD_5(sent),\"CD_5\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CD_6(sent),\"CD_1\")))\n",
    "        curSentLabels.append(tuple(updatePriority(EX_1(sent),\"EX_1\")))\n",
    "        curSentLabels.append(tuple(updatePriority(EX_2(sent),\"EX_2\")))\n",
    "        curSentLabels.append(tuple(updatePriority(AP_1(sent),\"AP_1\")))\n",
    "        curSentLabels.append(tuple(updatePriority(AP_2(sent),\"AP_2\")))\n",
    "        curSentLabels.append(tuple(updatePriority(AP_3(sent),\"AP_3\")))\n",
    "        for term in titleTermList:\n",
    "            curSentLabels.append(tuple(updatePriority(CM_4(sent, termsSeenList, term),\"CM_4\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CM_1(sent,originalLabel),\"CM_1\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CM_2(sent, \"04_9-example-dual-spinner-stability.en_labels.txt\"),\"CM_2\")))\n",
    "        curSentLabels.append(tuple(updatePriority(CM_3(sent),\"CM_3\")))\n",
    "        #print(curSentLabels)\n",
    "        \n",
    "        if getFinalLabelTupleList(curSentLabels) in (\"SM\", \"AP\", \"EX\", \"CD\", \"CM\"):\n",
    "            prevLabeledSentLABEL = finalLabelSent     # before assigning the new value, we still have the old one\n",
    "            finalLabelSent = getFinalLabelTupleList(curSentLabels)\n",
    "            prevLabeledSent = sent\n",
    "        else:\n",
    "            finalLabelSent = prevLabeledSentLABEL\n",
    "            prevLabeledSent = sent\n",
    "                       \n",
    "        print(\"Original: \", originalLabel, \" | Assigned: \", finalLabelSent)\n",
    "        print(finalLabelSent, \"\\t\", sent, \"\\n\")\n",
    "        \n",
    "        termInCurSent = \"\"\n",
    "        curSentLabels.clear()\n",
    "    #print(termsSeenList)\n",
    "\n",
    "# label_sentences(takesAFile)\n",
    "# get_ngrams(takesASentence)\n",
    "# ruleX(takesASentence)\n",
    "# main(takesInputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# figure out how to detect whether a term has been seen in a file or not. (meaning a term from the title)\n",
    "\n",
    "def termSeen(listOfTermsSeen, term):\n",
    "    \n",
    "    if term in listOfTermsSeen: return 1\n",
    "    else: return 0\n",
    "    \n",
    "\"\"\"\n",
    "    // TODO\n",
    "    When this returns 0, we can update the value for the respective term in titleTermsSeen from 0 to 1 and \n",
    "    we can call CM_4 for each term and it will work only if the term's value is 0, \n",
    "    so for each sentence:\n",
    "        check dictionary, if value is 0, call CM_4\n",
    "        if CM_4 returns a value \"CM\", then update the dictionary for that value with 1\n",
    "        else don't call CM_4 at all\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAIN METHOD (LOGIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(iFile, oPathNoExt):    # main application with all logic following the pseudocode\n",
    "    correctLabels = 0\n",
    "    correctCM = 0\n",
    "    correctCD = 0\n",
    "    correctAP = 0\n",
    "    correctSM = 0\n",
    "    correctEX = 0\n",
    "    #print(oPathNoExt)\n",
    "\n",
    "    print(\"[LABELLING file: ] \" + os.path.basename(iFile.name))\n",
    "    #print(\"============================================================================\\n\")\n",
    "    baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "    OFName = baseName + \".en_AutoRuleLabels.txt\"\n",
    "\n",
    "    sentences = iFile.read().lower().split(\"\\n\")\n",
    "    \n",
    "    \n",
    "    ### ---------- Local variables - reset per file ------------------------\n",
    "    \n",
    "    originalLabel = \"\"\n",
    "    res = {}\n",
    "    lineNR = 0\n",
    "    totalLines = len(sentences)\n",
    "    prevLabeledSent = \"\"\n",
    "    prevLabeledSentLABEL = \"\"\n",
    "    curSentLabels = []          # All the labels assigned to the current sentence (to get majority vote from it later)\n",
    "    finalLabelSent = \"\"\n",
    "    \n",
    "    titleTermList = title_getConcept(oPathNoExt)\n",
    "    termsSeenList = []   # will hold values for main terms\n",
    "    #accuracy = (countCorLabels/totalSentences) * 100\n",
    "    #accuracy = (correctLabels/totalLines) * 100\n",
    "    #print(\"Accuracy: {0:.2f} %\".format(accuracy))\n",
    "    \n",
    "    #for term in titleTermList:\n",
    "        #titleTermsSeen[term] = 0\n",
    "        \n",
    "    with open(OFName, \"w\") as oFile:    # opening the output file to write in the same place where the original file is\n",
    "        oFile.write(\"Original|Assigned|Sentence\\n\".upper())\n",
    "        for sent in sentences:\n",
    "            if len(sent) == 0:   #empty line\n",
    "                totalLines -= 1   # don't count these sentences as part of the labelling and directly assign \"NL\" to them\n",
    "                continue\n",
    "            else:\n",
    "                lineNR += 1\n",
    "                splitSent = sent.split(\"|\")\n",
    "                originalLabel = splitSent[0].upper()\n",
    "                sent = splitSent[1]\n",
    "\n",
    "                if originalLabel == \"NL\":\n",
    "                    oFile.write(originalLabel+\"|\"+\"NL\"+\"|\"+sent+\"\\n\")\n",
    "                    totalLines -= 1   # don't count these sentences as part of the labelling and directly assign \"NL\" to them\n",
    "                    continue\n",
    "                else:\n",
    "                    curSentLabels.append(tuple(updatePriority(SM_1(sent),\"SM_1\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(SM_2(sent),\"SM_2\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(SM_3(sent),\"SM_3\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(SM_4(sent),\"SM_4\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(SM_5(sent, lineNR, totalLines),\"SM_5\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(SM_6(sent),\"SM_6\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CD_1(sent),\"CD_1\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CD_2(sent),\"CD_2\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CD_3(sent),\"CD_3\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CD_4(sent),\"CD_4\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CD_5(sent),\"CD_5\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CD_6(sent),\"CD_1\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(EX_1(sent),\"EX_1\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(EX_2(sent),\"EX_2\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(AP_1(sent),\"AP_1\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(AP_2(sent),\"AP_2\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(AP_3(sent),\"AP_3\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CM_1(sent, \"04_9-example-dual-spinner-stability.en_labels.txt\"),\"CM_1\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CM_2(sent, \"04_9-example-dual-spinner-stability.en_labels.txt\"),\"CM_2\")))\n",
    "                    curSentLabels.append(tuple(updatePriority(CM_3(sent),\"CM_3\")))\n",
    "                    #for term in titleTermList:\n",
    "                     #   curSentLabels.append(tuple(updatePriority(CM_4(sent, termsSeenList, term),\"CM_4\")))\n",
    "                    #curSentLabels.append(tuple(updatePriority(CM_4(sent, \"04_9-example-dual-spinner-stability.en_labels.txt\", 0),\"CM_4\")))\n",
    "                    #print(curSentLabels)\n",
    "\n",
    "                    if getFinalLabelTupleList(curSentLabels) in (\"SM\", \"AP\", \"EX\", \"CD\", \"CM\"):\n",
    "                        prevLabeledSentLABEL = finalLabelSent     # before assigning the new value, we still have the old one\n",
    "                        finalLabelSent = getFinalLabelTupleList(curSentLabels)\n",
    "                        prevLabeledSent = sent\n",
    "                    else:\n",
    "                        finalLabelSent = prevLabeledSentLABEL\n",
    "                        prevLabeledSent = sent\n",
    "\n",
    "                    if originalLabel == finalLabelSent:\n",
    "                        correctLabels += 1\n",
    "                        if originalLabel == \"CM\" and finalLabelSent == \"CM\": correctCM += 1\n",
    "                        if originalLabel == \"CD\" and finalLabelSent == \"CD\": correctCD += 1\n",
    "                        if originalLabel == \"AP\" and finalLabelSent == \"AP\": correctAP += 1\n",
    "                        if originalLabel == \"SM\" and finalLabelSent == \"SM\": correctSM += 1\n",
    "                        if originalLabel == \"EX\" and finalLabelSent == \"EX\": correctEX += 1\n",
    "\n",
    "            oFile.write(originalLabel+\"|\"+finalLabelSent+\"|\"+sent+\"\\n\")\n",
    "            #print(\"Original: \", originalLabel, \" | Assigned: \", finalLabelSent, \" | Label of prev. sent: \", prevLabeledSentLABEL)\n",
    "            #print(finalLabelSent, \"\\t\", sent, \"\\n\")\n",
    "\n",
    "            curSentLabels.clear()\n",
    "                \n",
    "    lineNR = lineNR - 2 #removing a line for the header and because at the end of every file there's one empty line\n",
    "    \n",
    "    #accuracyCM,accuracyCD,accuracyEX,accuracySM,accuracyAP = 0,0,0,0,0\n",
    "    accuracy = (correctLabels/totalLines) * 100\n",
    "    \n",
    "    try:\n",
    "        accuracyCM = (correctCM / correctLabels) * 100\n",
    "        accuracyCD = (correctCD / correctLabels) * 100\n",
    "        accuracyEX = (correctEX / correctLabels) * 100\n",
    "        accuracySM = (correctSM / correctLabels) * 100\n",
    "        accuracyAP = (correctAP / correctLabels) * 100\n",
    "        print(\"Accuracy: {0:.2f} %\".format(accuracy))\n",
    "        #print(\"Accuracy: {0:.2f} % | CM = {1:.2f} % | CD = {2:.2f} % | AP = {3:.2f} % | EX = {4:.2f} % | SM = {5:.2f} % |\".format(accuracy, accuracyCM, accuracyCD, accuracyAP, accuracyEX, accuracySM))\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    #print(\"Accuracy: {0:.2f} %\".format(accuracy))\n",
    "    \n",
    "    finalTuple = (accuracy,accuracyCM,accuracyCD,accuracyAP,accuracyEX,accuracySM)\n",
    "    #print(finalTuple)\n",
    "    return finalTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---RUN THIS PART--- (going over all files and calling the main program on each of them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### With multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f862e82ab8bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                     \u001b[1;31m# Running the main method and assigning the tuple it returns to the local file tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                     \u001b[0mcurFileRes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileExtRemoved\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                     \u001b[1;31m# ----------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "#path = r\"C:\\Users\\a.dimitrova\\Desktop\\Course data Thesis\\INTENT MINING\"     # Toshiba path\n",
    "#path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\INTENT MINING\"    # HP path SINGLE FILE\n",
    "path = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\Intent Mining ALL files\"  #HP path ALL files\n",
    "# return [0]accuracy [1]accuracyCM [2]accuracyCD [3]accuracyEX [4]accuracySM [5]accuracyAP\n",
    "\n",
    "counter = 0\n",
    "accuracyAllFiles,accuracyAllCM,accuracyAllCD,accuracyAllEX,accuracyAllSM,accuracyAllAP = [],[],[],[],[],[]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if filePath.endswith(\".txt\"):\n",
    "                if filePath.endswith(\"_AutoRuleLabels.txt\"): \n",
    "                    pass\n",
    "                elif filePath.endswith(\".txt\"):\n",
    "                    curFileRes = ()\n",
    "                    counter += 1\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "\n",
    "                    # Running the main method and assigning the tuple it returns to the local file tuple\n",
    "                    curFileRes = main(curFile, fileExtRemoved) \n",
    "                    # ----------------------------------------------------------------------------------\n",
    "                    \n",
    "                    curFAccuracy,curFCM,curFCD,curFEX,curFSM,curFAP = curFileRes  # assigning parts of tuple to variables\n",
    "                    accuracyAllFiles.append(curFAccuracy)\n",
    "                    accuracyAllCM.append(curFCM)\n",
    "                    accuracyAllCD.append(curFCD)\n",
    "                    accuracyAllEX.append(curFEX)\n",
    "                    accuracyAllSM.append(curFSM)\n",
    "                    accuracyAllAP.append(curFAP)                    \n",
    "                    \n",
    "                    curFile.close()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(\"Average accuracy: {0:.2f} %\".format(avg(accuracyAllFiles))) \n",
    "print(\"Average accuracy per label: CM {0:.2f} % | CD {1:.2f} % | EX {2:.2f} % | SM {3:.2f} % | AP {4:.2f} % \".format(avg(accuracyAllCM),avg(accuracyAllCD),avg(accuracyAllEX),avg(accuracyAllSM),avg(accuracyAllAP)))\n",
    "\n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time: {0:.2f} min\".format((end - start)/60))\n",
    "\n",
    "\"\"\"\n",
    "# 62.05 %\n",
    "# updating priority 62.08 %\n",
    "\n",
    "OUTPUT:\n",
    "Average accuracy: 62.08 %\n",
    "Execution time: 1.35 min\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LABELLING file: ] allMerged.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d4754682779e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Running the main method and assigning the tuple it returns to the local file tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcurFileRes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileExtRemoved\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# ----------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-39ae9e75a8ba>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(iFile, oPathNoExt)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mfinalLabelSent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mtitleTermList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtitle_getConcept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moPathNoExt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mtermsSeenList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m# will hold values for main terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m#accuracy = (countCorLabels/totalSentences) * 100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a04122e51912>\u001b[0m in \u001b[0;36mtitle_getConcept\u001b[1;34m(oFileNoExt)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# remove all surrounding stuff like my naming convention etc from the title\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mmainTitlelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moFileNoExt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmaintitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmainTitlelist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mpunct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mfinaltitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "filePath = r\"C:\\Users\\ani\\Desktop\\Course data Thesis\\INTENT MINING\\allMerged.txt\"\n",
    "\n",
    "curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "\n",
    "# Running the main method and assigning the tuple it returns to the local file tuple\n",
    "curFileRes = main(curFile, fileExtRemoved) \n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "curFAccuracy,curFCM,curFCD,curFEX,curFSM,curFAP = curFileRes  # assigning parts of tuple to variables\n",
    "\n",
    "curFile.close()\n",
    "\n",
    "end = time.time()\n",
    "print(\"\\nExecution time: {0:.2f} min\".format((end - start)/60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
