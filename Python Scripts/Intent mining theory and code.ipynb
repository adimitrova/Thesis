{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### How do researchers deal with it:\n",
    "- [Word embeddings Wiki](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "- [Gensim Python library](https://en.wikipedia.org/wiki/Gensim)\n",
    "- [Inference Rules Wiki](https://en.wikipedia.org/wiki/Rule_of_inference)\n",
    "\n",
    "### LSTM: \n",
    "- [Long-Short term memory (LSTM)](https://www.datacamp.com/community/tutorials/lstm-python-stock-market#lstm)\n",
    "- [Learn via example](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)\n",
    "\n",
    "### Literature:\n",
    "- [Intent extraction from social media texts using sequential segmentation and deep learning models](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8119461) uses CRFs and Bi-LSTM for intent extraction from texts from social media in 2 categories - Cosmetics and Tourism. Look into these algos\n",
    "    - Citation: \n",
    "`@INPROCEEDINGS{8119461, \n",
    "author={T. L. Luong and M. S. Cao and D. T. Le and X. H. Phan}, \n",
    "booktitle={2017 9th International Conference on Knowledge and Systems Engineering (KSE)}, \n",
    "title={Intent extraction from social media texts using sequential segmentation and deep learning models}, \n",
    "year={2017}, \n",
    "pages={215-220}, \n",
    "doi={10.1109/KSE.2017.8119461}, \n",
    "month={Oct},}`\n",
    "\n",
    "\n",
    "- In [Semantic Indexing for Recorded Educational Lecture Videos](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1598977) they extracted scripts from videos with timestamps on each word and cluster them in order to allow for finding of the exact position of a particular thing in the video. They also use a retrieval method to find “example”, “explanation”, “overview”, “repetition”, “exercise” for a particular word or topic word. \n",
    "    - Citation: `@INPROCEEDINGS{1598977, \n",
    "author={S. Repp and M. Meinel}, \n",
    "booktitle={Fourth Annual IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOMW'06)}, \n",
    "title={Semantic indexing for recorded educational lecture videos}, \n",
    "year={2006}, \n",
    "pages={5 pp.-245}, \n",
    "month={March},}`\n",
    "\n",
    "\n",
    "- In [Olex: Effective Rule Learning for Text Categorization](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4641927) Sees the problem as a text classification task and applied Inference Rules onto it. Not particularly for intent mining, but for different categories, similar to what I have. The inference rules are of the form: \\begin{equation}If \\space T_1 \\space or \\space \\dots \\space T_n \\space occurs \\space in \\space document \\space d,\\space and \\space none \\space of \\space T_{n+1} \\dots T_{n+m} \\space occurs \\space in \\space d, \\space then \\space classify \\space d \\space under \\space category \\space C \\end{equation}  This includes `one` positive literal and `0+` negative literals and temrs are `n-grams`\n",
    "\n",
    "    - Citation `@ARTICLE{4641927, \n",
    "author={P. Rullo and V. L. Policicchio and C. Cumbo and S. Iiritano}, \n",
    "journal={IEEE Transactions on Knowledge and Data Engineering}, \n",
    "title={Olex: Effective Rule Learning for Text Categorization}, \n",
    "year={2009}, \n",
    "volume={21}, \n",
    "number={8}, \n",
    "pages={1118-1132}, \n",
    "doi={10.1109/TKDE.2008.206}, \n",
    "ISSN={1041-4347}, \n",
    "month={Aug},}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Rules method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "### General Rules\n",
    "1. If sentence has no label, proceed with label search.\n",
    "2. If no label can be assigned, assign the last applied labeled from a previous sentence\n",
    "\n",
    "-----------------------------------\n",
    "### ALL RULES\n",
    "- EX_1 <<< `example` || OR `for instance` || `assume` || `suppose` || `imagine` || `as` || `simulation` || `diagram` [✔] \n",
    "- EX_2 <<< `Let's` && try || think || see || pick || take a look || say .. [✔]\n",
    "- CD_1 <<< `Let's` && look at || make || put || do || start || prove || evaluate || back || try || just && NOT `example` || `assume` || `suppose` || `imagine` || `diagram` [✔]\n",
    "- CD_2 <<< `in other words` && present tense\n",
    "- CD_3 <<< `so` && `it's` || `i'm`\n",
    "- CD_4 <<< `so this is` || `actually` && NOT `example` || `summary` || `next` || `last`\n",
    "- SM_1 <<< `Let's` && summarize\n",
    "- SM_2 <<< `in other words` && past tense\n",
    "- SM_3 <<< `later` || `next time` || `last time` || `summary` || `summarize`\n",
    "- SM_4 <<< if (lineNr < 10 `OR` lineNr > fileLinesNr - 10) `&&` (past tense) =>> (within the first or last 10 lines + past tense)\n",
    "- SM_5 <<< `going to` && `look` || `see` || `be` || `think`\n",
    "- AP_1 <<< `in other words` && `should` || `could` || `would`\n",
    "- AP_2 <<< `encourage` || `step` || `first` || `finally` || `second` || `should` || `could` || `would` || `best practice(s)` || `need to`\n",
    "- AP_3 <<< `if` && `use` || `can` || `should` || `could` || `want`\n",
    "- CM_1 <<< `called` && concept\n",
    "- CM_2 <<< `what is` .. && concept\n",
    "- CM_3 <<< `theorem` || `algorithm` || `method` || `let's use`\n",
    "- CM_4 <<< `let's` && `use`\n",
    "- CM_5 <<< first occurence of the terms in the title of the file\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "### Logical expressions (copy/paste in thesis later - LATEX style)\n",
    "#### ♦ EXAMPLE\n",
    "1. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"let's\" \\in d \\space) \\space \\land (\"try\" \\in d \\space \\lor \"see\" \\in d \\space \\lor \"think\" \\in d \\space \\lor \"pick\" \\in d \\space \\lor \"say\" \\in d) \\end{equation} \n",
    "\n",
    "2. \\begin{equation} d \\leftarrow EX \\space, if\\space (\"example\" \\in d \\space) \\lor (\"for \\space instance\" \\in d) \\space \\lor (\"suppose\" \\in d) \\space \\lor (\"assume\" \\in d) \\space \\lor (\"includes\" \\in d) \\space \\lor (\"imagine\" \\in d) \\space \\end{equation} \n",
    "\n",
    "Latex Formula Formatter: https://www.codecogs.com/eqnedit.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "\n",
    "`Disregard all sentences that have NL label, totally ignore, then: [✔]\n",
    "    if sentence has no label:\n",
    "        for line in text:\n",
    "            if lineNR < 10 OR lineNR > nrOfLines-10:\n",
    "                for word in line:\n",
    "                    if (60%+ of the words on the line are in PAST TENSE):\n",
    "                        go over the SM rules  (append res to curSentLabels)\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in PRESENT TENSE):\n",
    "                        go over the CD rules  (append res to curSentLabels)\n",
    "                    if (60%+ of the words on the line are in FUTURE TENSE):\n",
    "                        go over the CM rules  (append res to curSentLabels)\n",
    "            if lineNR > 10 AND lineNR < nrOfLines-10:\n",
    "                go over the EX rules  (append res to curSentLabels)\n",
    "                go over the CD rules  (append res to curSentLabels)\n",
    "                go over the AP rules  (append res to curSentLabels)\n",
    "                go over the CM rules  (append res to curSentLabels)\n",
    "        Count the labels with a special method for this: [✔]\n",
    "            if all rules fail to assign a label, i.e. if all labels return count 0:\n",
    "                search for the last labeled sentence:\n",
    "                    assign its label to the current sentence`                                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the tense of the verbs in the sentence\n",
    "\n",
    "- [All POS Tags](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "NLTK PPOS TAGS for verbs:\n",
    "\n",
    "- VBD\t\tverb, past tense\t\t\t\t\t`took`\n",
    "- VBN\t\tverb, past participle\t\t\t\t`taken`\n",
    "- VB\t\tverb, base form\t\t\t\t\t\t`take`\n",
    "- VBG\t\tverb, gerund/present participle\t\t`taking`\n",
    "- VBP\t\tverb, sing. present, non-3d\t\t\t`take`\n",
    "- VBZ\t\tverb, 3rd person sing. present\t\t`takes`\n",
    "\n",
    "\n",
    "**Simply counting verbs isn't enough, artificial boost is added if certain types of verbs are present so that they\n",
    "form a specific English tense. All listed below:**\n",
    "\n",
    "- **Past perfect**: `VBD`(had) + `VBN`(been) ------- BUT NO VBG(-ing/gerund)\n",
    "- **Past continuous tense**: `VBD`(was/were) + `VBG`(-ing/gerund)\n",
    "- **Past perfect continuous**: `VBD`(had) + `VBN`(been) + `VBG`(-ing/gerund)\n",
    "- **PRESENT perfect**: `VBP`(have) + `VBN`(been) ------- BUT NO VBG(-ing/gerund)\n",
    "- **PRESENT perfect continuous**: `VBP`(have) + `VBN`(been) + `VBG`(-ing/gerund)\n",
    "- **PRESENT continuous**: `VBP`(is/are) + `VBG`(-ing/gerund)\n",
    "- **Future continuous**: `MD`(WILL) + `VBG`(-ing/gerund)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Past tense chance: 50.00 %\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"\"\"I have lived here since 1987.\"\"\"\n",
    "\n",
    "puncDict = [\",\",\".\",\";\",\"-\",\"_\",\"`\",\"'\",\"?\",\"!\",\":\"]\n",
    "\n",
    "def checkPastTense(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs, pastTense, VBGgerund, VBNpp, MDmodalverb, VBDPast = 0, 0, 0, 0, 0, 0    \n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "#            print(tag)\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBD\": \n",
    "                pastTense += 1\n",
    "                VBDPast += 1\n",
    "            elif str(tag[1]) == \"VBN\": \n",
    "                pastTense += 1\n",
    "                VBNpp += 1\n",
    "            elif str(tag[1]) == \"VBG\":\n",
    "                pastTense += 1\n",
    "                VBGgerund += 1\n",
    "        \n",
    "        if str(tag[1]) == \"MD\":\n",
    "            MDmodalverb += 1\n",
    "    \n",
    "    # artifial boost over the past tense verbs +1 if there is at least one from all gerund, past participle \n",
    "    # and modal verb that defines past continuous tense\n",
    "    if VBDPast > 0 and VBNpp > 0 and VBGgerund == 0: pastTense += 2                   #Past perfect \n",
    "    elif VBDPast > 0 and VBNpp == 0 and VBGgerund > 0: pastTense += 2                 #Past continuous tense \n",
    "    elif VBDPast > 0 and VBNpp > 0 and VBGgerund > 0: pastTense += 3                  #Past perfect continuous\n",
    "    \n",
    "    # big / small = 100 / x\n",
    "    #calculate verbs over words\n",
    "    ratio_verbs = len(text) / verbs\n",
    "    perc_verbs= (100 / ratio_verbs) \n",
    "    \n",
    "    #calculate past tense verbs over all verbs\n",
    "    ratio_PastVerbs = verbs / pastTense\n",
    "    perc_PastVerbsOverVerbs = (100 / ratio_PastVerbs)\n",
    "    \n",
    "    #calculate past tense verbs over all words\n",
    "    ratio_PastVerbsOverWords = len(text) / pastTense\n",
    "    perc_VerbsPastTenseOverWords = 100 / ratio_PastVerbsOverWords\n",
    "    \n",
    "    if perc_PastVerbsOverVerbs >= 50:\n",
    "        return perc_PastVerbsOverVerbs\n",
    "    else:\n",
    "        return \"NPAST\"\n",
    "\n",
    "\n",
    "# -------------- TO DELETE -----------------\n",
    "#checkPastTense(sentence[0],\",\",sentence[1])\n",
    "#print(\"Verbs: {0:.2f} %\".format(checkPastTense(sentence)[0]))\n",
    "#print(\"Past tense over verbs: {0:.2f} %\".format(checkPastTense(sentence)[1]))\n",
    "#print(\"Past tense over words: {0:.2f} %\".format(checkPastTense(sentence)[2]))\n",
    "print(\"\\nPast tense chance: {0:.2f} %\".format(checkPastTense(sentence)))\n",
    "\n",
    "#print(checkPastTense(sentence))\n",
    "\n",
    "def checkPresentTense(sentence):\n",
    "    text = nltk.word_tokenize(sentence)\n",
    "    verbs = 0\n",
    "    presTense = 0\n",
    "    \n",
    "    textPOS = nltk.pos_tag(text)\n",
    "    for tag in textPOS:\n",
    "        if str(tag[1]).startswith(\"V\"):\n",
    "            verbs += 1\n",
    "            if str(tag[1]) == \"VBD\" or str(tag[1]) == \"VBN\":\n",
    "                presTense += 1\n",
    "    \n",
    "    # big / small = 100 / x\n",
    "    #calculate verbs over words\n",
    "    \n",
    "    #calculate past tense verbs over all verbs\n",
    "    ratio_PresVerbs = verbs / presTense\n",
    "    perc_PresVerbsOverVerbs = (100 / ratio_PastVerbs)\n",
    "    \n",
    "    if perc_PresVerbsOverVerbs >= 50:\n",
    "        return perc_PresVerbsOverVerbs\n",
    "    else:\n",
    "        return \"NPRES\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Inference Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RULES #####\n",
    "\n",
    "### ------------------EX--------------------\n",
    "def EX_1(sent):\n",
    "    nrOfWordsFound = 0\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"example\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"for instance\" in bigrams: nrOfWordsFound += 1\n",
    "    elif \"assume\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"suppose\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"imagine\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"as\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"simulation\" in monograms: nrOfWordsFound += 1\n",
    "    elif \"diagram\" in monograms: nrOfWordsFound += 1\n",
    "        \n",
    "    if nrOfWordsFound > 0:\n",
    "        return \"EX\"\n",
    "    else: \n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### ------------------EX--------------------\n",
    "    \n",
    "def EX_2(sent):\n",
    "    mainWordNR = 0      # Let's\n",
    "    secondaryWordsNR = 0     \n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    trigrams = get_ngrams(sent, 3)\n",
    "    \n",
    "    if \"let's\" in bigrams or \"let 's\" in bigrams: mainWordNR += 1\n",
    "    \n",
    "    if \"try\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"think\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"see\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"pick\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"take a look\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"say\" in monograms: secondaryWordsNR += 1\n",
    "        \n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0:\n",
    "        return \"EX\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "    \n",
    "### -------------------CD-------------------\n",
    "\n",
    "def CD_1(sent):\n",
    "    mainWordNR = 0      # Let's     # main word looking for in conjunction with one or more of the secondary words\n",
    "    secondaryWordsNR = 0 \n",
    "    negWords = 0      # words that must NOT occur for the label to apply, i.e. this should stay at ZERO\n",
    "    monograms = get_ngrams(sent, 1)\n",
    "    bigrams = get_ngrams(sent, 2)\n",
    "    \n",
    "    if \"let's\" in bigrams or \"let 's\" in bigrams: mainWordNR += 1\n",
    "    \n",
    "    if \"look at\" in bigrams: secondaryWordsNR += 1\n",
    "    elif \"make\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"put\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"do\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"start\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"prove\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"back\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"try\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"just\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"be\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"take\" in monograms: secondaryWordsNR += 1\n",
    "    elif \"bring\" in monograms: secondaryWordsNR += 1\n",
    "\n",
    "    if \"example\" in monograms: negWords += 1\n",
    "    if \"diagram\" in monograms: negWords += 1\n",
    "    if \"assume\" in monograms: negWords += 1\n",
    "    if \"imagine\" in monograms: negWords += 1\n",
    "    if \"suppose\" in monograms: negWords += 1\n",
    "        \n",
    "    if secondaryWordsNR > 0 and mainWordNR > 0 and negWords == 0:\n",
    "        return \"CD\"\n",
    "    else:\n",
    "        return \"NOLBL\"\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "def CD_2(sent):\n",
    "    \n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "### --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final label count method + title keywords exttract to search for CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testSent = \"so let's just get rid of the ones we don't need, for example\"\n",
    "res = EX_dictSearch(testSent)\n",
    "# ----------- disregard the above test ---------\n",
    "\n",
    "# WORKS - takes a file name\n",
    "# USE to find key words from tghe title and find the first occurence of the concepts in the text and label as CM\n",
    "def title_getConcept(oFileNoExt):\n",
    "    \n",
    "    # remove all surrounding stuff like my naming convention etc from the title \n",
    "    mainTitlelist = oFileNoExt.split(\"_\")\n",
    "    maintitle = mainTitlelist[1]\n",
    "    punct = {'_','-','.'}\n",
    "    finaltitle = \"\"\n",
    "    \n",
    "    # extract the final title, i.e. the main part of the title\n",
    "    for word in maintitle.split():\n",
    "        for letter in word:\n",
    "            if letter in punct:\n",
    "                finaltitle += \" \"\n",
    "                pass\n",
    "            else:\n",
    "                finaltitle += letter\n",
    "    \n",
    "    title_keywords = []\n",
    "    for word in finaltitle.split(\" \"):\n",
    "        if word == 'en':\n",
    "             pass\n",
    "        else:\n",
    "            title_keywords.append(word)\n",
    "    \n",
    "    return title_keywords  # returns a list of keywords\n",
    "\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# WORKS \n",
    "# USE at the end to export only one label per sentence - the one with the majority vote\n",
    "# NOTE: equal case is NOT considered, so it may crash\n",
    "def getFinalLabel(curSentLabels):     # gets the list of assigned labels after all rules have been checked\n",
    "    EX,AP,CD,CM,SM, maxCount = 0, 0, 0, 0, 0, 0\n",
    "    labels = []\n",
    "    maxLabel = \"\"\n",
    "    \n",
    "    # counting the labels returned from the rules checks\n",
    "    for item in curSentLabels:\n",
    "        if item == \"EX\": EX += 1\n",
    "        elif item == \"AP\": AP += 1\n",
    "        elif item == \"CD\": CD += 1\n",
    "        elif item == \"CM\": CM += 1\n",
    "        elif item == \"SM\": SM += 1\n",
    "        else: pass   #pass NOLBL items\n",
    "\n",
    "    # adding the labels into a list of items to make it easy to get the max value\n",
    "    labels.append(\"EX,\"+str(EX))\n",
    "    labels.append(\"AP,\"+str(AP))\n",
    "    labels.append(\"CD,\"+str(CD))\n",
    "    labels.append(\"CM,\"+str(CM))\n",
    "    labels.append(\"SM,\"+str(SM))\n",
    "\n",
    "    # getting the max value \n",
    "    for item in labels:\n",
    "        parts = item.split(\",\")\n",
    "        label = parts[0]\n",
    "        count = int(parts[1])\n",
    "        if count > 0:\n",
    "            if count > maxCount:\n",
    "                maxCount = count\n",
    "                maxLabel = label\n",
    "\n",
    "    return maxLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-45b87129613f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcorrectLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtotalSentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrectLabels\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotalSentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m### ---------- Local variables - reset per file ------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "### ---------- Global variables ----------------------------------------\n",
    "correctLabels = 0\n",
    "totalSentences = 0\n",
    "accuracy = correctLabels / totalSentences\n",
    "\n",
    "### ---------- Local variables - reset per file ------------------------\n",
    "lineNR = 0\n",
    "totalLines = 0\n",
    "curSentLabels = []          # All the labels assigned to the current sentence (to get majority vote from it later)\n",
    "\n",
    "originalLabel = \"\"          # label from the sentence\n",
    "finalMajorityLabel = \"\"          # label assigned after the rules application\n",
    "finalMajorityLabelUsers = \"\"     # label assigned by other users (majority vote)\n",
    "##### ----------------------------------------------------------------------------------------------------- #####\n",
    "\n",
    "ignore_dict = ['inaudible','OMITTED','NUMBER','sound','music','laughter','yeah','blank_audio']\n",
    "\n",
    "curSentLabels.append(EX_dictSearch(testSent))\n",
    "curSentLabels.append(EX_lets(testSent))\n",
    "curSentLabels.append(CD_lets(testSent))\n",
    "\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# label_sentences(takesAFile)\n",
    "# get_ngrams(takesASentence)\n",
    "# ruleX(takesASentence)\n",
    "# main(takesInputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def main(iFile, oPathNoExt):    # main application with all logic following the pseudocode=\n",
    "        print(\"[LABELLING file: ] \" + os.path.basename(iFile.name))\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_AutoRuleLabels.txt\"\n",
    "        \n",
    "        sentences = iFile.read().split(\"\\n\")\n",
    "        sent_processed = []\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:    # opening the output file to write in the same place where the original file is\n",
    "            for sent, lineNR in sentences:\n",
    "                originalLabel = sent.split(\"|\")[0]\n",
    "                sent = sent.split(\"|\")[1]\n",
    "                \n",
    "                if originalLabel == \"NL\":\n",
    "                    pass\n",
    "                else: \n",
    "                    if finalMajorityLabel == \"\":\n",
    "                        if lineNR < 10 or lineNR > len(sentences)-10:   # assigning threshold of first or last 10 sentences\n",
    "                            if (checkPastTense(sent) > 60)\n",
    "            # TODO: reset finalMajorityLabel back to \"\"   finalMajorityLabel = \"\"\n",
    "            # TODO: oFile.write(finalMajorityLabel,\"|\",\"sent\")   #write the final label and sentence in the output file\n",
    "        \n",
    "        oFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RUN THIS PART - going over all files and calling the main program on each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
