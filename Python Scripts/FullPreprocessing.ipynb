{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data pre-processing \n",
    "Data is collected from 11 MOOCs in the field of Computer Science, robotics, mathematics and physics and are processed total of 563 TXT files. \n",
    "\n",
    "#### Preprocessing steps: \n",
    "\n",
    "- sentence by sentence split\n",
    "- lower case\n",
    "- noise removal \n",
    "    - SOME punctuation STAYS \n",
    "        - point, single space and comma\n",
    "    - SOME stopwords stay \n",
    "        - between,we,i,in,here,that,you,it,that,this,there,few,if,so,to,a,an,is,until,while\n",
    "    - mention removal\n",
    "- word normalization\n",
    "    - tokenization \n",
    "    - lemmatization \n",
    "    - stemming \n",
    "- word standardization\n",
    "    - regex\n",
    "\n",
    "#### Overall logic:\n",
    "1. traverse recursively all folder and files\n",
    "2. When a file is found, save it's name into a file list\n",
    "3. For each file in the file list, apply all the **preprocessing steps** and save it as a new file with \"\\_PREPROCESSED\" added at the end in the same folder\n",
    "    3.1. Changed: Now I keep deparate file after each preprocessing step as I may need certain file for some algorithms. \n",
    "\n",
    "#### Next steps (In the Data Analysis file) :  \n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?\n",
    "8. Try finding n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from pathlib import Path\n",
    "from beautifultable import BeautifulTable\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS', 'FW']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'MD']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS', 'WRB']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        # returns a\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        # returns n\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        # returns r\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        # returns v\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ METHODS  --------------------------------------------------------\n",
    "\n",
    "# Create a set of all allowed characters.\n",
    "# {...} is the syntax for a set literal in Python.\n",
    "allowedPunct = {\",\", \".\", \" \"}.union(string.ascii_lowercase)\n",
    "\n",
    "# Use full for BoF and others. \n",
    "# Use part in order to keep some data important if we need to match tuples from the text based on natual speach\n",
    "path_swAni_full = \"/media/sf_Shared_Folder/stopwordsALL.txt\"\n",
    "path_swAni_part = \"/media/sf_Shared_Folder/stopwords.txt\"\n",
    "\n",
    "swFile = open(path_swAni_full, 'r', encoding = \"ISO-8859-1\")\n",
    "stopWords = swFile.read().split()\n",
    "#print(stopWords)\n",
    "\n",
    "#stopWords = stopwords.words('english')\n",
    "#print(stopWords)\n",
    "\n",
    "# function #1    \n",
    "def splitSentences(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_Raw.txt\"):\n",
    "        print(\"[Splitting sentences on file:] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_sent.txt\"\n",
    "        \n",
    "        # the WITH keyword makes it possible to omit the file.close() function at the end to close the file\n",
    "        with open(OFName,\"w\") as oFile:\n",
    "            print(oFile.name)\n",
    "            text = iFile.read()    \n",
    "            # initial text is full of new lines, so we have to remove them first. \n",
    "            text = text.replace(\"\\n\", \" \")    \n",
    "            # have the sentences split and print them one by one\n",
    "            sentences = sent_tokenize(text)\n",
    "            # write to the output file\n",
    "            for sent in sentences:\n",
    "                oFile.write(sent+\"\\n\")\n",
    "        oFile.close()\n",
    "    \n",
    "\"\"\"\n",
    "# -------------- discarded -----------------\n",
    "# function #2 (optional)\n",
    "# only leaves comma, space, dot\n",
    "# read input file, save to output file\n",
    "def removePunctuation(iFile, oPathNoExt):\n",
    "    #text = re.sub(\"[\\[\\].;@#$%^&*:()][^,]\", \" \", file)\n",
    "    if filePath.endswith(\"_sent.txt\"):\n",
    "        print(\"[Punctuation removal on file: ] \" + iFile.name)\n",
    "        with open(oPathNoExt + \"_noPunct.txt\",\"w\") as oFile:\n",
    "            oFile.write(\"\".join([letter for letter in iFile if letter in allowed]))\n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "\"\"\"\n",
    "      \n",
    "# function #3\n",
    "def sentTokenize(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sent.txt\"):\n",
    "        print(\"[Tokenizing file: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokens.txt\"\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            tokens = iFile.read()\n",
    "            tokens = word_tokenize(tokens)\n",
    "            for tok in tokens:\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\", \"=\", \"*\", \"/\", \"+\", \n",
    "                                  \"-\", \"%\", \"#\", \"?\", \"(\", \")\", \"-\", \"_\", \n",
    "                                  \";\", \":\", \"'\", \"\\\"\", \"^\", \"`\"}.union(string.ascii_lowercase)\n",
    "                if tok in UNallowedPunct:\n",
    "                    pass\n",
    "                else:\n",
    "                    oFile.write(tok+\"\\n\")\n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# function #4 - partOfSpeechTag Tagging\n",
    "def rmStopWords(iFile, oPathNoExt):\n",
    "    \n",
    "    if not iFile.name.endswith(\"_tokens.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Removing StopWords: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_noStopWordsALL.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:     \n",
    "            final = []\n",
    "            \n",
    "            for tok in tokenList:\n",
    "                if tok in stopWords:\n",
    "                    # if token is a stop word, don't save in the output (skip)\n",
    "                    pass\n",
    "                else:\n",
    "                    final.append(tok)\n",
    "                    oFile.write(tok+\"\\n\")\n",
    "            oFile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function #5 - partOfSpeechTag Tagging\n",
    "def POStag(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_noStopWordsALL.txt\"):\n",
    "        print(\"[Part of Speech tagging: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokPOStag.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:            \n",
    "            taggedTok = pos_tag(tokenList)\n",
    "            \n",
    "            for tok in taggedTok:\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\"}.union(string.ascii_lowercase)\n",
    "                if tok[0] in UNallowedPunct:\n",
    "                    # if token is a punctuation symbol, don't save in the output\n",
    "                    pass\n",
    "                else:\n",
    "                    #print(tok[0],',',tok[1])    # output: now , RB\n",
    "                    #print(tok)                  # output: ('now', 'RB')\n",
    "                    outpLine = tok[0]+\",\"+tok[1]\n",
    "                    oFile.write(outpLine+\"\\n\")\n",
    "                    #oFile.write(str(tok)+\"\\n\")\n",
    "                    \n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Outputs data as a tuple (Word,WordNetPOSTag,Stem)\n",
    "\n",
    "POS TAG: read line by line, split each line by (\",\"), write line[0] to the file with comma, following it\n",
    "STEM: take line[1] and compare POS tag, return the appropriate POS tag as per Wordnet, i.e. \n",
    "r for adverb\n",
    "a for adjective\n",
    "n for noun\n",
    "v for verb\n",
    "\"\"\"\n",
    "def stem(iFile, oPathNoExt):\n",
    "    stemmer = LancasterStemmer()\n",
    "    \n",
    "    #if not iFile.name.endswith(\"_tokPOStag.txt\"):\n",
    "    if not iFile.name.endswith(\"NounVerbAdjectiveAdverb.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Stemming: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_stemmedbyPOS.txt\"\n",
    "        \n",
    "        content = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:   \n",
    "           \n",
    "            for line in content:\n",
    "                curline = line.split(\",\")    #list with 2 elements\n",
    "                word = curline[0]\n",
    "                posTag = curline[1]\n",
    "                \n",
    "                stem = stemmer.stem(word)\n",
    "                \n",
    "                if(penn_to_wn(posTag) == \"a\"):\n",
    "                    oFile.write(word+\",a,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"n\"):\n",
    "                    oFile.write(word+\",n,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"r\"):\n",
    "                    oFile.write(word+\",r,\"+stem+\"\\n\")\n",
    "                elif(penn_to_wn(posTag) == \"v\"):\n",
    "                    oFile.write(word+\",v,\"+stem+\"\\n\")\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            oFile.close() \n",
    "            \n",
    "# Requires the output from stem() and more specifically the POS tag in order to know what \n",
    "# word to make out of the stem. If not specific, it assumes NOUN\n",
    "def lemmatize(iFile, oPathNoExt):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    if not iFile.name.endswith(\"en_stemmedbyPOS.txt\"):\n",
    "    #if not iFile.name.endswith(\"NounVerbAdjectiveAdverb.en_stemmedbyPOS.txt\"):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"[Lemmatizing: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_lemmatized.txt\"\n",
    "        \n",
    "        content = iFile.read().split()\n",
    "        \n",
    "        #print(\"Word | Stem | Lemma | LemmaPOS\")\n",
    "        #print(\"------------------------------\")\n",
    "        with open(OFName, \"w\") as oFile:    \n",
    "            for line in content:\n",
    "                curline = line.split(\",\")    #list with 2 elements\n",
    "                word = curline[0]\n",
    "                posTag = curline[1]\n",
    "                stem = curline[2]\n",
    "                \n",
    "                lemma = nltk.stem.WordNetLemmatizer().lemmatize(stem)\n",
    "                lemmaPOS = nltk.stem.WordNetLemmatizer().lemmatize(stem, 'v')\n",
    "                #print(word+\" | \"+stem+\" | \"+lemma+\" | \"+lemmaPOS)\n",
    "                oFile.write(lemmaPOS+\"\\n\")\n",
    "            \n",
    "            oFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemmatizing: ] /media/sf_Shared_Folder/TEST/one file/01_what-is-the-definition-of-derivative.en_stemmedbyPOS.txt\n",
      "[Lemmatizing: ] /media/sf_Shared_Folder/TEST/one file/04_how-does-wiggling-x-affect-f-x.en_stemmedbyPOS.txt\n",
      "[Lemmatizing: ] /media/sf_Shared_Folder/TEST/one file/NounVerbAdjectiveAdverb.en_stemmedbyPOS.txt\n",
      "\n",
      "Total number of 21 TXT files found.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------- PROGRAM  -------------------------------------------------------\n",
    "\n",
    "path = \"/media/sf_Shared_Folder/TEST/one file\"   # TEST DATA PATH\n",
    "#path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"   # REAL DATA PATH\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            else: \n",
    "                # else create a new txt file with \"_PROC.txt\" to store the output and process the original file\n",
    "                try: \n",
    "                    counter += 1\n",
    "                    #fileName = print(os.path.abspath(filePath))\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    #outpFile = open(os.path.abspath)\n",
    "\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    #outpFileBase = open(FileExtRemoved + \"_PROC.txt\",\"w\")\n",
    "\n",
    "                    \"\"\"\n",
    "                    call each processing function here and pass it the file\n",
    "                    First argument: current input file\n",
    "                    Second argument: path without extension of the current file\n",
    "                    the path will be used to save the output file with the same name and same location\n",
    "                    but with different file ending based on what the fuunction did\n",
    "                    \n",
    "                    Functions take input files ending on:\n",
    "                    splitSentences()\t>>\t_Raw.txt\n",
    "                    sentTokenize()\t\t>>\t_sent.txt\n",
    "                    POStag()\t\t\t>>\t_noStopWordsALL.txt\n",
    "                    rmStopWords()\t\t>>\t_tokens.txt\n",
    "                    stem()\t\t\t\t>>\t_tokPOStag.txt\n",
    "                    lemmatize()\t\t\t>>\t_stemmed.txt\n",
    "                    \"\"\"  \n",
    "                    #removePunctuation(curFile, fileExtRemoved)\n",
    "                    #splitSentences(curFile, fileExtRemoved)\n",
    "                    #sentTokenize(curFile, fileExtRemoved)\n",
    "                    #rmStopWords(curFile, fileExtRemoved)\n",
    "                    #POStag(curFile, fileExtRemoved)\n",
    "                    #stem(curFile, fileExtRemoved)\n",
    "                    lemmatize(curFile, fileExtRemoved)\n",
    "                    \n",
    "                    if curFile.name.endswith(\"_Raw.txt\"):\n",
    "                        counter += 1\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "        \n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next:\n",
    "- get the _tokPOStag.txt files\n",
    "- read and save every line as key-value pair or list of 2 elements\n",
    "- compare the second element of each line, i.e the POS, match with the wordnet pos tags\n",
    "- process stemming correctly\n",
    "- [lemmatize](https://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word)\n",
    "- [find n-grams](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "- Perform [Bag of Words](https://pythonprogramminglanguage.com/bag-of-words/)\n",
    "\n",
    "---------- offtopic ---------\n",
    "\n",
    "- [Puthon theory](http://xahlee.info/python/python_basics.html)\n",
    "- [Text classification](https://gallery.azure.ai/Experiment/Text-Classification-Step-2-of-5-text-preprocessing-2)\n",
    "- [Preprocessing steps](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook Shortcuts:\n",
    "- **A**: Insert cell --**ABOVE**--\n",
    "- **B**: Insert cell --**BELOW**--\n",
    "- **M**: Change cell to --**MARKDOWN**--\n",
    "- **Y**: Change cell to --**CODE**--\n",
    "    \n",
    "- **Shift + Tab** will show you the Docstring (**documentation**) for the the object you have just typed in a code cell  you can keep pressing this short cut to cycle through a few modes of documentation.\n",
    "- **Ctrl + Shift + -** will split the current cell into two from where your cursor is.\n",
    "- **Esc + O** Toggle cell output.\n",
    "- **Esc + F** Find and replace on your code but not the outputs.\n",
    "\n",
    "[MORE SHORTCUTS](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "\n",
    "### ----------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
