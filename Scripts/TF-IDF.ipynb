{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to supply several documents and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01-understanding-research-data/01_research-data-defined.en.txt\n",
    "document1 = tb(\"\"\"To begin this course,\n",
    "we need to first discuss some basics. The most fundamental question for\n",
    "us is what are data? While this may seem like\n",
    "a straightforward question, numerous organizations have tackled it\n",
    "resulting in a range of definitions. It's important to note that data\n",
    "are different for various disciplines and different context. By the end of this lesson, you will be introduced to multiple\n",
    "types of data in an array of contexts. Data come in many forms from numeric and textual data to biological samples and\n",
    "physical collections. You will also be able to make\n",
    "the distinction between research data and other associated researcher materials. Some of which may be\n",
    "required alongside the data to understand the data themselves. And finally, and this is probably one of the most\n",
    "important concepts in the entire course. You will understand data in the context\n",
    "of the research data life cycle. To understand data management, is to\n",
    "understand data in all of its constructs. From project planning, through the\n",
    "collection of data to archiving that data as a research output after\n",
    "the project period has ended. First, let's take a look at\n",
    "how others have defined data. The National Institutes of Health, or NIH,\n",
    "define data as “recorded factual material commonly accepted in\n",
    "the scientific community as necessary to validate\n",
    "research findings.” Key concepts here include recorded and accessible data that others may look\n",
    "at to validate a studies findings. The National Science Foundation or\n",
    "NSF considers data to be something, determined by the community of interest\n",
    "through the process of peer review and project management. NSF’s definition points out the data are\n",
    "essential to the research community and not just to individual researchers or\n",
    "research teams. NSF also offers some examples of data that make the rather abstract\n",
    "definition easier to understand. As we know, examples are always useful. The NSF list includes data, publications, samples, physical collections,\n",
    "software, and models. It's interesting that NSF includes\n",
    "data as an example of data, but the point here is there are many types\n",
    "of data beyond quantitative datasets. The National Endowment for\n",
    "the Humanities, or NEH, also offers examples of data that\n",
    "includes citations, software code, algorithms, digital tools,\n",
    "documentation, databases, geospatial coordinates,\n",
    "reports and articles. It is interesting to note that NEH\n",
    "includes a wider array of data types than we see from NSF or NIH. Any of these data types NEH\n",
    "defines as humanities data, or “materials generated or collected during\n",
    "the course of conducting research.” So what are some key concepts\n",
    "in these definitions? For NIH and NSF, the research\n",
    "community establishes definitions of data that involve validity and\n",
    "presume data sharing among the community. When you look at the examples provided\n",
    "by any of these organizations, you can see that data come in\n",
    "quite a significant variety of forms, almost to the point of being nebulous. What is important to understand here is that data are products of research\n",
    "that are heterogeneous across and contextualized within\n",
    "the academic disciplines. So to reiterate,\n",
    "data should be valid, shared, and are heterogeneous, and contextualized\n",
    "within research communities.\"\"\")\n",
    "\n",
    "# 01-understanding-research-data/02_types-of-data-and-metadata.en.txt\n",
    "document2 = tb(\"\"\"Let's look at different types of data. Probably what most of us think about\n",
    "when we use the term data is numeric or tabular data, what researchers might\n",
    "refer to as quantitative data. And then there are other types. Samples such as DNA or\n",
    "blood samples, physical collections, including plant specimens,\n",
    "software programs and code, databases, algorithms, model, and geodatabases. Obviously, these are not all\n",
    "the types of data out there. Can you think of other examples? What types have you encountered? Please share them in the forum. There are other research products that\n",
    "need to be considered alongside data in order for the data to be meaningful. These include questionnaires, code books,\n",
    "and descriptions of methodologies. Jillian Wallis, Elizabeth Rolando, and Christine Borgman describe\n",
    "this as background data. Background data provides contextual\n",
    "information important in the analysis of the primary foreground\n",
    "data collected for analysis. For example, what the temperature was\n",
    "at the time a sensor reading is taken may be critical to understanding\n",
    "variances in data a sensor collected. And then there are research products\n",
    "built on the data that are essential for secondary analysis or meta-analysis and\n",
    "for content dissemination. These include reports, conference posters,\n",
    "articles, white papers, and books. And we should include websites and blogs. Another essential concept in\n",
    "data management is metadata. This is a term that you will hear\n",
    "me refer to throughout this course. Metadata is often defined\n",
    "as data about data. This, of course, is a basic and circular\n",
    "definition that may not help us too much. More specifically, metadata is\n",
    "structured information that describes, explains, locates, or\n",
    "otherwise represents something else. For our interest in this course,\n",
    "that something else is, of course, our target research data. Metadata makes it easier to retrieve,\n",
    "use or manage an information source. Data are nothing without metadata,\n",
    "especially digital data. One cannot search for, identify, or\n",
    "interpret data without robust metadata. For each dataset, we need to know\n",
    "at minimum, who created the data, when the data were created or\n",
    "published, and a title or descriptive name\n",
    "used to refer to the dataset. In this digital landscape,\n",
    "we also need a unique and persistent identifier for\n",
    "the data so that we can locate it, even if the data are moved to\n",
    "a different location on the web. Beyond these metadata elements,\n",
    "we can increase our ability to find and identify data if we have\n",
    "information about these, along with the other 11 metadata elements that make\n",
    "up the Dublin Core Metadata Element Set. Dublin Core allows us to find and\n",
    "identify data. But to be able to interpret and\n",
    "use data as they were intended, we also need to know quite a bit of\n",
    "other information about the data. The Data Documentation Initiative, or DDI, developed a metadata scheme\n",
    "specifically for this purpose. Along with basic Dublin Core Elements, DDI prescribes additional\n",
    "metadata elements that provide specific information\n",
    "about data collection processes, variable-level descriptions,\n",
    "and methodologies.\"\"\")\n",
    "\n",
    "# 01-understanding-research-data/03_research-data-lifecycle.en.txt\n",
    "document3 = tb(\"\"\"When thinking about and doing data\n",
    "management, it is critical to understand data in terms of their lifecycle and\n",
    "the research project’s lifecycle. For project planning to archiving, proper data management happens\n",
    "throughout the research lifecycle. Each stage of the lifecycle\n",
    "produces specific data products and requires a variety of considerations,\n",
    "responsibilities, and activities. There are numerous lifecycle\n",
    "models from simple to complex. Let's look at a few. Here is the research lifecycle\n",
    "model from the UK Data Archive. Once data are created, which is\n",
    "represented in the circle at 12 o'clock, the data undergo subsequent stages,\n",
    "processing, analyzing, preserving, providing access to,\n",
    "and re-using the data. The University of Virginia library created\n",
    "this research data life cycle model, which I think directly aligns\n",
    "to common research practice. Here is the more complex and comprehensive data lifecycle model\n",
    "developed by the Digital Curation Center. With data at the center of the graphic,\n",
    "you can see the various data curation activities as you\n",
    "move to the outer rings. This model brings together data,\n",
    "researchers and curators. Now that you have been\n",
    "introduced to data and understand that data management refers to\n",
    "activities throughout the data lifecycle, please take a moment to introduce yourself\n",
    "on the forums and tell us about your data experiences, or maybe even questions\n",
    "you have about data or the course. We have also provided some\n",
    "additional readings for you to explore if you're\n",
    "interested in learning more. And make sure you check out\n",
    "the resources for this module.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally the **MAIN()** method: traversing through the documents and output the terms and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 terms in document 1 | 01_research-data-defined.en.txt\n",
      "\t| TERM: To | TF-IDF: 0.01542\n",
      "\t| TERM: NSF | TF-IDF: 0.0054\n",
      "\t| TERM: As | TF-IDF: 0.00463\n",
      "\t| TERM: You | TF-IDF: 0.00385\n",
      "\t| TERM: community | TF-IDF: 0.00385\n",
      "\t| TERM: It | TF-IDF: 0.00308\n",
      "\t| TERM: Some | TF-IDF: 0.00308\n",
      "\t| TERM: includes | TF-IDF: 0.00308\n",
      "\t| TERM: NIH | TF-IDF: 0.00231\n",
      "\t| TERM: National | TF-IDF: 0.00231\n",
      "\t| TERM: By | TF-IDF: 0.00231\n",
      "\t| TERM: concepts | TF-IDF: 0.00231\n",
      "\t| TERM: here | TF-IDF: 0.00231\n",
      "\t| TERM: NEH | TF-IDF: 0.00231\n",
      "\t| TERM: From | TF-IDF: 0.00231\n",
      "\n",
      "Top 15 terms in document 2 | 02_types-of-data-and-metadata.en.txt\n",
      "\t| TERM: Metadata | TF-IDF: 0.00901\n",
      "\t| TERM: metadata | TF-IDF: 0.00901\n",
      "\t| TERM: In | TF-IDF: 0.00573\n",
      "\t| TERM: information | TF-IDF: 0.00491\n",
      "\t| TERM: These | TF-IDF: 0.0041\n",
      "\t| TERM: Can | TF-IDF: 0.00328\n",
      "\t| TERM: elements | TF-IDF: 0.00328\n",
      "\t| TERM: Elements | TF-IDF: 0.00328\n",
      "\t| TERM: use | TF-IDF: 0.00246\n",
      "\t| TERM: our | TF-IDF: 0.00246\n",
      "\t| TERM: Core | TF-IDF: 0.00246\n",
      "\t| TERM: analysis | TF-IDF: 0.00246\n",
      "\t| TERM: identify | TF-IDF: 0.00246\n",
      "\t| TERM: Dublin | TF-IDF: 0.00246\n",
      "\t| TERM: refer | TF-IDF: 0.00246\n",
      "\n",
      "Top 15 terms in document 3 | 03_research-data-lifecycle.en.txt\n",
      "\t| TERM: lifecycle | TF-IDF: 0.01277\n",
      "\t| TERM: activities | TF-IDF: 0.00479\n",
      "\t| TERM: complex | TF-IDF: 0.00319\n",
      "\t| TERM: Center | TF-IDF: 0.00319\n",
      "\t| TERM: more | TF-IDF: 0.00319\n",
      "\t| TERM: center | TF-IDF: 0.00319\n",
      "\t| TERM: Curation | TF-IDF: 0.00319\n",
      "\t| TERM: Here | TF-IDF: 0.00319\n",
      "\t| TERM: curation | TF-IDF: 0.00319\n",
      "\t| TERM: brings | TF-IDF: 0.0016\n",
      "\t| TERM: stage | TF-IDF: 0.0016\n",
      "\t| TERM: few | TF-IDF: 0.0016\n",
      "\t| TERM: resources | TF-IDF: 0.0016\n",
      "\t| TERM: refers | TF-IDF: 0.0016\n",
      "\t| TERM: Once | TF-IDF: 0.0016\n"
     ]
    }
   ],
   "source": [
    "doclist = [document1, document2, document3]\n",
    "docnames = [\"01_research-data-defined.en.txt\",\"02_types-of-data-and-metadata.en.txt\",\"03_research-data-lifecycle.en.txt\"]\n",
    "topNwords = 15;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "        print(\"\\t| TERM: {} | TF-IDF: {}\".format(term, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "TF-IDF doesn't output the necessary result, I need n-grams selected as a combined keyword and these are often very general words like `for example` or `key concept` etc. in order to classify the text into the GOAL element. Although for document3 it finds the word `lifecycle` which is indeed one of the keywords and is part of the title of the resource. For document2 it find metadata and data which are of course keywords, but we need to identify also the word `type` which is not found. \n",
    "\n",
    "TextBlob provides options for n-grams and also connection to WordNet ontology which could be useful, so will look more into it.\n",
    "\n",
    "Full list of identified key words so far [HERE](https://docs.google.com/spreadsheets/d/1Dj4UAh6U5jAelcsz-gDCdDE9JRVhwaNei0Ctn8m0Ui4/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
