{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data pre-processing \n",
    "Data is collected from 11 MOOCs in the field of Computer Science, robotics, mathematics and physics and are processed total of 563 TXT files. \n",
    "\n",
    "#### Preprocessing steps: \n",
    "\n",
    "- sentence by sentence split\n",
    "- lower case\n",
    "- noise removal \n",
    "    - SOME punctuation STAYS \n",
    "        - point, single space and comma\n",
    "    - SOME stopwords stay \n",
    "        - between,we,i,in,here,that,you,it,that,this,there,few,if,so,to,a,an,is,until,while\n",
    "    - mention removal\n",
    "- word normalization\n",
    "    - tokenization \n",
    "    - lemmatization \n",
    "    - stemming \n",
    "- word standardization\n",
    "    - regex\n",
    "\n",
    "#### Overall logic:\n",
    "1. traverse recursively all folder and files\n",
    "2. When a file is found, save it's name into a file list\n",
    "3. For each file in the file list, apply all the **preprocessing steps** and save it as a new file with \"\\_PREPROCESSED\" added at the end in the same folder\n",
    "\n",
    "#### Next steps: \n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import string\n",
    "import time\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from pathlib import Path\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ METHODS  --------------------------------------------------------\n",
    "\n",
    "# Create a set of all allowed characters.\n",
    "# {...} is the syntax for a set literal in Python.\n",
    "allowedPunct = {\",\", \".\", \" \"}.union(string.ascii_lowercase)\n",
    "\n",
    "#stopWords = stopwords.words('english')\n",
    "#print(stopWords)\n",
    "\n",
    "# function #1    \n",
    "def splitSentences(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_Raw.txt\"):\n",
    "        print(\"[Splitting sentences on file:] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        OFName = baseName + \".en_sent.txt\"\n",
    "        \n",
    "        # the WITH keyword makes it possible to omit the file.close() function at the end to close the file\n",
    "        # all file processing happens there\n",
    "        # create a file and open it to write inside\n",
    "        with open(OFName,\"w\") as oFile:\n",
    "            print(oFile.name)\n",
    "            text = iFile.read()    \n",
    "            # initial text is full of new lines, so we have to remove them first. \n",
    "            text = text.replace(\"\\n\", \" \")    \n",
    "            # have the sentences split and print them one by one\n",
    "            sentences = sent_tokenize(text)\n",
    "            # write to the output file\n",
    "            for sent in sentences:\n",
    "                oFile.write(sent+\"\\n\")\n",
    "        oFile.close()\n",
    "    \n",
    "\n",
    "# function #2 (optional)\n",
    "# only leaves comma, space, dot\n",
    "# read input file, save to output file\n",
    "def removePunctuation(iFile, oPathNoExt):\n",
    "    #text = re.sub(\"[\\[\\].;@#$%^&*:()][^,]\", \" \", file)\n",
    "    if filePath.endswith(\"_sent.txt\"):\n",
    "        print(\"[Punctuation removal on file: ] \" + iFile.name)\n",
    "        with open(oPathNoExt + \"_noPunct.txt\",\"w\") as oFile:\n",
    "            oFile.write(\"\".join([letter for letter in iFile if letter in allowed]))\n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "      \n",
    "# function #3\n",
    "def sentTokenize(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_sent.txt\"):\n",
    "        print(\"[Tokenizing file: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokens.txt\"\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:\n",
    "            tokens = iFile.read()\n",
    "            tokens = word_tokenize(tokens)\n",
    "            for tok in tokens:\n",
    "                oFile.write(tok+\"\\n\")\n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "# function #4 - partOfSpeechTag Tagging\n",
    "def POStag(iFile, oPathNoExt):\n",
    "    if iFile.name.endswith(\"_tokens.txt\"):\n",
    "        print(\"[Part of Speech tagging: ] \" + iFile.name)\n",
    "        baseName = oPathNoExt.split(\".en\", 1)[0]\n",
    "        # print(\"BASE NAME: \", baseName)\n",
    "        OFName = baseName + \".en_tokPOStag.txt\"\n",
    "        \n",
    "        tokenList = iFile.read().split()\n",
    "        \n",
    "        with open(OFName, \"w\") as oFile:            \n",
    "            taggedTok = pos_tag(tokenList)\n",
    "            tokens = []\n",
    "            \n",
    "            for tok in taggedTok:\n",
    "# UNallowedPunct = {\",\", \".\", \"[\", \"]\", \"*\", \"/\", \"+\", \"-\", \"%\", \"#\", \"(\", \")\", \"-\", \"_\", \";\", \":\", \"'\", \"\\\"\", \"`\"}.union(string.ascii_lowercase)\n",
    "                UNallowedPunct = {\",\", \".\", \"[\", \"]\"}.union(string.ascii_lowercase)\n",
    "                if tok[0] in UNallowedPunct:\n",
    "                    # if token is a punctuation symbol, don't save in the output\n",
    "                    pass\n",
    "                else:\n",
    "                    tokens.append(tok)\n",
    "                    oFile.write(str(tok)+\"\\n\")\n",
    "                    \n",
    "        oFile.close()\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of 556 TXT files found.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------- PROGRAM  -------------------------------------------------------\n",
    "\n",
    "# LOCATION OF TEST FILES: /media/sf_Shared_Folder/TEST/RAW\n",
    "# path = \"/media/sf_Shared_Folder/Courses/Coursera Downloads Processed\"\n",
    "# path = \"/media/sf_Shared_Folder/TEST/RAW\"\n",
    "path = \"/media/sf_Shared_Folder/Coursera Downloads PreProcessed\"\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # check for file extension and if not TXT, continue and disregard the current file\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            else: \n",
    "                # else create a new txt file with \"_PROC.txt\" to store the output and process the original file\n",
    "                try: \n",
    "                    counter += 1\n",
    "                    #fileName = print(os.path.abspath(filePath))\n",
    "                    curFile = open(filePath, 'r', encoding = \"ISO-8859-1\") #IMPORTANT ENCODING! UTF8 DOESN'T WORK\n",
    "                    #outpFile = open(os.path.abspath)\n",
    "\n",
    "                    fileExtRemoved = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                    #outpFileBase = open(FileExtRemoved + \"_PROC.txt\",\"w\")\n",
    "\n",
    "                    \"\"\"\n",
    "                    call each processing function here and pass it the file\n",
    "                    First argument: current input file\n",
    "                    Second argument: path without extension of the current file\n",
    "                    the path will be used to save the output file with the same name and same location\n",
    "                    but with different file ending based on what the fuunction did\n",
    "                    \"\"\"  \n",
    "                    #removePunctuation(curFile, fileExtRemoved)\n",
    "                    splitSentences(curFile, fileExtRemoved)\n",
    "                    sentTokenize(curFile, fileExtRemoved)\n",
    "                    POStag(curFile, fileExtRemoved)\n",
    "                    \n",
    "                finally: \n",
    "                    curFile.close()\n",
    "        \n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet.\n",
    "\n",
    "##### Term frequency\n",
    "\\\\( tf(t,d) = 0.5 + 0.5 * (\\frac{f_{t,d}}{f_{t',d}:t' \\in d}) \\\\) \n",
    "\n",
    "##### Inversed document frequency\n",
    "\\\\( idf(t,D) = log * (\\frac{N}{d \\in D  :  t \\in d}) \\\\)\n",
    "\n",
    "##### Computing tf-idf\n",
    "\\\\( tfidf(t,d,D) = tf(t,d) * idf(t,D) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TF-IDF with data\n",
    "\n",
    "#### TODO: Fix the input, it takes strings, and not files right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse each folder and sub-folder\n",
    "# create an array of files to add each file in it\n",
    "# if the file is TXT, add to the array\n",
    "# create a String array of documents with the file of the array with files \n",
    "# so we can store the contents of each inside\n",
    "# read each line of each file and save to the strings\n",
    "# process each string by tokenization, lemmatization etc. \n",
    "# perform tf-idf on the documents\n",
    "\n",
    "# 01-understanding-research-data/01_research-data-defined.en.txt\n",
    "document1 = tb(\"\"\"[sound] so, who knows what a function is, right\n",
    "but i know what it does\n",
    "it takes an input value, and produces an output value\n",
    "and we've got a whole bunch of functions, right\n",
    "and we can take these functions and start asking questions about them\n",
    "\"\"\")\n",
    "\n",
    "# 01-understanding-research-data/02_types-of-data-and-metadata.en.txt\n",
    "document2 = tb(\"\"\"\n",
    "take a look at the further reading, whereyou'll find additional resources to learn more about file formats, compression,normalization, and data transformations\n",
    "you may wish to move on to the next moduleand return to these references later\n",
    "we recommend that you move on tothe module about documentation and data citation.\"\"\")\n",
    "\n",
    "# 01-understanding-research-data/03_research-data-lifecycle.en.txt\n",
    "document3 = tb(\"\"\"her like lego pieces in a big lego drawing\n",
    "and, most importantly, are there traps\n",
    "are there issues that arise because of these switches that we don't fully know how to deal with\n",
    "now, this module will deal with all of this, \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------ TF-IDF --------------------------------------------------------\n",
    "\n",
    "# arrays to hold the terms found in text and also a custom list to test domain-specific terms\n",
    "exportedList = []\n",
    "ownList = {\"data management\",\"database\",\"example\",\"iot\",\"lifecycle\",\"bloom\",\"filter\",\"integrity\",\n",
    "           \"java\",\"pattern\",\"design pattern\",\"svm\",\"Support vector machine\",\"knn\",\"k-nearest neighbors\",\"machine learning\"}\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.column_headers = [\"TERM\", \"TF-IDF\"]\n",
    "\n",
    "doclist = [document1, document2, document3]\n",
    "#doclist = [document4, document5, document6]\n",
    "docnames = [\"01_research-data-defined.en.txt\",\"02_types-of-data-and-metadata.en.txt\",\"03_research-data-lifecycle.en.txt\"]\n",
    "topNwords = 15;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topNwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "         table.append_row([term, round(score, 5)]) \n",
    "         exportedList.append(term)\n",
    "    \n",
    "    print(table)\n",
    "#    print(exportedWords, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------- NLTK, WORDNET -------------------------------------------\n",
    "print(\"\\n\\n------- EXPORTED TERMS in WORDNET ----------\") \n",
    "for word in exportedList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "\n",
    "print(\"\\n\\n------- CUSTOM TERMS in WORDNET (also domain specific) ----------\")    \n",
    "for word in ownList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
