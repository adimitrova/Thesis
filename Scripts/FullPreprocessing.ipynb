{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data pre-processing \n",
    "Data is collected from 11 MOOCs in the field of Computer Science, robotics, mathematics and physics and are processed total of 563 TXT files. \n",
    "\n",
    "#### Preprocessing steps: \n",
    "\n",
    "- sentence by sentence split\n",
    "- lower case\n",
    "- noise removal \n",
    "    - punctuation STAYS\n",
    "    - SOME stopwords stay\n",
    "    - mention removal\n",
    "- word normalization\n",
    "    - tokenization \n",
    "    - lemmatization \n",
    "    - stemming \n",
    "- word standardization\n",
    "    - regex\n",
    "\n",
    "#### Overall logic:\n",
    "1. traverse recursively all folder and files\n",
    "2. When a file is found, save it's name into a file list\n",
    "3. For each file in the file list, apply all the **preprocessing steps** and save it as a new file with \"\\_PREPROCESSED\" added at the end in the same folder\n",
    "\n",
    "#### Next steps: \n",
    "1. Apply TF-IDF\n",
    "2. Try Wikipedia linking\n",
    "3. Try linking with WordNet\n",
    "4. Try Bag of Words\n",
    "5. Try other algorithms? \n",
    "6. Define a clear dictionary with words for each category\n",
    "7. Other Classification algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary modules for EVERYTHING here\n",
    "\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "# ---- for TF-IDF & NLTK\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from beautifultable import BeautifulTable\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[music] welcome to the lesson on cloudcomputing and cyber physical systems\n",
      "in this video lecture, we will look intothe relationship between cloud computing and the future of cyber physicalsystems or embedded systems in general\n",
      "let's talk about cloud computing\n",
      "in the simplest form,cloud computing is about utilizing computational power as a utilitylike electricity or gas\n",
      "instead of building your ownin-house computational capabilities, you rent them out from peoplewho have already designed them\n",
      "this model of renting out resources basedon one's needs has several benefits\n",
      "first, flexibility\n",
      "the resources you utilize like storage,computing capacity or network can scale up orscale down based on your needs\n",
      "second, low maintenance\n",
      "as the servers are managedby your provider, you don't have to worry aboutmaintenance or software updates\n",
      "third, paper use, you only pay forthe resources you use\n",
      "fourth, security, cloud servicescan maintain better security and hire better professionals comparedto a small business or startup in the words of a researcherin the field of security, cloud is using somebody else's computer\n",
      "in another lesson, we talked aboutdifferent protocols like gsm, bluetooth, and zigby that enable us to connectour cyber physical systems\n",
      "we also pointed out the one's that supportgcpip to enable ip based communication\n",
      "when you take a small embedded device andassign it an ip address, you can uniquely identify it,remotely access and control it andshare its data across the internet\n",
      "what you have done is that you havebuilt an internet of things device or iot device\n",
      "you had a device, a thing, andyou made it a part of the internet\n",
      "irrespective of all the hype andwell justified interest in the internet of things, at its very basic level,it is a very large number of embedded systems talking to eachother via ip networks\n",
      "if you look at it from this perspective,to an embedded systems engineer, iot is more of an evolution of the technologiesthat were already available to him\n",
      "the reason we are talking aboutthe internet of things is that it shares a lot of similaritieswith cyber physical systems\n",
      "iot is about monitoring or sensing thereal world with interconnected sensors\n",
      "cyber physical systems are the same\n",
      "any platform that billsitself as an iot platform can also be cyber physicalsystems platform\n",
      "from now on, in this lesson,we will use the term iot instead of cyber physicalsystems as most of the cloud platforms like to associatethemselves with the internet of things\n",
      "cloud computing andiot compliment each other very well\n",
      "in the vision of internet of things, you have hundreds of thousandsof sensors generating data\n",
      "cloud computing helps you store andprocess that data\n",
      "you can see an example of trackingpeople's fitness on the screen\n",
      "if you are an iot device andnetwork provider, you can focus on your core competence,building and connecting sensors\n",
      "you don't have to worry aboutbuilding your own it infrastructure\n",
      "another problem is makingsense of all that data\n",
      "there are cloud computing toolsavailable that will help you analyze and generate actionableintelligence from your data\n",
      "if you are monitoring traffic flow or power consumption ina large housing society, or monitoring the air quality of a largecity, you need a strong it infrastructure\n",
      "cloud computing offers you that\n",
      "this growing interest incombining cloud computing and internet of things has createdon new field called cloud iot\n",
      "cloud iot is providingon demand computational infrastructure designed specifically forthe internet of things\n",
      "cloud iot providers canoffer different services\n",
      "here are some basic considerationsto look into when choosing one\n",
      "do they offer support for the communication hardware andsoftware you will be using\n",
      "do they support the communicationprotocols you are using in your solution\n",
      "do they offer you remotenode management like controlling your device remotely, issuingcommands or upgrading its firmware\n",
      "do they offer any data analytics\n",
      "if yes, is it according to your needs\n",
      "what kind of pricing model do they have\n",
      "are they charging by the numberof nodes or sensors you have or by the number of resources you use\n",
      "here is a short list of different cloudiot platforms and service providers\n",
      "in this lesson, we looked atthe merging of cloud computing and the internet of things phenomenon\n",
      "the next video lecture, we will look at a phenomenon thatis opposite to cloud computing\n",
      "[sound]\n",
      "None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'close' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-0d5ffd3004c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremovePunctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTotal number of {} {} files found.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TXT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'close' is not defined"
     ]
    }
   ],
   "source": [
    "# only leaves commas \n",
    "def removePunctuation(text):\n",
    "    text = re.sub(\"[\\[\\].;@#$%^&*:()][^,]\", \" \", file)\n",
    "\n",
    "\n",
    "# path = \"/media/sf_Shared_Folder/Courses/Coursera Downloads Processed\"\n",
    "path = \"/media/sf_Shared_Folder/TEST/02_3-4-cloud-and-cyber-physical-systems 2\"\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for root, subdirs, files in os.walk(path):\n",
    "\n",
    "    for curFile in os.listdir(root):\n",
    "\n",
    "        filePath = os.path.join(root, curFile)\n",
    "\n",
    "        if os.path.isdir(filePath):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            if not filePath.endswith(\".txt\"):\n",
    "                pass\n",
    "            else: \n",
    "                counter += 1\n",
    "                #fileName = print(os.path.abspath(filePath))\n",
    "                curFile = open(filePath, 'r')\n",
    "                #outpFile = open(os.path.abspath)\n",
    "                nameNoExt = os.path.splitext(os.path.abspath(filePath))[0]\n",
    "                outpFile = nameNoExt + \"_PROC.txt\"\n",
    "                \n",
    "                #open(outpFile, 'x')\n",
    "                text = curFile.read().strip()\n",
    "                print(text)\n",
    "                print(removePunctuation(text))\n",
    "                close(curFile)\n",
    "        \n",
    "print(\"\\nTotal number of {} {} files found.\".format(counter, \"TXT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF implementation\n",
    "### --------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "IF-IDF is implemented in order to check whether the terms extracted from LOs will have anything in common with the terms that would be extracted with manual MOOC analysis and to compare with of the two methods will bring better results in the classification part\n",
    "\n",
    "Below is the main TF-IDF implementation without any text provided to it yet.\n",
    "\n",
    "##### Term frequency\n",
    "\\\\( tf(t,d) = 0.5 + 0.5 * (\\frac{f_{t,d}}{f_{t',d}:t' \\in d}) \\\\) \n",
    "\n",
    "##### Inversed document frequency\n",
    "\\\\( idf(t,D) = log * (\\frac{N}{d \\in D  :  t \\in d}) \\\\)\n",
    "\n",
    "##### Computing tf-idf\n",
    "\\\\( tfidf(t,d,D) = tf(t,d) * idf(t,D) \\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob is the the text where to look for the word\n",
    "def tf(term, doc):\n",
    "    #return ratio between nr of certain word count and total document word count\n",
    "    return doc.words.count(term) / len(doc.words)\n",
    "\n",
    "def docsWithTermIn(term, doclist):\n",
    "    return sum(1 for doc in doclist if term in doc.words)\n",
    "\n",
    "def idf(term, doclist):\n",
    "    return math.log(len(doclist) / (1 + docsWithTermIn(term, doclist)))\n",
    "\n",
    "def tfidf(term,doc,doclist):\n",
    "    return tf(term, doc) * idf(term,doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TF-IDF with data\n",
    "\n",
    "#### TODO: Fix the input, it takes strings, and not files right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse each folder and sub-folder\n",
    "# create an array of files to add each file in it\n",
    "# if the file is TXT, add to the array\n",
    "# create a String array of documents with the file of the array with files \n",
    "# so we can store the contents of each inside\n",
    "# read each line of each file and save to the strings\n",
    "# process each string by tokenization, lemmatization etc. \n",
    "# perform tf-idf on the documents\n",
    "\n",
    "# 01-understanding-research-data/01_research-data-defined.en.txt\n",
    "document1 = tb(\"\"\"[sound] so, who knows what a function is, right\n",
    "but i know what it does\n",
    "it takes an input value, and produces an output value\n",
    "and we've got a whole bunch of functions, right\n",
    "and we can take these functions and start asking questions about them\n",
    "\"\"\")\n",
    "\n",
    "# 01-understanding-research-data/02_types-of-data-and-metadata.en.txt\n",
    "document2 = tb(\"\"\"\n",
    "take a look at the further reading, whereyou'll find additional resources to learn more about file formats, compression,normalization, and data transformations\n",
    "you may wish to move on to the next moduleand return to these references later\n",
    "we recommend that you move on tothe module about documentation and data citation.\"\"\")\n",
    "\n",
    "# 01-understanding-research-data/03_research-data-lifecycle.en.txt\n",
    "document3 = tb(\"\"\"her like lego pieces in a big lego drawing\n",
    "and, most importantly, are there traps\n",
    "are there issues that arise because of these switches that we don't fully know how to deal with\n",
    "now, this module will deal with all of this, \"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------ TF-IDF --------------------------------------------------------\n",
    "\n",
    "# arrays to hold the terms found in text and also a custom list to test domain-specific terms\n",
    "exportedList = []\n",
    "ownList = {\"data management\",\"database\",\"example\",\"iot\",\"lifecycle\",\"bloom\",\"filter\",\"integrity\",\n",
    "           \"java\",\"pattern\",\"design pattern\",\"svm\",\"Support vector machine\",\"knn\",\"k-nearest neighbors\",\"machine learning\"}\n",
    "\n",
    "table = BeautifulTable()\n",
    "table.column_headers = [\"TERM\", \"TF-IDF\"]\n",
    "\n",
    "doclist = [document1, document2, document3]\n",
    "#doclist = [document4, document5, document6]\n",
    "docnames = [\"01_research-data-defined.en.txt\",\"02_types-of-data-and-metadata.en.txt\",\"03_research-data-lifecycle.en.txt\"]\n",
    "topNwords = 15;\n",
    "\n",
    "for i, doc in enumerate(doclist):\n",
    "    print(\"\\nTop {} terms in document {} | {}\".format(topNwords, i + 1, docnames[i]))\n",
    "    scores = {term: tfidf(term, doc, doclist) for term in doc.words}\n",
    "    sortedTerms = sorted(scores.items(),key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for term, score in sortedTerms[:topNwords]:\n",
    "         table.append_row([term, round(score, 5)]) \n",
    "         exportedList.append(term)\n",
    "    \n",
    "    print(table)\n",
    "#    print(exportedWords, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------- NLTK, WORDNET -------------------------------------------\n",
    "print(\"\\n\\n------- EXPORTED TERMS in WORDNET ----------\") \n",
    "for word in exportedList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "\n",
    "print(\"\\n\\n------- CUSTOM TERMS in WORDNET (also domain specific) ----------\")    \n",
    "for word in ownList:\n",
    "    if not wn.synsets(word):\n",
    "        print(\"\\n\", word, \": NO SYNSETS\\n\")\n",
    "    else:\n",
    "        print(\"\\n\", word)\n",
    "        for ss in wn.synsets(word):\n",
    "            print(\"- \",ss.name(),\" | \",ss.definition())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
