we a model a robot, we know 
robot get position information, in 
this case we used wheel encoders 
there ways we talked 
compasses accele accelerometers 
robot also needs to know 
world around looks like that 
you need sensors we going to 
spending much time modeling 
different kinds sensors, see 
is difference between an infrared 
an ultrasonic range sensor instead 
going to come an abstraction 
that captures a lot different 
sensing modalities and, going 
to based whats called the, 
range sensor skirt this is standard 
sensor suite in a lot, a lot robots 
and, basically a collection 
sensors that collection that is, 
gathered around robot that measures 
distances in different directions so, 
infra red skirts, ultrasound lidar, 
laser scanners examples 
range sensors theyre going to 
show a lot there 
standard external sensors course, 
vision, tactile sensors, we 
bumpers ways physically 
interacting world gps im 
putting in quotation there 
ways faking gps 
instance, in lab im using a motioning, 
motion captioning system to pretend 
that i gps going to 
do, mainly assumed that we this 
kind setup a skirt around 
robot that measure distances to, to 
to things in environment in 
fact, here is chipera a 
simulation chipera chipera 
in this case, a number infrared 
sensors well you see cones, you 
blue red cones, you 
red rectangles red rectangles 
obstacles going to able 
to is measure direction 
distance to obstacles so this is 
type information going to get 
rangesensor skirts here 
right you see two pictures 
sensing modalities that we 
selfdriving car that developed 
georgia tech we laser scanners 
radar vision point is 
skirt doesnt always to uniform 
even homogeneous across sensors here 
we a skirt that is heterogeneous 
across different sensing modalities but, 
roughly you kind 
abstraction a car like this, well 
hey, chipera, little mobile 
differential drive, robot okay, so, 
thats fine, we dont actually want to 
worry particular sensors we need to 
come an abstraction this, 
sensor skirt, that, that makes sense, that 
we reason we design 
controller so, going to is, 
going to some, perform whats 
called a disk abstract abstraction 
so heres robot, sitting here in 
middle around sensors in 
fact, if you look this picture here, 
here little infrared sensors in 
fact, here ultrasonic sensorsyou see 
that scattered around this robot 
a skirt range censors were, 
typically an effective range, 
going to extract that say there 
is a disk around robot, a certain 
radius, robot see whats 
going on, right, so this is this, this 
pinkish disk around robot 
detect obstacles that around 
so two red symbols there 
obstacles we is we figure 
far away two obstacles 
so, d is distance to obstacle one, 
is this guy this is obstacle 
two, well, okay join ratts ensure 
pi one is angle to that obstacle, 
similarly d is distance to obstacle 
phi is angle to obstacle two one 
thing to keep in mind though is that robot 
coordinate system in sense 
that this, if this is x axis 
robot right now, pi one is measure 
relative to robots x axis, so 
robots heading, right so we need to take 
that account if we want to know 
globally obstacles so lets 
that if you that, if you know 
pose, so we know x, y pi 
since measured headings to 
obstacles so this is pi one is 
measuring measuring this 
relative to orientation lets say that 
orientation is this right so here is 
phi here is pi two say, course 
actual direction to obstacle two is 
going to pi plus pi so, we 
could do, is we could take this 
account compute global positions 
obstacles if we know 
robot is so, instance, global 
position obstacle one x y well, 
position robot plus 
distance to that obstacle times cosine 
sine this pi plus pi term so we 
actually know globally obstacles 
if we know robot actually 
is so this is an assumption going 
to make going to assume that we 
know x, y pi a corollary to 
that, going to assume that we know 
position obstacles around this in 
environment so thats abstraction 
that going to designing 
controllers around i want to 
show you a im using an example 
this, this is known rendezvous 
problem in multi agent robotics, you 
lots robots that supposed to 
meet common location theyre 
allowed to talk, theyre allowed 
to agree this would 
chatting instead to move in 
a way that end meeting in 
location one way this is to 
assume you a rain sensor disk around 
you you see robots in 
that disk instead thinking 
obsticles we think buddies so 
we going to is robot is 
going to aim toward center gravity 
neighboors so everyone that is 
in that disk, disk, 
disk assumption disk abstraction we 
talked about, we actually compute 
center gravi ty is 
neighbors so heres an example 
this looks like every robot is shrinking 
two, robots shrink to 
meet point, without 
communication, simply taking disk 
around them, looking 
neighbors in that disk, we know 
to compute that and, then, computing 
center gravity neighbors, 
aiming towards said center gravity 
okay, we a robot model we a 
model figuring to know 
robot is, we a model 
we know obstacles things in 
environment we use 
things course to actually start 
designing controllers, so thats 
going to to next i want 
to point though that model 
real encoder, disk abstraction 
an example you 
do, you make kinds 
abstractions different kinds 
robots, different types models 
abstractions may appropriate 
