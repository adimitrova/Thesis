1
00:00:00,012 --> 00:00:05,227
We have a model of a robot, we know how
the robot can get position information, in

2
00:00:05,239 --> 00:00:10,138
this case we used the wheel encoders but
there are other ways we talked about

3
00:00:10,150 --> 00:00:15,312
compasses and accele accelerometers but
the robot also needs to know what the

4
00:00:15,324 --> 00:00:20,887
world around. It looks like. And for that
you need sensors. And we are not going to

5
00:00:20,899 --> 00:00:25,584
be spending too much time modeling
different kinds of sensors, and see what

6
00:00:25,596 --> 00:00:30,645
is the difference between an infrared and
an ultrasonic range sensor. Instead.

7
00:00:30,652 --> 00:00:35,662
We're going to come up with an abstraction
that captures what a lot of different

8
00:00:35,674 --> 00:00:40,759
sensing modalities can do. And, it's going
to be based on what's called the, the

9
00:00:40,771 --> 00:00:46,052
range sensor skirt. This is the standard
sensor suite in a lot, on a lot of robots.

10
00:00:46,162 --> 00:00:51,545
And, it's basically a. Collection of
sensors that are. The collection that is,

11
00:00:51,908 --> 00:00:56,913
gathered around the robot. That measures
distances in different directions. So,

12
00:00:57,019 --> 00:01:02,469
infra red skirts, ultrasound. LIDAR, which
are laser scanners. These are all examples

13
00:01:02,481 --> 00:01:07,090
of these range sensors. They're going to
show up a lot. Now there are other

14
00:01:07,102 --> 00:01:11,470
standard external sensors of course,
vision, or tactile sensors, we have

15
00:01:11,482 --> 00:01:16,070
bumpers or other ways of physically
interacting with the world or "GPS" or I'm

16
00:01:16,082 --> 00:01:20,385
putting them in quotation because there
are other ways of faking GPS. For

17
00:01:20,397 --> 00:01:25,205
instance, in my lab I'm using a motioning,
or motion captioning system to pretend

18
00:01:25,217 --> 00:01:30,530
that I have GPS. But what we're going to
do, mainly. It's assumed that we have this

19
00:01:30,542 --> 00:01:36,025
kind of setup. Where a skirt around the
robot that can measure distances to, to

20
00:01:36,037 --> 00:01:41,907
other to things in the environment. And in
fact, here is the Chipera It's a

21
00:01:41,919 --> 00:01:48,310
simulation of the Chipera. And the Chipera
in this case, has a number of infrared

22
00:01:48,322 --> 00:01:54,255
sensors. And Well you see the cones, you
have blue and red cones, and then you have

23
00:01:54,267 --> 00:01:59,480
red rectangles. The red rectangles are
obstacles and what we're going to be able

24
00:01:59,492 --> 00:02:04,355
to do is measure the direction and
distance to obstacles. So this is what

25
00:02:04,367 --> 00:02:10,180
type of information we're going to get out
of these range-sensor skirts. over here on

26
00:02:10,192 --> 00:02:15,143
the right you see two pictures of the
sensing modalities that we had on The

27
00:02:15,155 --> 00:02:20,585
self-driving car that was developed at
Georgia Tech. And we have laser scanners

28
00:02:20,597 --> 00:02:26,557
and radar and vision. but the point is the
skirt doesn't always have to be uniform or

29
00:02:26,569 --> 00:02:31,999
even homogeneous across the sensors. Here
we have a skirt that is heterogeneous

30
00:02:32,011 --> 00:02:37,213
across different sensing modalities. But,
roughly you have the same kind of

31
00:02:37,225 --> 00:02:42,175
abstraction for a car like this, as well
as for. Hey, Chipera, little mobile

32
00:02:42,187 --> 00:02:46,578
differential drive, robot. Okay, so,
that's fine, but we don't actually want to

33
00:02:46,590 --> 00:02:50,788
worry about particular sensors. We need to
come up with an abstraction of this,

34
00:02:51,111 --> 00:02:55,502
sensor skirt, that, that makes sense, that
we can reason about when we design our

35
00:02:55,514 --> 00:02:59,930
controller. So, what we're going to do is,
we're going to do some, or perform what's

36
00:02:59,942 --> 00:03:04,755
called a disk abstract. Abstraction.
So here's the robot, sitting here in the

37
00:03:04,767 --> 00:03:09,675
middle. around it are sensors. And in
fact, if you look at this picture here,

38
00:03:09,780 --> 00:03:14,903
here are little infrared sensors. And in
fact, here are ultrasonic sensors.You see

39
00:03:14,915 --> 00:03:20,065
that scattered around this robot are. It's
a skirt of range censors. We're, they

40
00:03:20,077 --> 00:03:24,900
typically have an effective range, and
we're going to extract that and say there

41
00:03:24,912 --> 00:03:29,766
is a disk around the robot, of a certain
radius, where the robot can see what's

42
00:03:29,778 --> 00:03:34,820
going on, right, so this is this, this
pinkish disk around the robot and it can

43
00:03:34,832 --> 00:03:39,931
detect obstacles that are. Around it.
So the two red symbols there are the

44
00:03:39,943 --> 00:03:45,386
obstacles. What we can do is we can figure
out how far away are the two obstacles.

45
00:03:45,500 --> 00:03:50,693
So, D1 is the distance to obstacle one,
which is this guy. And this is obstacle

46
00:03:50,766 --> 00:03:56,880
two, well, okay. join with ratts of ensure
and Pi one is the angle to that obstacle,

47
00:03:56,999 --> 00:04:02,792
similarly d2 is the distance to obstacle
2. Phi 2 is the angle to obstacle two. One

48
00:04:02,804 --> 00:04:08,875
thing to keep in mind though is that robot
has its own coordinate system in the sense

49
00:04:08,887 --> 00:04:14,377
that this, if this is the x axis of the
robot right now, then Pi one is measure

50
00:04:14,389 --> 00:04:20,618
relative to. The robot's x axis, so the
robot's heading, right. So we need to take

51
00:04:20,630 --> 00:04:26,478
that into account if we want to know
globally where the obstacles are. So let's

52
00:04:26,490 --> 00:04:32,695
do that. If you have that, and if you know
our own pose, so we know x, y and Pi. Then

53
00:04:32,707 --> 00:04:38,290
since the measured headings to the
obstacles. So this is Pi one which is

54
00:04:38,302 --> 00:04:44,555
measuring and we're measuring this
relative to our orientation. Lets say that

55
00:04:44,567 --> 00:04:50,995
our orientation is this right. So here is
phi and here is Pi two say, then of course

56
00:04:51,007 --> 00:04:56,043
the actual. direction to obstacle two is
going to be Pi 2 plus Pi. So, what we

57
00:04:56,055 --> 00:05:01,033
could do, is we could take this into
account and compute the global position's

58
00:05:01,045 --> 00:05:06,290
of these obstacles if we know where the
robot is. So, for instance, the global

59
00:05:06,302 --> 00:05:12,518
position for obstacle one x1 and y1. Well,
it's the position of the robot plus the

60
00:05:12,530 --> 00:05:18,809
distance to that obstacle times cosine and
sine of this Pi 1 plus Pi term. So we

61
00:05:18,821 --> 00:05:25,581
actually know globally where the obstacles
are if we know where The robot actually

62
00:05:25,593 --> 00:05:29,478
is. So this is an assumption we're going
to make. We're going to assume that we

63
00:05:29,490 --> 00:05:33,450
know x, y and Pi. And as a corollary to
that, we're going to assume that we know

64
00:05:33,462 --> 00:05:37,776
the position of obstacles around this in
the environment. So that's the abstraction

65
00:05:37,788 --> 00:05:41,249
that we're going to be designing our
controllers around. And I just want to

66
00:05:41,554 --> 00:05:45,857
show you a. And I'm using an example of
this, this is known as the rendezvous

67
00:05:45,869 --> 00:05:50,328
problem in multi agent robotics, where you
have lots of robots that are supposed to

68
00:05:50,340 --> 00:05:54,656
meet at the common location but they're
not allowed to talk, they're not allowed

69
00:05:54,668 --> 00:05:58,226
to agree on where this would be by
chatting instead they have to move in such

70
00:05:58,238 --> 00:06:03,527
a way that. They end up meeting in same
location and one way of doing this is to

71
00:06:03,539 --> 00:06:09,178
assume you have a rain sensor disk around
you and then when you see other robots in

72
00:06:09,190 --> 00:06:14,726
that disk instead of thinking of them as
obsticles we think of them as buddies so

73
00:06:14,738 --> 00:06:20,246
what we are going to do is each robot is
going to aim toward the center of gravity

74
00:06:20,258 --> 00:06:25,590
of all it's neighboors so everyone that is
in that disk, Disk, and because of the

75
00:06:25,602 --> 00:06:30,494
disk assumption or disk abstraction we
just talked about, we can actually compute

76
00:06:30,506 --> 00:06:35,052
where the center of gravi ty is of our
neighbors. So here's an example of what

77
00:06:35,064 --> 00:06:40,186
this looks like. Every robot is shrinking
down. Two, all the robots shrink down to

78
00:06:40,198 --> 00:06:44,849
meet at the same point, without any
communication, simply by taking the disk

79
00:06:44,861 --> 00:06:49,269
around them, looking where are my
neighbors in that disk, and now we know

80
00:06:49,281 --> 00:06:54,434
how to compute that. And, then, computing
the center of gravity of my neighbors, and

81
00:06:54,446 --> 00:06:59,483
aiming towards said center of gravity.
Okay, now we have a robot model. We have a

82
00:06:59,495 --> 00:07:04,054
model for figuring out how to know where
the robot is, we have a model for how do

83
00:07:04,066 --> 00:07:08,397
we know where obstacles and things in
environment are. Now we can use these

84
00:07:08,409 --> 00:07:12,665
things of course to actually start
designing controllers, so that's what

85
00:07:12,677 --> 00:07:17,249
we're going to have to do next. I do want
to point out though that the model The

86
00:07:17,261 --> 00:07:22,162
real encoder, and the disk abstraction.
These are but an example of what you can

87
00:07:22,174 --> 00:07:27,279
do, and how you should make these kinds of
abstractions. But for different kinds of

88
00:07:27,291 --> 00:07:31,634
robots, different types of models and
abstractions may be appropriate.