we
have
a
model
of
a
robot
,
we
know
how
the
robot
can
get
position
information
,
in
this
case
we
used
the
wheel
encoders
but
there
are
other
ways
we
talked
about
compasses
and
accele
accelerometers
but
the
robot
also
needs
to
know
what
the
world
around
.
it
looks
like
.
and
for
that
you
need
sensors
.
and
we
are
not
going
to
be
spending
too
much
time
modeling
different
kinds
of
sensors
,
and
see
what
is
the
difference
between
an
infrared
and
an
ultrasonic
range
sensor
.
instead
.
we
're
going
to
come
up
with
an
abstraction
that
captures
what
a
lot
of
different
sensing
modalities
can
do
.
and
,
it
's
going
to
be
based
on
what
's
called
the
,
the
range
sensor
skirt
.
this
is
the
standard
sensor
suite
in
a
lot
,
on
a
lot
of
robots
.
and
,
it
's
basically
a.
collection
of
sensors
that
are
.
the
collection
that
is
,
gathered
around
the
robot
.
that
measures
distances
in
different
directions
.
so
,
infra
red
skirts
,
ultrasound
.
lidar
,
which
are
laser
scanners
.
these
are
all
examples
of
these
range
sensors
.
they
're
going
to
show
up
a
lot
.
now
there
are
other
standard
external
sensors
of
course
,
vision
,
or
tactile
sensors
,
we
have
bumpers
or
other
ways
of
physically
interacting
with
the
world
or
``
gps
''
or
i
'm
putting
them
in
quotation
because
there
are
other
ways
of
faking
gps
.
for
instance
,
in
my
lab
i
'm
using
a
motioning
,
or
motion
captioning
system
to
pretend
that
i
have
gps
.
but
what
we
're
going
to
do
,
mainly
.
it
's
assumed
that
we
have
this
kind
of
setup
.
where
a
skirt
around
the
robot
that
can
measure
distances
to
,
to
other
to
things
in
the
environment
.
and
in
fact
,
here
is
the
chipera
it
's
a
simulation
of
the
chipera
.
and
the
chipera
in
this
case
,
has
a
number
of
infrared
sensors
.
and
well
you
see
the
cones
,
you
have
blue
and
red
cones
,
and
then
you
have
red
rectangles
.
the
red
rectangles
are
obstacles
and
what
we
're
going
to
be
able
to
do
is
measure
the
direction
and
distance
to
obstacles
.
so
this
is
what
type
of
information
we
're
going
to
get
out
of
these
range-sensor
skirts
.
over
here
on
the
right
you
see
two
pictures
of
the
sensing
modalities
that
we
had
on
the
self-driving
car
that
was
developed
at
georgia
tech
.
and
we
have
laser
scanners
and
radar
and
vision
.
but
the
point
is
the
skirt
does
n't
always
have
to
be
uniform
or
even
homogeneous
across
the
sensors
.
here
we
have
a
skirt
that
is
heterogeneous
across
different
sensing
modalities
.
but
,
roughly
you
have
the
same
kind
of
abstraction
for
a
car
like
this
,
as
well
as
for
.
hey
,
chipera
,
little
mobile
differential
drive
,
robot
.
okay
,
so
,
that
's
fine
,
but
we
do
n't
actually
want
to
worry
about
particular
sensors
.
we
need
to
come
up
with
an
abstraction
of
this
,
sensor
skirt
,
that
,
that
makes
sense
,
that
we
can
reason
about
when
we
design
our
controller
.
so
,
what
we
're
going
to
do
is
,
we
're
going
to
do
some
,
or
perform
what
's
called
a
disk
abstract
.
abstraction
.
so
here
's
the
robot
,
sitting
here
in
the
middle
.
around
it
are
sensors
.
and
in
fact
,
if
you
look
at
this
picture
here
,
here
are
little
infrared
sensors
.
and
in
fact
,
here
are
ultrasonic
sensors.you
see
that
scattered
around
this
robot
are
.
it
's
a
skirt
of
range
censors
.
we
're
,
they
typically
have
an
effective
range
,
and
we
're
going
to
extract
that
and
say
there
is
a
disk
around
the
robot
,
of
a
certain
radius
,
where
the
robot
can
see
what
's
going
on
,
right
,
so
this
is
this
,
this
pinkish
disk
around
the
robot
and
it
can
detect
obstacles
that
are
.
around
it
.
so
the
two
red
symbols
there
are
the
obstacles
.
what
we
can
do
is
we
can
figure
out
how
far
away
are
the
two
obstacles
.
so
,
d1
is
the
distance
to
obstacle
one
,
which
is
this
guy
.
and
this
is
obstacle
two
,
well
,
okay
.
join
with
ratts
of
ensure
and
pi
one
is
the
angle
to
that
obstacle
,
similarly
d2
is
the
distance
to
obstacle
2.
phi
2
is
the
angle
to
obstacle
two
.
one
thing
to
keep
in
mind
though
is
that
robot
has
its
own
coordinate
system
in
the
sense
that
this
,
if
this
is
the
x
axis
of
the
robot
right
now
,
then
pi
one
is
measure
relative
to
.
the
robot
's
x
axis
,
so
the
robot
's
heading
,
right
.
so
we
need
to
take
that
into
account
if
we
want
to
know
globally
where
the
obstacles
are
.
so
let
's
do
that
.
if
you
have
that
,
and
if
you
know
our
own
pose
,
so
we
know
x
,
y
and
pi
.
then
since
the
measured
headings
to
the
obstacles
.
so
this
is
pi
one
which
is
measuring
and
we
're
measuring
this
relative
to
our
orientation
.
lets
say
that
our
orientation
is
this
right
.
so
here
is
phi
and
here
is
pi
two
say
,
then
of
course
the
actual
.
direction
to
obstacle
two
is
going
to
be
pi
2
plus
pi
.
so
,
what
we
could
do
,
is
we
could
take
this
into
account
and
compute
the
global
position
's
of
these
obstacles
if
we
know
where
the
robot
is
.
so
,
for
instance
,
the
global
position
for
obstacle
one
x1
and
y1
.
well
,
it
's
the
position
of
the
robot
plus
the
distance
to
that
obstacle
times
cosine
and
sine
of
this
pi
1
plus
pi
term
.
so
we
actually
know
globally
where
the
obstacles
are
if
we
know
where
the
robot
actually
is
.
so
this
is
an
assumption
we
're
going
to
make
.
we
're
going
to
assume
that
we
know
x
,
y
and
pi
.
and
as
a
corollary
to
that
,
we
're
going
to
assume
that
we
know
the
position
of
obstacles
around
this
in
the
environment
.
so
that
's
the
abstraction
that
we
're
going
to
be
designing
our
controllers
around
.
and
i
just
want
to
show
you
a.
and
i
'm
using
an
example
of
this
,
this
is
known
as
the
rendezvous
problem
in
multi
agent
robotics
,
where
you
have
lots
of
robots
that
are
supposed
to
meet
at
the
common
location
but
they
're
not
allowed
to
talk
,
they
're
not
allowed
to
agree
on
where
this
would
be
by
chatting
instead
they
have
to
move
in
such
a
way
that
.
they
end
up
meeting
in
same
location
and
one
way
of
doing
this
is
to
assume
you
have
a
rain
sensor
disk
around
you
and
then
when
you
see
other
robots
in
that
disk
instead
of
thinking
of
them
as
obsticles
we
think
of
them
as
buddies
so
what
we
are
going
to
do
is
each
robot
is
going
to
aim
toward
the
center
of
gravity
of
all
it
's
neighboors
so
everyone
that
is
in
that
disk
,
disk
,
and
because
of
the
disk
assumption
or
disk
abstraction
we
just
talked
about
,
we
can
actually
compute
where
the
center
of
gravi
ty
is
of
our
neighbors
.
so
here
's
an
example
of
what
this
looks
like
.
every
robot
is
shrinking
down
.
two
,
all
the
robots
shrink
down
to
meet
at
the
same
point
,
without
any
communication
,
simply
by
taking
the
disk
around
them
,
looking
where
are
my
neighbors
in
that
disk
,
and
now
we
know
how
to
compute
that
.
and
,
then
,
computing
the
center
of
gravity
of
my
neighbors
,
and
aiming
towards
said
center
of
gravity
.
okay
,
now
we
have
a
robot
model
.
we
have
a
model
for
figuring
out
how
to
know
where
the
robot
is
,
we
have
a
model
for
how
do
we
know
where
obstacles
and
things
in
environment
are
.
now
we
can
use
these
things
of
course
to
actually
start
designing
controllers
,
so
that
's
what
we
're
going
to
have
to
do
next
.
i
do
want
to
point
out
though
that
the
model
the
real
encoder
,
and
the
disk
abstraction
.
these
are
but
an
example
of
what
you
can
do
,
and
how
you
should
make
these
kinds
of
abstractions
.
but
for
different
kinds
of
robots
,
different
types
of
models
and
abstractions
may
be
appropriate
.
