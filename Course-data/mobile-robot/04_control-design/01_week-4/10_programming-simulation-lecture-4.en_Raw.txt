welcome back.
this is week four.
so we're halfway through the programming
and
simulation lectures as well as the
assignments.
so congratulations on making it this far.
last week i talked about goal to goal
controllers which allowed our
robots to drive from their current
location to some location in the
environment.
and this week i'm going to talk about.
how to create an obstacle avoidance
controller.
and this obstacle avoidance controller,
the point of it is that we want to be
able to drive around the environment
without colliding with any of the
obstacles.
so if you're driving a robot in your
living room you
don't want it to drive into your sofa and
get destroyed.
so this week we're going to take care of
that.
and there are actually many ways of doing
obstacle avoidance.
and i've picked out one particular way of
doing obstacle avoidance.
and this is how we do it.
first, we're going to interpret the
ir distance as a point in the coordinate
frame of the
sensor and transform this point into the
coordinate frame of the robot.
then we're going to take this point that's
in the robots coordinate frame, and
transform it once
again into the world coordinate frame, so
that
we know where each obstacle is in the
world.
what we will do then is compute a vector
from these points to
the robot, and we will use that as an, and
sum them together
as an obstacle avoidance vector.
and then what we will do is the same thing
as last week is we're going to
use this vector and use a pid controller
to
steer the robot towards the orientation of
the vector.
and effectively what will happen is that
the robot will drive
in the direction that points away from any
obstacles that are nearby.
and thus will avoid collisions.
so i've mentioned coordinate frame
at least four or five times on this slide.
what do i mean by that?
we use coordinate frames to give meaning
to points in some sort of space.
so when i tell you that the robot is
located at
xy theta, you need to know which
coordinate frame i'm talking about.
so, in most cases i'm talking about the
world frame.
and this world frame is centered at the
origin,
which i'm going to denote as just zero,
zero.
so i pick this, where this is in the
simulator.
and with respect to this world frame, the
location of the robot is given by x and y.
and of course, also with respect to this
world
frame, we have an orientation of theta, of
it.
so, the, the robot, right here has an
orientation given by theta, and it's
important that this theta is defined with,
with
respect to the x axis of this world frame.
now, the next coordinate frame that we
have is the robot's
coordinate frame, and this coordinate
frame is located right at the robot.
so wherever the robot is, at the center of
this robot, there's
a cornered frame and it's, this, this we
call the robot frame.
so, with respect to the robot, this
direction right here going out in
front of the robot, which aligns with the
robot's orientation is, would be theta,
what it's maybe called theta prime is
equal to zero, in that robot's frame.
but, theta prime in the world frame, so
maybe i'll call this theta
prime w f, would be equal to the actual
theta of the robot.
now,
the reason that we care about the robot's
frame of reference, is because we
only know the location of the sensors and
the orientation of the sensors;
with respect to the robot, robot frame.
so the robot knows for example, that it
has one particular sensor, mounted right
here which is, sensor number one.
and this sensor, it knows is, is located
at, at this point
with respect to its own robot's coordinate
frame, and its
orientation along in this direction.
and this orientation, with respect to the
robot, is 90 degrees.
but in the world frame,
this orientation would be also a function
of the actual orientation of the robot.
so, let's call this maybe theta s prime.
so we said that was 90 degrees, which is
the same thing
as pi over 2.
and what we want
to do is, to figure out where the sensor
is, so theta prime
s, in the world frame this would be not,
nothing more than, pi
divided by 2 plus the orientation of the
robot.
so this would give you the angle of this
sensor in the world frame.
now, the way that i've denoted this in, in
the manual is, using.
sorry, i'm going to color this out.
is using, x sub s four.
so this is the x position of sensor four
in the robot's frame.
the y position of sensor four in the, in
the robot's frame and the orientation of
sensor four in the, in the robot's frame,
so in this case, theta s four is actually,
so this is equivalent to minus 45 degrees.
because that's the orientation of the
sensor with respect to the robot.
now, we can go even one step further than
this.
what we can define is, a coordinate frame,
with respect to the sensor itself.
so each sensor has its own coordinate
frame.
so we're getting fairly
deep in here, but, the point of that is
that when we defy it for example
here this sensor i's frame and in this
case it's sensor you know, it's actually
sensor four.
and again the origin of this coordinate
frame is located where the sensor is
located.
and the x axis of that coordinate
frame aligns with the orientation of that
sensor.
and this is important because
what we're going to do, is we're going to
figure out that the distance
that we're measuring is actually nothing
more
than a point in this coordinate system.
so, here, what i've done is, the robot
measures a distance of, so this distance
right here is d
4 and all i've done is i said well in
this,
particular coordinate frame, this point
right here
is equal to this vector right here.
so d 4 0 so because we're going a distance
of d 4, the
x direction the distance is 0 and
the y direction in that particular
coordinate frame.
now, what we really care about in this,
in, in,
in the controller is, well if i have this
point in
this sensor's coordinate frame, what does
this
point correspond to in the world frame.
because, as you can imagine, if i for
example have a, a point
up here that is on an obstacle, i can say
that the obstacle is
at this location in the world, and i know
where the robot is and
from that we can make an informed decision
about how to move the robot.
and that's exactly what we'll do.
so, in order for you to be able to
calculate the transformation between the
different coordinate frames, you're
going to need to
know how to rotate and translate in 2d.
and what i mean by that is that if we have
a coordinate frame,
so say, this is my coordinate frame right
here, and i have this point.
right here, so i have a point right here,
lets call that 1,0.
now this i can also, i can pretend that
this point is a
vector going from the origin to this point
and suppose
what i wanted to do, is rotate this vector
and then translate it.
and let's say that i want to rotate it and
i'm going
to use this notation: r, and i'm going to
say, translated by one
unit in the x direction, two units in the
y direction and pi over four is the
translation.
so what does this r actually mean?
well, the r is given by this
transformation matrix.
and what this transformation matrix is,
does, is exactly what i just described.
it's going to take a vector and, when you
pre-multiply
this vector by r you're going to get the,
the, the vector transformed in space by x
and y, translated by x and y and then
rotated by this theta prime right here.
so we're actually translating by x prime y
prime according to my notation here.
so going back to my example.
what, what really, what's really going on
is
that first, we're going to rotate the
vector by,
pi over 4, which means
that it's now located at, squared of 2
over 2, squared 2 over 2.
and then we're going add a translation in
the x direction and then a translation in
the y direction of one unit and two units.
which means that, the vector that i get,
corresponds to 1 plus squared of 2 over 2
and 2 plus squared of 2 over 2.
so what i've effectively done here is i've
taken this point, 1,0 and
i have translate, rotated and translated
it to this point up here, which is 1, 1
plus square root of 2 over 2.
2 plus square root of 2 over 2.
and that's how the rotation and
translation works, and so,
so whether it's a point or vector doesn't
really matter.
the point really is that i've gone through
this,
this transformation, i've gotten a, a
translation and a rotation.
and to be a little bit more specific, what
i've really
done is so, on this side i would have so,
so here
this, this should have been r,1 comma 2
comma pi over 4.
and, what i'm multiplying, the vector that
i'm multiplying with, what you'll
notice is that this is first of all a 3 by
matrix, so we want to make sure that this
is going to
be a 3 by 1 matrix for this to be a valid
multiplication.
and what we're going to do is we're
going to put the point that we're
translating,
so let's call this xy, so this
is x, which would've been, which was equal
to 1, y equal to 0.
and then we're always going to place to
make, going to place a 1 right here.
so, this stays a 1 independent of what x
and y are.
so, why am i telling you about this?
well, you need this tool, you need this
transformation
matrix in order to do our transformation
from the
point in the centers, reference framed all
the way
back to the world, uh,to the world
coordinate frame.
and this is the entire calculation that
you're going to be doing.
as you can see here what i've done is i
have, the point
d sub i 0.
so this is the distance that was measured
by sensor i.
then, i'm doing the transformation from
the sensor frame to
the robot frame and my input into the
rotation and
translation matrix is the position of the,
and position and
orientation of the sensor on the robot, in
the robot's frame.
so now this entire thing gives
us this point in the robot's reference
frame, instead of the sensor's reference
frame.
so, we have to do another rotation
rotation and translation.
and we do that by using the robot's
location
location and orientation in the in the, in
the world frame.
when we do that, this entire thing, right
here will,
point, give us the point, this original
point right here, in the world frame.
and this is exactly what this is.
so these are the world frame coordinates
of this,
of this point detected by this particular
infrared sensor i,
now, let's get to the code.
now that we're armed with this tool.
first of all, we're going to create, or
we've already created a new file for this
controller.
it's going to be it's own file, so
it's it's own controller, it's going to be
called avoidobstacles.m.
and like this comment says, avoidobstacles
is really
for steering the robot away from any
nearby obstacles.
and really, the way to think about
it, is we're going to steer it towards
free space.
and, first of all, all these
transformations are
going to happen inside of this function
called apply_sensor_geometry.
we're going to do all three parts in there
before we
even get to the pid part of the, of the
controller.
and the first part is to apply the
transformation,
from the sensor, sensor's, coordinate
frame to the robot frame.
and really, what i'm doing here is i'm em,
already giving you the location of the
sensor,
of each sensor eye in the robot's
coordinate frame.
and that what i, what i what i want you
to do is first properly implement the get
transformation matrix.
according to the equation that i gave on,
on previous slide, previous slides.
and once you've done that properly, of
course, you're going to have to input
these here.
and then you have to figure out the proper
multiplication
and again, you should look back at what
i've done before.
but remember, we're only doing one step,
so really what you should be doing is, r,
xs, ys, theta s multiplied
by di, 0, and 1.
so this is
what i expect you to implement, right
here.
then, we're going to do the next part,
which is transforming from the,
the point from the robot's coordinate
frame over to the world coordinate frame.
so, this follows a si, the, a similar
pattern as what we did in the previous
slide.
again, what you want to make sure is that
this becomes the input for this.
you've already implemented get
transformation
matrix so it's spits out the
appropriate r, and then you're going to
again have to do the calculation.
so you pick the previous vector, so i'm
just going to represent
that by scribbles and multiply that by r,
x, y, theta.
and that should give me all of the ir
distances, all, all
the points that correspond to the ir
sensors in the world frame.
and now we know where, where obstacles are
in the world or free space
in the world, depending on whether you're
picking up
an obstacle or you're not picking up an
obstacle.
now, so like i said, we've computed the
world frame coordinates of all of these
points.
so each one of these points right here in
green, we know where they're located in
the world, right.
so, what we can do, is we can compute
vectors from the robots to each one of
these points.
and then what we can do, is since they're
all vectors we
can just sum them up.
and we can sum them up and we get this
large vector
that's point, that is going to end up
pointing away from any obstacles.
and the reason for that is that these
sensors so for example.
this sensor right here, that is detecting
no obstacles or the distance is
really great, is going to contribute much
more than this sensor which is picking
up an obstacle close by.
so when you sum up all these sensors, the
main contribution is
going to be along a direction which is,
away from the obstacle.
and then once we've done that, we can use
the orientation of this vector and use
pid control much like we did in the doog,
google controller to steer the robot in
to the direction of the free space.
so here is the code, in the
main execute function of the avoid
obstacles controller.
one thing i would like for you to pay
attention to is that i have created this
sensor gains,
and i ask you in the manual to think
about how do you want to structure these
sensor gains.
do you want all of the sensors to count
equally,
do we contribute with equally, do you may
be care about a particular sensor more?
so for example do you care about the one
that's in the front and that's the third
sensor.
so maybe this should be 3, so that you pay
more
attention more to the obstacles that are
in front of you.
do you or do you want to pay more
attention
to obstacles that are on the side in that
case you would maybe increase these two,
so sensor
one and five, that go to the left and to
the, to the right of the robot.
so, play around with that and you should
get some interesting results.
and again, here i'm going to, here i'm
asking you to properly
calculate each of the uis, so each one of
the vectors.
and then, we're just going to sum them
together and then you're going to
have to figure out the angle of this
vector, so you get theta ao.
and then, just like last weekend, the goal
to
goal controllers, so you're already
familiar with that, you need
to compute the error between, this angle
that you want
to steer to, and the actual angle of the
robot.
now, to test this we're just going to
implement this controller
and run it on the robot so let's see what
happens.
i mean, obviously we're, we hope that the
robot's not going to collide with
any of the walls or any of the obstacles
that i've added to the environment.
but let's make sure this actually happens.
i've already implemented the controller,
so i'm going
to head eh, go ahead and launch the
simulator.
i'm going to hit play, and we're going to
follow this robot and see.
so i clicked on it to follow it, and we're
going to see what, what it does.
it's already turned away from the, from
the obstacle.
now there is a wall and phew,
we made it and here's another wall and yet
again, the robot turns away.
and since i've implemented this controller
correctly, the
robot should just drive around aimlessly
around this environment.
we haven't told him where he needs to go,
we
just have told him that he needs to avoid
any obstacles.
and, in fact, next week, we'll talk about
how
to combine goal to goal controllers and
avoid obstacle controllers.
my tips for this week are again to refer
to
the section for week 4 in the manual for
more details.
and i have always provided very detailed
instructions
that should help you get through these
assignments.
and also keep in mind that, even though
this seems like a really difficult
approach and really
tedious, and you could probably think of,
of a
way easier ways of doing obstacle
avoidance with robots.
the reason that i've done it in a way
where
we end up with a vector is that it'll make
it a lot easier
next week, when we combine the goal
to goal controllers and the obstacle one
controllers.
because then it's a matter of just
blending two vectors
together, we know definitely know how to
sum vectors together so.
stick with it and i'll see you next week.
