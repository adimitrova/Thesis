next
topic
we
discuss
is
control
of
robotic
lighting
system
.
the
work
we
introduce
here
is
titled
lighty
,
a
painting
interface
for
room
illumination
by
robotic
light
array
.
so
as
a
program
,
we
want
to
address
here
is
that
it
is
difficult
to
control
many
light
,
especially
when
they
can
change
orientations
.
so
here
an
example
destination
,
so
suppose
we
have
many
lights
in
the
ceiling
,
and
the
individual
lights
can
change
its
orientation
.
and
this
is
a
very
huge
control
parameter
space
,
and
it
's
very
hard
to
control
appropriate
parameters
.
and
typical
user
interfaces
look
like
this
.
you
have
many
sliders
to
change
brightness
and
orientation
,
and
so
on
.
and
this
is
not
very
useful
for
people
to
quickly
sketch
desired
lighting
configuration
.
so
our
approach
is
to
use
painting
interface
.
so
configuration
looks
like
this
.
we
have
an
environment
,
and
we
have
many
robotic
lights
,
and
they
can
change
brightness
and
orientation
.
and
we
have
a
camera
here
to
capture
our
environment
.
and
then
in
this
kind
of
view
,
there
's
a
paint
,
desired
lighting
this
up
.
so
this
part
should
be
bright
,
this
should
be
,
this
part
should
be
dark
.
we
just
paint
desired
configuration
.
and
then
the
system
learns
inverse
relation
,
and
then
obtain
desired
parameter
setting
.
so
that
's
the
idea
.
let
me
show
you
a
video
.
so
again
,
this
is
a
system
work
view
.
so
you
have
actuated
the
light
on
the
ceiling
and
the
ui
in
the
environment
.
and
then
you
would
attempt
to
control
the
light
you
pick
up
a
tablet
and
they
use
these
environment
and
they
paint
on
it
.
so
this
is
a
prototype
hardware
we
developed
.
we
the
,
we
built
a
miniature
room
with
miniature
lights
and
miniature
furniture
and
this
is
a
array
of
robotic
lights
.
so
,
they
can
change
orientation
individually
,
and
also
can
change
brightness
.
so
,
each
light
has
three
degrees
of
freedom
.
so
if
we
have
12
light
,
which
means
36
yeah
,
degrees
of
freedom
.
so
,
here
's
a
painting
user
interface
.
so
,
given
this
screen
,
you
're
going
to
pick
up
a
color
and
paint
.
yeah
.
okay
.
so
here
lots
of
things
happening
here
.
so
this
view
is
always
a
real-time
capture
of
the
camera
view
.
so
you
see
camera
view
.
and
as
i
use
a
paint
,
user
's
paint
is
actually
feedback
is
given
as
these
quanta
lines
.
so
this
area
bright
,
this
area
is
a
little
bit
dark
.
and
then
given
this
control
on
user
request
,
system
continuously
learns
optimization
to
get
desired
parameters
,
and
it
's
a
real-time
.
the
environment
con
moves
around
the
light
.
and
then
up
,
and
you
see
the
result
in
real-time
in
this
camera
view
.
so
,
there
's
a
lots
of
happening
behind
the
scene
.
yeah
,
so
user
paint
system
searches
for
the
,
the
parameter
setting
interactively
.
and
then
you
see
the
layout
result
immediately
.
so
,
you
always
see
the
camera
view
.
yeah
.
so
,
depending
what
the
user
input
,
system
automatically
computes
the
parameters
said
and
then
derives
the
robotic
system
.
so
,
this
is
more
rapid
example
.
so
,
if
you
touch
down
,
and
move
around
,
it
will
actually
paint
uncommitted
.
but
,
if
you
hover
,
you
paint
it
on
top
of
the
surface
,
you
can
still
see
the
preview
of
the
painting
without
actually
committing
the
paint
.
so
this
is
a
mimic
of
oh
,
simulation
of
traditional
approach
.
so
you
have
12
lights
and
then
you
individually
control
brightness
and
orientation
.
so
,
and
this
a
typical
interface
,
and
it
's
very
tedious
control
one
by
one
,
to
get
the
desired
result
.
so
we
compare
this
interface
with
our
painting
interface
,
and
if
you
want
to
bright
somewhere
,
if
you
want
to
illuminate
somewhere
,
it
's
relatively
easy
.
you
just
turn
on
,
you
edit
right
,
and
then
delete
it
.
however
,
if
you
want
to
make
some
part
dark
,
it
suddenly
turns
out
to
be
very
difficult
using
traditional
interface
because
it
involves
control
of
many
lights
.
you
know
,
you
move
like
sideways
,
moving
away
,
looking
away
and
turn
off
.
but
there
's
lots
of
interruptions
behind
,
between
multiple
lights
,
so
that
's
a
difficulty
.
okay
,
so
that
's
the
video
i
think
.
so
all
the
benefit
of
our
system
is
that
in
addition
,
make
specific
regions
bright
,
you
can
also
make
specific
regions
dark
.
this
is
kind
of
negative
light
.
so
you
ask
a
specific
region
to
get
darker
,
and
the
system
do
it
,
and
looks
like
a
negative
light
,
and
if
you
,
figure
it
out
from
our
user
study
,
in
the
painting
if
the
user
asked
to
make
up
a
upper
left
corner
dark
,
just
paint
a
region
darker
and
you
get
this
result
.
however
,
if
you
use
traditional
direct
controller
,
it
takes
time
and
it
's
very
difficult
to
get
this
kind
of
result
.
and
let
me
briefly
describe
the
algorithm
,
behind
the
scene
.
so
,
this
is
what
's
happening
.
so
,
this
is
the
light
parameters
,
so
you
have
many
parameters
.
light
orientations
,
brightness
and
so
on
.
and
then
if
you
derive
the
light
,
and
if
you
get
this
out
,
then
you
know
,
physics
will
happen
and
then
you
will
get
this
camera
view
.
and
then
,
after
that
i
use
a
paint
,
desired
painting
result
.
so
,
system
compares
these
two
,
and
then
optimize
the
parameter
setting
and
then
we
'll
get
it
.
and
in
order
to
run
optimization
,
you
need
to
do
this
iteratively
many
times
,
so
instead
of
using
actually
driving
physical
lights
,
we
run
simulation
.
you
know
,
what
happens
if
this
parameter
setting
is
given
,
and
the
system
simulates
the
illumination
result
and
then
compare
the
result
compare
it
to
the
user
input
,
and
then
again
last
simulation
and
so
,
so
that
's
the
idea
.
so
the
physical
process
again
is
the
process
,
physical
process
is
too
complicate
to
obtain
analytic
model
.
so
in
order
to
do
this
simulation
,
we
use
a
data-driven
prediction
method
.
so
we
,
so
we
capture
many
,
many
illumination
results
.
so
you
have
many
parameters
,
like
lighting
parameters
.
so
this
light
and
brightness
,
the
second
right
query
orientation
.
so
you
have
many
parameters
and
then
this
,
you
get
many
,
many
camera
views
.
so
you
capture
many
images
.
and
then
based
on
this
data
,
you
predict
the
resulting
elimination
results
for
a
new
given
parameter
set
.
and
in
order
to
do
this
,
we
basically
individually
control
the
lighting
parameter
for
individual
light
,
and
then
we
add
them
together
.
however
,
important
point
is
naive
summation
of
image
data
where
pixel
values
does
not
work
.
so
suppose
you
have
pixel
a
illumination
rays
out
here
,
no
,
suppose
you
have
a
illumination
result
over
a
here
and
then
illumination
result
over
light
b
here
.
but
id
you
are
tired
of
,
to
doing
pixel
values
,
it
does
not
correspond
to
the
result
,
eliminated
by
light
a
and
b
simultaneously
.
that
is
because
of
the
non-linear
radiation
set
between
radiance
.
so
,
physical
physical
value
of
the
brightness
is
not
directly
linear
.
relate
,
linearly
related
to
the
pixel
value
,
because
there
's
a
non-linearity
.
so
in
order
to
handle
this
,
you
,
we
first
need
to
convert
pixel
values
into
a
radiance
value
.
so
re-award
brightness
value
.
after
converting
pixel
value
to
radiance
value
you
can
accurately
predict
the
summation
of
two
light
illumination
.
then
after
that
we
can
again
convert
it
to
the
pixel
body
to
get
the
prediction
.
so
that
's
what
you
need
to
do
to
get
this
kind
of
system
.
so
in
summary
,
we
,
i
just
shows
robotic
lighting
system
with
painting
interface
.
and
i
briefly
discussed
what
you
need
to
do
,
to
implement
this
kind
of
thing
.
so
,
you
need
to
compute
a
simulation
in
the
radiance
space
,
instead
of
pixel
space
,
pixel
brightness
value
space
.
so
the
reason
a
paper
was
published
as
design
and
the
enhancement
of
painting
interface
for
room
lights
.
and
if
you
want
to
know
more
about
radiance
computation
,
one
,
one
good
starting
point
is
this
paper
,
recovering
high
dynamic
range
,
dynamic
radiance
maps
from
photographs
.
so
this
paper
discuss
,
hot
to
compute
original
radiance
values
from
multiple
photographs
,
over
the
same
scene
.
and
also
our
lighting
control
with
painting
interfaces
,
there
are
a
couple
experiments
in
3d
graphics
,
and
one
example
is
this
one
,
lighting
with
paint
.
so
user
paints
desired
lighting
result
,
and
the
system
computes
,
appropriate
lighting
for
the
computer
graphics
.
so
what
do
we
do
is
a
real
world
version
of
this
one
.
so
that
's
it
for
this
week
