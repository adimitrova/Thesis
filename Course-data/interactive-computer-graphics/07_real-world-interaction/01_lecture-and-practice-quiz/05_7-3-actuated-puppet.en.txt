Our next topic is Actuated Puppet. The what we introduce here is an Actuated
Physical Puppet as an Input Device for Controlling a Digital Manikin. The Motivation here is that 3D
character posing is difficult. So, suppose you have a character
that you want to set the posing. And the most popular approach
I think is 3D widgets. On the screen,
you have a widget like this, and you use a mouse to control
individual joy dongles. However, this is very tedious to
control individual joint dongles. For each joint, you have three angles,
one by one, can take much time. Another approach is the Motion capture. You have a capture device and a unit. However, you need dedicated environment,
and that you also need a skilled actor, actor. And also another possibility is
to use Physical Input Device. This is very convenient. You can grab a Puppet and
then pose, and you get the 3D view. So you can use both
hands can be very good. However standard physical
input devices very stiff. You know if you, if it to relax,
it cannot hold a pose. And if its stiff enough to hold a pose,
it's very hard to control. And also you have to control,
manually, space for all joint angles,
which is inconvenient, in some cases. So, our approach is to add
several motors to each joint, so we get an Actuated Physical Puppet. So we not only use Puppet as input device,
but also the device reacts to the physical
batch of data to provide active feedback. And there are couple benefits to it. First, it's easy to load
existing pose into the robot, so you can start from existing
pose to accelerate posing. So it's one benefit. Another is intelligent
gravitational compensation. Here's a little bit explanation. As I said, if the joint is all free,
it's easy to control. But you know,
the it cannot hold it's posture. So it's inconvenient to And then another approach is always
approach turn off the sabo motors. Then it can be too stiff. It can hold existing pose, like this legs. However as a negative side it very
hard to control, change a pose. So what we do is, adaptively do it,
intelligent gravitational compensation. So if you do not touch this arm,
these legs, its con, fixed by a sabo motor by producing
a power against grav, gravity. However, if the user start
to manipulating the joint, the system automatically detects it. And then automatically turns
off some sabo motors, so you can push a berry with
very light weight force. So, that is intelligent
gravitational compensation. And then finally,
we also implemented active guidance, using measured human data. And this consist of joint coupling,
and data driven inverse kinematics. Let me describe a little bit more. So joint coupling is two
joints work together. What's easy to understand the example
is this hip joint, and knee joint. You know,
as you know if you move your leg. Upwards you if you control hip joint,
then knee joint also follows, right? It's very hard to keep the legs
straight as you rotate the hip joint. So, if you rotate the hip joint, it's very natural to also
bend the knee joint together. So, this is joint coupling. And with actuation you can do it,
you know, if you control the hip joint, the system
will automatically rotate the knee joint. This is joint coupling, which is very useful to generate
very natural human posture. So this one automatically avoids,
unnatural configuration. The another is,
data driven inverse kinematics. If you just control position. For example, suppose you control
the fingertip, and they go downwards, and if you apply simple inverse
kinematics result is like this. So it's mechanical value. Geometric, geometrically bodied. Finger tip follows the user control. However, joint only rotates along the alo,
along the arm. And then, resulting pose is very strange,
a unnatural force. And instead of that, we use data. We measure up, lots of natural
reaching gesture, over humour. And the user synthesize
appropriate motion. So here if the user pull down
the fingertip downwards, and then the character, the robot,
automatically puppet moves making a very natural motion,
using data. So, let me show you a video. So as I said Physical Puppet not only
controls a physical body of character, also body of character provides
feedback to the Physical Puppet to guide not to oppose anything. [INAUDIBLE] To the eight degrees
of freedom and connect to the PC. Yeah so primarily it's an input device. You control the pulse, and then you
sink [INAUDIBLE] up the same pulse. So this provides a very intuitive way,
of controlling character posture. This can be very tedious,
if you use two dimensional mouse. And one benefit of using sabo motors is, uploading pre defined postures
into a robot, our puppet. So, you can get pre-defined
postures in to the puppet. So this is a very good starting point for design, and
then this is Gravitational Compensation. You know, if the Torque is OFF to relax
then it cannot hold a given posture, you know, if you release it goes down. And on the other hand, if you to,
always turn on the torque power, then it can hold the posture,
but, very rigid, you know? You need strong power. To rotate. And this is a result of
automatic gravity compensation. So as you start pushing. The system turns off the power. So, you can change the pose,
with very light weight force. But also,
system can hold the posture after release. And then,
this is a result of Data-driven IK. So, if the Data-driven IK is turned off. The motion is very unnatural. It's very robotic motion, because the
robot doesn't change entire body shape. However.
If you turn on the inverse driven, Data-driven IK,
then the resulting motion is vary natural. So user control the fingertip, and then
system automatically bend his legs and then move the head and other arm. And this is a joint coupling. The user is controlling the hip joint, and the system automatically
controls knee joint, so. So, let me show you a couple
application scenarios. So, suppose you design in car seat, and
then the visibility should be preserved. So this, in this way, you can intuitively control the detail
monitoring in the car environment. And you can check the here,
is reachability. So you can check whether this puppet
can meet specific target in the car. So this one is load assessment. So this led indicates where the power
is necessary to hold the posture. So, this is useful for
analysis of environment. And this is a Visibility analysis. So by changing the posture of puppet, we can see which part of
the environment the puppet can see. So that's the idea. So let be briefly describe the algorithm. Specifically, I will briefly describe how
to do Data-driven Inverse-Kinematics. So as I said,
Data-driven I case like this. If you move a fingertip, then the puppet
makes a very natural human like motion. In order to do so. We get data. But let me first describe
Inverse-Kinematics in general. So in that Inverse-Kinematics,
is a very fundamental program, in character animation or
also in robotics. So suppose you have this kind of arc,
you have a base here, and then you have joint one,
joint two and joint three. And you have each joint angles. So, Forward Kinematics
is the standard way. So, given joint angles,
you get the position of the fingertip. So, this is fairly deterministic
single computation. However, what people usually want
to do is to specify target x, y position of fingertip. And then try to convert
appropriate joint angles. This is very infrequently
occurring problem to solve. And then it looks like this, given x,
y, you have gone to joint angles. This is inverse problem,
which is very difficult to solve. For many solutions, many other And there are couple
possible ways like purely geometric. So something like, minimize a change
from the current configuration or physically most you know, physically most
plausible shape, or as a data-driven. And Data-driven is a method we use. So, we do some synaptics. In the motion capture environment,
we ask for the person to reach many places around his body, and then we measure all
the postures, to get always position. So you get many data like this, x, y,
z position, and joint angles, x, y, z position, and joint angles. We get many data of these samples. And after that user input is
target fingertip location. And then we are identify
the nearby samples. And then just blend them together. And then you'll get,
a desired joint angle. So this is what we do in
the Data-driven Inverse-Kinematics. So here's the summary. So we presented, I presented actuated
puppet device for character posing. The user controls a virtual character,
using physical puppet, and also gets feedback from the system. And one important feature is
intelligent gravity compensation, and active guidance from the system. And we, I present
the Data-driven Inverse Kinematics. So, we take many snapshots, example for this, and then blend them
together to get natural posture. So, to learn more,
original paper is titles, An Actuated Physical Puppet as an Input
Device for Controlling a Digital Manikin. And if you want to know more about
the previous standard puppet devices, there's two papers. Dinosaur Input Device. And those of mice and donkeys, and specialize in the device for
[INAUDIBLE] for the animations. And Inverse Kinematics, as I said,
lots of work on these topics. So example base like is this one. Artist Directed Inverse Kinematics using
Gladiator basis function interpolation. So this is an example of
a Data-driven Inverse ICAD. And also there is a geometric approach. So natural Motion, Animation Solute
Constraining, and Deconstraining at will. So this one introduces
a purely geometric approach, to get natural posture
from finger tip positions. That's it for this week. I guess [INAUDIBLE].