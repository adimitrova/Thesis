[music] the primary goal of preservation is to safeguard the authenticity and the integrity of digital content.
we want to be sure that the data are what we expect them to be.
as defined by the society of american archivists, authenticity is the quality of being genuine, not a counterfeit, and free from tampering, and is typically inferred from internal and external evidence.
including it's physical characteristics, structure, content, and context, because digital data are so vulnerable to change, whether inadvertent or not, ensuring the authenticity of any particular data set is especially important.
this is very much related to the issue of trust, particularly for data that will be shared for purposes of replication or secondary analysis.
users of the data must have guarantees that the data have not been tampered with and represent the actual data generated by the data producer.
if data have been altered in any way documentation detailing the alteration should be maintained and made available alongside the data.
establishing authenticity of digital data involves processes undertaken both by the researcher and the data repository.
the uk data archives website provides a useful list of best practices to assist researchers in maintaining the authenticity of their data files.
these best practices include maintaining a single master file of the data.
assigning responsibility for master files to a single project team member.
regulating write access to master versions of the data files.
recording all changes to master files.
maintaining all master files in case subsequent versions contain errors.
archiving copies of master files at regular intervals.
and developing a formal procedure for the destruction of master files.
as you can see, this list focuses on lessening the potential for unwanted or unintended changes to a file through write-access controls.
this list also points to the importance of keeping careful documentation in relation to any changes made to the master data file.
having these formal procedures and clear roles and responsibilities in place will help researchers maintain an authentic final copy of their data, free of unintentional and unwanted changes.
likewise, repositories have procedures and processes in place to maintain the authenticity of the data they house for safekeeping.
establishing authenticity of data submissions requires the repository to collect and preserve descriptive and contextual metadata to allow for appropriate identification and interpretation of the data.
the repository must also document any operations performed on the data, including any automatic or manual manipulation of data files and their content.
validation mechanisms should be in place to ensure transformed or modified data file versions are representative of the original.
all of these actions are necessary to provide users with a degree of trust in the authenticity of the data.
data that are properly preserved not only have the quality of being authentic, but they also have integrity.
having integrity means that a digital object has not been corrupted over time or in transit between storage locations or systems.
as clifford lynch, director of the coalition for networked information puts it, integrity is knowing with certainty that we have in hand the same set of sequences of bits that came into existence when the object was created.
just like with authenticity, there are some best practices that both researchers and information professionals can perform to attend to the integrity of digital data.
these include backing up critical or frequently used files using an automated backup process.
storing master files in open source formats for long-term software readability.
verifying backup copies of files against original files by comparing checksum values, file size, and date.
storing copies of files on two different types of storage media.
and copying or migrating files to new storage media every two to five years to avoid the effects of physical degradation of storage media.
these simple tasks can assist in avoiding corruption of digital files over time through redundancy and migration.
in addition, the verification of files through checksums is an important mechanism for data repositories to use to ensure the data files have not been modified in transit to the repository.
we will discuss these tasks in more detail,while technology has increased our ability to invent new ways of collecting and analyzing data, it also has inherent vulnerabilities.
authenticity and the integrity of data can be achieved only through an active preservation strategy based on standards and best practices.
[sound]
