[MUSIC] The primary goal of preservation is
to safeguard the authenticity and the integrity of digital content. We want to be sure that the data
are what we expect them to be. As defined by the Society of
American Archivists, authenticity is the quality of being genuine, not
a counterfeit, and free from tampering, and is typically inferred from
internal and external evidence. Including it's physical characteristics,
structure, content, and context, because digital data are so vulnerable to change,
whether inadvertent or not, ensuring the authenticity of any particular
data set is especially important. This is very much related to
the issue of trust, particularly for data that will be shared for purposes
of replication or secondary analysis. Users of the data must have guarantees
that the data have not been tampered with and represent the actual
data generated by the data producer. If data have been altered in any way
documentation detailing the alteration should be maintained and
made available alongside the data. Establishing authenticity of
digital data involves processes undertaken both by the researcher and
the data repository. The UK Data Archives website provides
a useful list of best practices to assist researchers in maintaining
the authenticity of their data files. These best practices include maintaining
a single master file of the data. Assigning responsibility for master
files to a single project team member. Regulating write access to master
versions of the data files. Recording all changes to master files. Maintaining all master files in case
subsequent versions contain errors. Archiving copies of master
files at regular intervals. And developing a formal procedure for
the destruction of master files. As you can see, this list focuses on
lessening the potential for unwanted or unintended changes to a file
through write-access controls. This list also points to
the importance of keeping careful documentation in relation to any
changes made to the master data file. Having these formal procedures and
clear roles and responsibilities in place will help researchers maintain
an authentic final copy of their data, free of unintentional and
unwanted changes. Likewise, repositories have procedures and
processes in place to maintain the authenticity of
the data they house for safekeeping. Establishing authenticity of data
submissions requires the repository to collect and preserve descriptive and
contextual metadata to allow for appropriate identification and
interpretation of the data. The repository must also document any
operations performed on the data, including any automatic or manual manipulation of data files and
their content. Validation mechanisms should be
in place to ensure transformed or modified data file versions
are representative of the original. All of these actions are necessary
to provide users with a degree of trust in the authenticity of the data. Data that are properly preserved not only
have the quality of being authentic, but they also have integrity. Having integrity means that a digital
object has not been corrupted over time or in transit between storage locations or
systems. As Clifford Lynch, Director of the
Coalition for Networked Information puts it, integrity is knowing with certainty
that we have in hand the same set of sequences of bits that came into
existence when the object was created. Just like with authenticity, there are some best practices
that both researchers and information professionals can perform to
attend to the integrity of digital data. These include backing up critical or frequently used files using
an automated backup process. Storing master files in
open source formats for long-term software readability. Verifying backup copies of
files against original files by comparing checksum values,
file size, and date. Storing copies of files on two
different types of storage media. And copying or migrating files to
new storage media every two to five years to avoid the effects of physical
degradation of storage media. These simple tasks can assist
in avoiding corruption of digital files over time
through redundancy and migration. In addition, the verification
of files through checksums is an important mechanism for
data repositories to use to ensure the data files have not been modified
in transit to the repository. We will discuss these tasks in
more detail,while technology has increased our ability to invent
new ways of collecting and analyzing data,
it also has inherent vulnerabilities. Authenticity and the integrity of
data can be achieved only through an active preservation strategy based
on standards and best practices. [SOUND]