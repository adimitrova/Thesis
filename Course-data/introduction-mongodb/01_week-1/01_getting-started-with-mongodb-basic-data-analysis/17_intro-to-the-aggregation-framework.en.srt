1
00:00:00,000 --> 00:00:03,885
All right. Welcome to our discussion of MongoDB's aggregation framework.

2
00:00:03,885 --> 00:00:08,430
The aggregation framework is a set of analytics tools within MongoDB,

3
00:00:08,430 --> 00:00:11,790
that allow you to run various types of reports or

4
00:00:11,790 --> 00:00:15,230
analysis on documents in one or more MongoDB collections.

5
00:00:15,230 --> 00:00:19,360
The aggregation framework is based on the concept of a pipeline.

6
00:00:19,360 --> 00:00:22,680
The idea with an aggregation pipeline is that we take input from

7
00:00:22,680 --> 00:00:24,750
a MongoDB collection and pass the documents from

8
00:00:24,750 --> 00:00:27,555
that collection through one or more stages,

9
00:00:27,555 --> 00:00:31,530
each of which performs a different operation on its inputs.

10
00:00:31,530 --> 00:00:34,110
Now, each stage takes its input,

11
00:00:34,110 --> 00:00:37,350
whatever the stage before it, produced as output.

12
00:00:37,350 --> 00:00:41,895
And the inputs and outputs for all stages are documents,

13
00:00:41,895 --> 00:00:44,275
a stream of documents if you will.

14
00:00:44,275 --> 00:00:47,475
Now, if you're familiar with pipelines in a Linux shell,

15
00:00:47,475 --> 00:00:50,745
such as Bash, this is a very similar idea.

16
00:00:50,745 --> 00:00:53,760
Each stage has a specific job that it does.

17
00:00:53,760 --> 00:00:56,700
It's expecting a specific form of document and

18
00:00:56,700 --> 00:01:01,485
produces a specific output which is itself a stream of documents.

19
00:01:01,485 --> 00:01:03,080
At the end of the pipeline,

20
00:01:03,080 --> 00:01:08,210
we get access to the output much in the same way that we would by executing a fine query.

21
00:01:08,210 --> 00:01:10,770
And by that, I simply mean we get a stream of

22
00:01:10,770 --> 00:01:13,590
documents back that we can then do additional work with,

23
00:01:13,590 --> 00:01:16,245
whether it's creating a report of some kind,

24
00:01:16,245 --> 00:01:20,100
generating a website, or some other type of task.

25
00:01:20,100 --> 00:01:23,810
Now, let's dive in a little bit deeper and consider individual stages.

26
00:01:23,810 --> 00:01:28,745
So, an individual stage of an aggregation pipeline is a data processing unit.

27
00:01:28,745 --> 00:01:32,230
As I mentioned that stage takes a stream of input documents, one at a time,

28
00:01:32,230 --> 00:01:34,410
processes each document one at a time,

29
00:01:34,410 --> 00:01:36,525
and produces an output stream of documents.

30
00:01:36,525 --> 00:01:38,395
Again, one at a time.

31
00:01:38,395 --> 00:01:42,630
Each stage provides a set of knobs or tunables that we can

32
00:01:42,630 --> 00:01:49,205
control to parameterize the stage to perform whatever task we're interested in doing.

33
00:01:49,205 --> 00:01:51,870
So, a stage performs a generic task,

34
00:01:51,870 --> 00:01:54,300
a general purpose task of some kind and we

35
00:01:54,300 --> 00:01:58,020
parameterize the stage for the particular set of

36
00:01:58,020 --> 00:02:00,420
documents that we're working with and

37
00:02:00,420 --> 00:02:04,905
exactly what we would like that stage to do with those documents.

38
00:02:04,905 --> 00:02:08,730
These tunables typically take the form of operators that we can supply,

39
00:02:08,730 --> 00:02:10,140
that will modify fields,

40
00:02:10,140 --> 00:02:13,560
perform arithmetic operations, reshape documents,

41
00:02:13,560 --> 00:02:16,590
or do some sort of accumulation task,

42
00:02:16,590 --> 00:02:18,580
as well as a variety of other things.

43
00:02:18,580 --> 00:02:20,925
So finally, the last thing I'd like to mention

44
00:02:20,925 --> 00:02:23,580
about aggregation pipelines is that it's frequently

45
00:02:23,580 --> 00:02:25,740
the case that we'll want to include

46
00:02:25,740 --> 00:02:29,955
the same type of stage multiple times within a single pipeline.

47
00:02:29,955 --> 00:02:32,910
For example, we may want to perform an initial filter,

48
00:02:32,910 --> 00:02:36,540
so that we don't have to pass the entire collection into our pipeline.

49
00:02:36,540 --> 00:02:39,750
But then later on, following some additional processing,

50
00:02:39,750 --> 00:02:44,065
want to filter once again using a different set of criteria.

51
00:02:44,065 --> 00:02:48,585
So, to recap, pipelines work with a MongoDB collection.

52
00:02:48,585 --> 00:02:50,160
They're composed of stages,

53
00:02:50,160 --> 00:02:53,640
each of which does a different data processing task on

54
00:02:53,640 --> 00:02:58,370
its input and produces documents as output to be passed to the next stage.

55
00:02:58,370 --> 00:03:00,195
And finally, at the end of the pipeline,

56
00:03:00,195 --> 00:03:03,930
output is produced that we can then do something with in our application.

57
00:03:03,930 --> 00:03:06,960
In many cases it's necessary to include the same type of

58
00:03:06,960 --> 00:03:10,890
stage multiple times within an individual pipeline.

59
00:03:10,890 --> 00:03:15,295
In subsequent lessons, we will look in detail at the various pipeline stages,

60
00:03:15,295 --> 00:03:18,205
what they do, and how to prioritize them.

61
00:03:18,205 --> 00:03:23,000
We'll also look at a number of different examples of building aggregation pipelines.