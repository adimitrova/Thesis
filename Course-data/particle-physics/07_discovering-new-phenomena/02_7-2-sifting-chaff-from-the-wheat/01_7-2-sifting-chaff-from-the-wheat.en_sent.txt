[music] in this module, we discuss searches for new phenomena beyond the known ones described by the standard model.
in this video, we will talk about the life cycle of a collision event.
from the time the event is produced in the heart of a detector, until it gets ready to be processed in a physics analysis.
after watching this video, you will know why we trigger on physics events in the collider experiments.
how we translate the analog and digital signals of the detectors to physical objects that we associate with standard model particles for the events we have triggered on.
and how we deal with the large amounts of data that are produced in the collider experiments.
all that in connection to what we need to search for new physics.
as you already learned in module 3, proton bunches each containing hundreds of billions of protons collide at the lhc in collision points where experiments are installed.
this is one of the general purpose detectors of the lhc, the atlas detector.
we have already gone through its elements and their functions in video 3.10a.
if you take a look at the transverse view of the detector you will see that it has an onion structure.
it is made up of various layers of different detectors.
an inner detector inside a solenoid used for tracking charged particles.
calorimeters for electromagnetic and hadronic shower measurements.
and a muon spectrometer in toroidal magnetic field, for muon momentum measurements.
proton-proton collisions result in many particles which decay to the lightest elementary particles compatible with their quantum numbers.
these leave their signatures in the detector as digital and analogue signals, which reconstruct as objects that we associate with elementary particles.
so the particles we reconstruct in the detector are electrons, and photons, and muons and quarks, and gluons as particle jets.
details about the detector response to various particle types are given in video 3.10a.
events that contain sufficiently interesting features to be further analyzed are triggered on and stored for further processing.
these events go through event reconstruction.
to ensure an accurate measurement of the properties of the reconstructed objects, a calibration procedure is applied.
calibrated objects are now ready for analysis.
in our analysis, we need to compare our data to theoretical calculations.
to achieve this, event simulations called monte carlo simulations are performed, which create artificial events based on a specific theoretical assumption.
with these components in place, reconstructed and calibrated data on one hand, and data simulated according to theoretical predictions on the other hand, we are ready to search for new physics in our data.
but let's first understand these components better.
and let's start from the trigger.
what does this actually do?
the lhc is colliding proton bunches with a maximum rate of 40 mhz.
each event recorded by the experiment, containing all digital and analog information from the detector, has a data size of about one megabyte.
so saving all events would lead to about 40 terabytes of data per second.
this is a huge amount of data to store, but also to process.
furthermore, most of these events only contain known physics, so we need to select which of them to keep.
this is where the trigger system comes in.
the trigger system of the large multi-purpose experiments consists of two main components, a hardware based one and a software based one.
the trigger system is built to be extremely robust and fast as it needs to take a reliable decision in a very short time.
the trigger helps the selection of interesting events, but also applies upper limits in the number of events we select after each of its steps.
triggering on events that contain new phenomena is an extremely big challenge.
we don't know a priori how these events would look like.
we therefore have to build our trigger selections generic enough to ensure we don't lose anything interesting, as events we don't trigger on are are simply lost forever.
for events that were triggered the detector tells us about the elementary particles that were created in the collision process as well as their properties.
typically most of these particles are relevant in the searches for new physics.
and we will see why in the following videos of this module.
an overview is given in video 3.10a.
here, we summarize the detector response to the main types of particles.
electrons and muons both leave tracks in the inner detector.
the electrons are reconstructed by matching the track with the electromagnetic shower.
for the muon, the inner detector track is matched to a muon track in the muon spectrometer.
photons, being neutral, leave no track in the inner detector.
and are therefore associated to an electromagnetic shower with no track pointing to it.
quarks and gluons are much more complicated objects to reconstruct.
they can't live by themselves.
they have to exist in bound states, as was already explained in module 5. they hadronize and are therefore reconstructed in the calorimeter as a hadronic shower.
algorithms scan the calorimeter data in order to identify the most significant energy depositions, corresponding to quarks or gluons.
the resulting objects are the jets.
it's worth pointing out that the dominant process resulting from a proton-proton interaction is the production of quarks and gluons.
therefore jets dominate the detected final states.
neutrinos do not interact with a detector and therefore escape undetected.
however, we can get information about neutrinos using the knowledge that in the transverse plane, because of conservation of momentum, the vector sum of the momenta of all the objects has to be zero.
so neutrinos are associated with a missing momentum in the transverse plane any neutral and weakly interacting particle, such as dark matter candidates, will in fact behave just like neutrinos.
and will be identified as missing transverse momentum in the detector.
missing transverse momentum is extremely important in our searches for new physics.
w, z, higgs bosons, top quarks, and tau leptons decay immediately to elementary matter particles.
and we reconstruct them through their decay products.
in a detector, we can therefore observe various event topologies.
and based on their characteristics we can classify the events.
this is in fact what we are doing already at trigger level.
we are running algorithms that perform simplified reconstruction.
and based on the characteristics of the events, we accept or discard them.
for example, events with high momentum objects, or large multiplicity of objects.
are accepted by the trigger for further processing.
events with only low energy particles are not generally accepted, as these are of no general interest since they correspond to low q^2 processes.
it would also be impossible to store and process those as they are the most common ones.
despite the fact that we are finding ways to reduce the amount of data we need to process, we are still generating incredibly large amounts of data.
such large amounts are needed to ensure that the rarest processes will be produced and saved in our data.
such would be processes coming from new physics.
this data gets reconstructed and calibrated as we said.
it is the products of this process we are eventually using without ever discarding the original data, since we often need to reprocess them to, for example, improve reconstruction or calibration.
in order to make the best use of the large amount of data, we use sophisticated methodologies, such as machine learning techniques or other statistical methods which are often cpu intensive.
this adds to the computing needs of the lhc experiments.
it is not only data and simulation storage we need, but also processing power.
this is a combination that creates a great challenge.
this challenge is being addressed using major innovations in computing, such as the computing grid.
numerous computing centers in universities all over the world are interconnected, forming a computing grid.
they receive copies of the data.
and give the capability to scientists to run their analyses processes on this grid in an automatic and transparent way.
research and development in the computing domain is in active progress.
and the task is always extremely challenging and rewarding at the same time.
this concludes this lecture.
in the next video we will talk about a specific type of searches.
the so called bump searches.
[music]
